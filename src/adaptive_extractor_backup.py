#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
–ê–¥–∞–ø—Ç–∏–≤–Ω—ã–π –∏–∑–≤–ª–µ–∫–∞—Ç–µ–ª—å –¥–ª—è —Ä–∞–∑–Ω—ã—Ö —Ç–∏–ø–æ–≤ —Ç–∞–±–ª–∏—Ü 1–°
"""

import logging
import os
import sys
import time
from datetime import datetime

sys.path.insert(
    0, os.path.join(os.path.dirname(__file__), "..", "tools", "onec_dtools")
)

import json
from typing import Any, Dict, List, Optional

from onec_dtools.database_reader import DatabaseReader

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s - %(levelname)s - %(message)s",
    handlers=[logging.FileHandler("extraction_progress.log"), logging.StreamHandler()],
)
logger = logging.getLogger(__name__)

# –ò–º–ø–æ—Ä—Ç—ã –¥–ª—è Parquet –∏ DuckDB
try:
    import duckdb
    import pandas as pd
    import pyarrow as pa  # noqa: F401
    import pyarrow.parquet as pq  # noqa: F401

    PARQUET_DUCKDB_AVAILABLE = True
except ImportError:
    PARQUET_DUCKDB_AVAILABLE = False
    logger.warning(
        "‚ö†Ô∏è Parquet/DuckDB –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω—ã. –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ: pip install pandas pyarrow duckdb"
    )


class AdaptiveExtractor:
    """–ê–¥–∞–ø—Ç–∏–≤–Ω—ã–π –∏–∑–≤–ª–µ–∫–∞—Ç–µ–ª—å –¥–ª—è —Ä–∞–∑–Ω—ã—Ö —Ç–∏–ø–æ–≤ —Ç–∞–±–ª–∏—Ü"""

    def __init__(self) -> None:
        self.business_fields = {"_NUMBER", "_DATE_TIME", "_POSTED", "_MARKED"}

        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è
        self.extraction_stats = {
            "total_records_processed": 0,
            "successful_records": 0,
            "failed_records": 0,
            "blob_errors": 0,
            "start_time": time.time(),
            "last_checkpoint": 0,
        }

        # –ú–∞–ø–ø–∏–Ω–≥ –ø–æ–ª–µ–π –¥–ª—è —Ä–∞–∑–Ω—ã—Ö —Ç–∏–ø–æ–≤ —Ç–∞–±–ª–∏—Ü
        self.field_mapping = {
            "_DOCUMENTJOURNAL5354": {
                "amount_fields": [],  # –ù–µ—Ç –ø–æ–ª–µ–π —Å —Å—É–º–º–∞–º–∏
                "quantity_fields": [],
                "blob_fields": ["_FLD5363"],
            },
            "_DOCUMENTJOURNAL5287": {
                "amount_fields": [],
                "quantity_fields": [],
                "blob_fields": ["_FLD5299"],
            },
            "_DOCUMENTJOURNAL5321": {
                "amount_fields": ["_FLD5326"],
                "quantity_fields": ["_FLD5330", "_FLD5333", "_FLD5334"],
                "blob_fields": ["_FLD5336"],
            },
            "_DOCUMENT138": {
                "amount_fields": [],
                "quantity_fields": ["_FLD3111", "_FLD3112", "_FLD3113"],
                "blob_fields": ["_FLD3108"],
            },
            "_DOCUMENT156": {
                "amount_fields": ["_FLD3978"],
                "quantity_fields": ["_FLD3983", "_FLD3984", "_FLD3988"],
                "blob_fields": ["_FLD3980", "_FLD3981", "_FLD3982", "_FLD3986"],
            },
        }

    def analyze_table_structure(
        self, table_name: str, row_dict: Dict[str, Any]
    ) -> Dict[str, Any]:
        """–ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç —Å—Ç—Ä—É–∫—Ç—É—Ä—É —Ç–∞–±–ª–∏—Ü—ã –∏ –æ–ø—Ä–µ–¥–µ–ª—è–µ—Ç –±–∏–∑–Ω–µ—Å-–ø–æ–ª—è"""
        analysis: Dict[str, Any] = {
            "table_name": table_name,
            "total_fields": len(row_dict),
            "business_fields": {},
            "amount_fields": [],
            "quantity_fields": [],
            "blob_fields": [],
            "date_fields": [],
        }

        # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –∫–∞–∂–¥–æ–µ –ø–æ–ª–µ
        for key, value in row_dict.items():
            # –ë–∞–∑–æ–≤—ã–µ –±–∏–∑–Ω–µ—Å-–ø–æ–ª—è
            if key in self.business_fields:
                analysis["business_fields"][key] = value

            # –ü–æ–ª—è —Å —Å—É–º–º–∞–º–∏ (—á–∏—Å–ª–æ–≤—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è > 100)
            elif isinstance(value, (int, float)) and value > 100:
                analysis["amount_fields"].append({"field": key, "value": value})

            # –ü–æ–ª—è —Å –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞–º–∏ (—á–∏—Å–ª–æ–≤—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è <= 100)
            elif isinstance(value, (int, float)) and value <= 100:
                analysis["quantity_fields"].append({"field": key, "value": value})

            # BLOB –ø–æ–ª—è
            elif hasattr(value, "__class__") and "Blob" in str(type(value)):
                analysis["blob_fields"].append(key)

            # –ü–æ–ª—è —Å –¥–∞—Ç–∞–º–∏
            elif "DATE" in key or "TIME" in key:
                analysis["date_fields"].append({"field": key, "value": value})

        return analysis

    def log_progress(
        self,
        table_name: str,
        current_record: int,
        total_records: int,
        error_count: int = 0,
    ) -> None:
        """–õ–æ–≥–∏—Ä—É–µ—Ç –ø—Ä–æ–≥—Ä–µ—Å—Å –∏–∑–≤–ª–µ—á–µ–Ω–∏—è —Å –¥–µ—Ç–∞–ª—å–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–µ–π"""
        elapsed_time = time.time() - self.extraction_stats["start_time"]
        records_per_second = current_record / elapsed_time if elapsed_time > 0 else 0
        estimated_remaining = (
            (total_records - current_record) / records_per_second
            if records_per_second > 0
            else 0
        )

        progress_percent = (
            (current_record / total_records) * 100 if total_records > 0 else 0
        )

        logger.info(
            f"üìä {table_name}: {current_record:,}/{total_records:,} ({progress_percent:.1f}%) | "
            f"–°–∫–æ—Ä–æ—Å—Ç—å: {records_per_second:.1f} –∑–∞–ø/—Å–µ–∫ | "
            f"–û—Å—Ç–∞–ª–æ—Å—å: {estimated_remaining/60:.1f} –º–∏–Ω | "
            f"–û—à–∏–±–∫–∏: {error_count}"
        )

        # –û–±–Ω–æ–≤–ª—è–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
        self.extraction_stats["total_records_processed"] = current_record
        self.extraction_stats["last_checkpoint"] = current_record

    def log_error(
        self, table_name: str, record_index: int, error_type: str, error_message: str
    ) -> None:
        """–õ–æ–≥–∏—Ä—É–µ—Ç –æ—à–∏–±–∫–∏ —Å –¥–µ—Ç–∞–ª—å–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–µ–π"""
        self.extraction_stats["failed_records"] += 1
        if "blob" in error_type.lower():
            self.extraction_stats["blob_errors"] += 1

        logger.error(f"‚ùå {table_name}[{record_index}]: {error_type} - {error_message}")

    def save_checkpoint(self, table_name: str, records: List[Dict[str, Any]]) -> None:
        """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç checkpoint –¥–ª—è –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏—è"""
        checkpoint_file = f"checkpoint_{table_name}_{len(records)}.json"
        try:
            with open(checkpoint_file, "w", encoding="utf-8") as f:
                json.dump(
                    {
                        "table_name": table_name,
                        "records": records,
                        "timestamp": datetime.now().isoformat(),
                        "stats": self.extraction_stats,
                    },
                    f,
                    ensure_ascii=False,
                    indent=2,
                    default=str,
                )
            logger.info(f"üíæ Checkpoint —Å–æ—Ö—Ä–∞–Ω–µ–Ω: {checkpoint_file}")
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è checkpoint: {str(e)}")

    def extract_blob_content(self, blob_obj: Any) -> Dict[str, Any]:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ BLOB –ø–æ–ª—è"""
        blob_data: Dict[str, Any] = {
            "field_type": str(type(blob_obj)),
            "size": 0,
            "extraction_methods": [],
            "value": {"content": "", "type": "unknown", "length": 0},
        }

        try:
            # –ë–µ–∑–æ–ø–∞—Å–Ω–æ–µ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ —á–µ—Ä–µ–∑ value –∞—Ç—Ä–∏–±—É—Ç
            if hasattr(blob_obj, "value"):
                try:
                    blob_value = blob_obj.value
                    if isinstance(blob_value, bytes):
                        # –ü—Ä–æ–±—É–µ–º —Ä–∞–∑–Ω—ã–µ –∫–æ–¥–∏—Ä–æ–≤–∫–∏
                        for encoding in ["utf-8", "cp1251", "latin1"]:
                            try:
                                content = blob_value.decode(encoding)
                                blob_data["value"] = {
                                    "content": content,
                                    "type": f"str_{encoding}",
                                    "length": len(content),
                                }
                                blob_data["extraction_methods"].append(
                                    f"value_{encoding}"
                                )
                                blob_data["size"] = len(blob_value)
                                break
                            except UnicodeDecodeError:
                                continue
                        else:
                            # –ï—Å–ª–∏ –≤—Å–µ –∫–æ–¥–∏—Ä–æ–≤–∫–∏ –Ω–µ —Å—Ä–∞–±–æ—Ç–∞–ª–∏, –∏—Å–ø–æ–ª—å–∑—É–µ–º hex
                            blob_data["value"] = {
                                "content": blob_value.hex(),
                                "type": "hex",
                                "length": len(blob_value),
                            }
                            blob_data["extraction_methods"].append("value_hex")
                            blob_data["size"] = len(blob_value)
                    else:
                        # –ï—Å–ª–∏ value –Ω–µ bytes, –∫–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –≤ —Å—Ç—Ä–æ–∫—É
                        blob_data["value"] = {
                            "content": str(blob_value),
                            "type": "str_direct",
                            "length": len(str(blob_value)),
                        }
                        blob_data["extraction_methods"].append("value_direct")
                        blob_data["size"] = len(str(blob_value))
                except StopIteration:
                    # StopIteration - —ç—Ç–æ –Ω–æ—Ä–º–∞–ª—å–Ω–æ–µ –∑–∞–≤–µ—Ä—à–µ–Ω–∏–µ –∏—Ç–µ—Ä–∞—Ç–æ—Ä–∞
                    blob_data["value"] = {
                        "content": "BLOB data (StopIteration)",
                        "type": "blob_stopiteration",
                        "length": 0,
                    }
                    blob_data["extraction_methods"].append("stopiteration")
                    blob_data["size"] = 0
                except Exception as e:
                    # –î—Ä—É–≥–∏–µ –æ—à–∏–±–∫–∏ –ø—Ä–∏ –¥–æ—Å—Ç—É–ø–µ –∫ value
                    blob_data["value"] = {
                        "content": f"BLOB access error: {str(e)}",
                        "type": "blob_error",
                        "length": 0,
                    }
                    blob_data["extraction_methods"].append("value_error")
                    blob_data["size"] = 0
            else:
                # –ï—Å–ª–∏ –Ω–µ—Ç value –∞—Ç—Ä–∏–±—É—Ç–∞, –ø—Ä–æ–±—É–µ–º –¥—Ä—É–≥–∏–µ –º–µ—Ç–æ–¥—ã
                blob_data["value"] = {
                    "content": str(blob_obj),
                    "type": "str_object",
                    "length": len(str(blob_obj)),
                }
                blob_data["extraction_methods"].append("object_str")
                blob_data["size"] = len(str(blob_obj))

        except Exception as e:
            blob_data["error"] = f"–û—à–∏–±–∫–∞ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è: {str(e)}"
            blob_data["value"] = {
                "content": f"ERROR: {str(e)}",
                "type": "error",
                "length": 0,
            }

        return blob_data

    def extract_table_data(
        self, table_name: str, table: Any, max_records: Optional[int] = None
    ) -> List[Dict[str, Any]]:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –¥–∞–Ω–Ω—ã–µ –∏–∑ —Ç–∞–±–ª–∏—Ü—ã —Å –∞–¥–∞–ø—Ç–∏–≤–Ω–æ–π –ª–æ–≥–∏–∫–æ–π"""
        print(f"   üîÑ –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ {table_name}...")
        print(f"      üìä –í—Å–µ–≥–æ –∑–∞–ø–∏—Å–µ–π: {len(table):,}")

        records = []
        successful_records = 0

        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –ª–∏–º–∏—Ç –∑–∞–ø–∏—Å–µ–π
        if max_records is None:
            max_records = len(table)
        else:
            max_records = min(max_records, len(table))

        print(f"      üéØ –ò–∑–≤–ª–µ–∫–∞–µ–º {max_records:,} –∑–∞–ø–∏—Å–µ–π...")
        logger.info(f"üöÄ –ù–∞—á–∏–Ω–∞–µ–º –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ {table_name}: {max_records:,} –∑–∞–ø–∏—Å–µ–π")

        error_count = 0

        for i in range(max_records):
            try:
                row = table[i]

                # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –ø—É—Å—Ç—ã–µ –∑–∞–ø–∏—Å–∏
                if hasattr(row, "is_empty") and row.is_empty:
                    continue

                # –ò–∑–≤–ª–µ–∫–∞–µ–º –¥–∞–Ω–Ω—ã–µ
                row_dict = row.as_dict() if hasattr(row, "as_dict") else {}
                if not row_dict:
                    continue

                # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º —Å—Ç—Ä—É–∫—Ç—É—Ä—É (—Ç–æ–ª—å–∫–æ –¥–ª—è –ø–µ—Ä–≤—ã—Ö –∑–∞–ø–∏—Å–µ–π)
                if i < 3:
                    analysis = self.analyze_table_structure(table_name, row_dict)
                    logger.info(
                        f"üîç –ê–Ω–∞–ª–∏–∑ –∑–∞–ø–∏—Å–∏ {i}: {analysis['total_fields']} –ø–æ–ª–µ–π, "
                        f"{len(analysis['amount_fields'])} —Å—É–º–º, "
                        f"{len(analysis['quantity_fields'])} –∫–æ–ª–∏—á–µ—Å—Ç–≤, "
                        f"{len(analysis['blob_fields'])} BLOB"
                    )

                # –°–æ–∑–¥–∞–µ–º –∑–∞–ø–∏—Å—å
                record: Dict[str, Any] = {
                    "id": f"{table_name}_{i+1}",
                    "table_name": table_name,
                    "row_index": i + 1,
                    "fields": {},
                    "blobs": {},
                    "extraction_stats": {
                        "total_blobs": 0,
                        "successful": 0,
                        "failed": 0,
                    },
                }

                # –ò–∑–≤–ª–µ–∫–∞–µ–º –±–∏–∑–Ω–µ—Å-–ø–æ–ª—è
                for key, value in row_dict.items():
                    if key in self.business_fields:
                        record["fields"][key] = value

                # –ò–∑–≤–ª–µ–∫–∞–µ–º –ø–æ–ª—è —Å —Å—É–º–º–∞–º–∏
                for key, value in row_dict.items():
                    if isinstance(value, (int, float)) and value > 100:
                        record["fields"][f"amount_{key}"] = value

                # –ò–∑–≤–ª–µ–∫–∞–µ–º –ø–æ–ª—è —Å –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞–º–∏
                for key, value in row_dict.items():
                    if isinstance(value, (int, float)) and value <= 100:
                        record["fields"][f"quantity_{key}"] = value

                # –ò–∑–≤–ª–µ–∫–∞–µ–º BLOB –ø–æ–ª—è —Å –¥–µ—Ç–∞–ª—å–Ω—ã–º –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ–º
                for key, value in row_dict.items():
                    if hasattr(value, "__class__") and "Blob" in str(type(value)):
                        try:
                            blob_data = self.extract_blob_content(value)
                            record["blobs"][key] = blob_data
                            record["extraction_stats"]["total_blobs"] += 1
                            if blob_data.get("extraction_methods"):
                                record["extraction_stats"]["successful"] += 1
                            else:
                                record["extraction_stats"]["failed"] += 1
                        except StopIteration:
                            # StopIteration - —ç—Ç–æ –Ω–æ—Ä–º–∞–ª—å–Ω–æ–µ –∑–∞–≤–µ—Ä—à–µ–Ω–∏–µ –∏—Ç–µ—Ä–∞—Ç–æ—Ä–∞
                            blob_data = {
                                "field_type": str(type(value)),
                                "size": 0,
                                "extraction_methods": ["stopiteration"],
                                "value": {
                                    "content": "BLOB data (StopIteration)",
                                    "type": "blob_stopiteration",
                                    "length": 0,
                                },
                            }
                            record["blobs"][key] = blob_data
                            record["extraction_stats"]["total_blobs"] += 1
                            record["extraction_stats"]["successful"] += 1
                            logger.debug(f"‚úÖ BLOB {key} –æ–±—Ä–∞–±–æ—Ç–∞–Ω (StopIteration)")
                        except Exception as e:
                            # –î—Ä—É–≥–∏–µ –æ—à–∏–±–∫–∏ –ø—Ä–∏ –∏–∑–≤–ª–µ—á–µ–Ω–∏–∏ BLOB
                            error_count += 1
                            self.log_error(table_name, i, "BLOB_ERROR", str(e))
                            blob_data = {
                                "field_type": str(type(value)),
                                "size": 0,
                                "extraction_methods": [],
                                "value": {
                                    "content": f"BLOB extraction error: {str(e)}",
                                    "type": "blob_error",
                                    "length": 0,
                                },
                                "error": f"–û—à–∏–±–∫–∞ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è: {str(e)}",
                            }
                            record["blobs"][key] = blob_data
                            record["extraction_stats"]["total_blobs"] += 1
                            record["extraction_stats"]["failed"] += 1

                records.append(record)
                successful_records += 1
                self.extraction_stats["successful_records"] += 1

                # –ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ –ø—Ä–æ–≥—Ä–µ—Å—Å–∞ –∫–∞–∂–¥—ã–µ 1000 –∑–∞–ø–∏—Å–µ–π
                if i > 0 and i % 1000 == 0:
                    self.log_progress(table_name, i, max_records, error_count)

                # –°–æ—Ö—Ä–∞–Ω—è–µ–º checkpoint –∫–∞–∂–¥—ã–µ 10000 –∑–∞–ø–∏—Å–µ–π
                if i > 0 and i % 10000 == 0:
                    self.save_checkpoint(table_name, records)

            except Exception as e:
                error_count += 1
                self.log_error(table_name, i, "RECORD_ERROR", str(e))
                self.extraction_stats["failed_records"] += 1
                continue

        # –§–∏–Ω–∞–ª—å–Ω–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        elapsed_time = time.time() - self.extraction_stats["start_time"]
        logger.info(
            f"‚úÖ {table_name} –∑–∞–≤–µ—Ä—à–µ–Ω–∞: {successful_records:,} –∑–∞–ø–∏—Å–µ–π –∑–∞ {elapsed_time/60:.1f} –º–∏–Ω"
        )
        logger.info(
            f"üìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞: {successful_records:,} —É—Å–ø–µ—à–Ω—ã—Ö, {error_count} –æ—à–∏–±–æ–∫"
        )

        print(f"      ‚úÖ –ò–∑–≤–ª–µ—á–µ–Ω–æ {successful_records:,} –∑–∞–ø–∏—Å–µ–π –∏–∑ {table_name}")
        return records

    def extract_critical_tables(
        self, db: DatabaseReader
    ) -> Dict[str, List[Dict[str, Any]]]:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏–µ —Ç–∞–±–ª–∏—Ü—ã"""
        critical_tables = [
            "_DOCUMENTJOURNAL5354",  # 4,458,509 –∑–∞–ø–∏—Å–µ–π
            "_DOCUMENTJOURNAL5287",  # 2,798,531 –∑–∞–ø–∏—Å–µ–π
            "_DOCUMENTJOURNAL5321",  # 973,975 –∑–∞–ø–∏—Å–µ–π
            "_DOCUMENT138",  # 861,178 –∑–∞–ø–∏—Å–µ–π
            "_DOCUMENT156",  # 571,213 –∑–∞–ø–∏—Å–µ–π
        ]

        results = {}

        for table_name in critical_tables:
            if table_name in db.tables:
                table = db.tables[table_name]

                # –ò–∑–≤–ª–µ–∫–∞–µ–º –í–°–ï –∑–∞–ø–∏—Å–∏ (–ø–æ–ª–Ω–æ–µ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ)
                max_records = len(table)  # –í–°–ï –∑–∞–ø–∏—Å–∏ –∏–∑ —Ç–∞–±–ª–∏—Ü—ã

                records = self.extract_table_data(table_name, table, max_records)
                results[table_name] = records
            else:
                print(f"   ‚ùå –¢–∞–±–ª–∏—Ü–∞ {table_name} –Ω–µ –Ω–∞–π–¥–µ–Ω–∞")

        return results

    def save_to_parquet(self, results: Dict[str, List[Dict[str, Any]]]) -> None:
        """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤ Parquet —Ñ–æ—Ä–º–∞—Ç"""
        if not PARQUET_DUCKDB_AVAILABLE:
            logger.error("‚ùå Parquet/DuckDB –Ω–µ –¥–æ—Å—Ç—É–ø–Ω—ã")
            return

        try:
            logger.info("üíæ –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤ Parquet —Ñ–æ—Ä–º–∞—Ç...")

            # –°–æ–∑–¥–∞–µ–º DataFrame –¥–ª—è –∫–∞–∂–¥–æ–π —Ç–∞–±–ª–∏—Ü—ã
            parquet_files = {}

            for table_name, records in results.items():
                if not records:
                    continue

                # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –∑–∞–ø–∏—Å–∏ –≤ DataFrame
                df_data = []
                for record in records:
                    row_data = {
                        "id": record.get("id"),
                        "table_name": record.get("table_name"),
                        "row_index": record.get("row_index"),
                    }

                    # –î–æ–±–∞–≤–ª—è–µ–º –ø–æ–ª—è
                    for key, value in record.get("fields", {}).items():
                        row_data[f"field_{key}"] = value

                    # –î–æ–±–∞–≤–ª—è–µ–º BLOB –ø–æ–ª—è (—Ç–æ–ª—å–∫–æ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ)
                    for key, blob_data in record.get("blobs", {}).items():
                        row_data[f"blob_{key}_size"] = blob_data.get("size", 0)
                        row_data[f"blob_{key}_type"] = blob_data.get("value", {}).get(
                            "type", "unknown"
                        )

                    # –î–æ–±–∞–≤–ª—è–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –∏–∑–≤–ª–µ—á–µ–Ω–∏—è
                    stats = record.get("extraction_stats", {})
                    row_data["total_blobs"] = stats.get("total_blobs", 0)
                    row_data["successful_blobs"] = stats.get("successful", 0)
                    row_data["failed_blobs"] = stats.get("failed", 0)

                    df_data.append(row_data)

                if df_data:
                    df = pd.DataFrame(df_data)
                    parquet_file = f"complete_1c_database_{table_name}.parquet"
                    df.to_parquet(parquet_file, engine="pyarrow")
                    parquet_files[table_name] = parquet_file
                    logger.info(
                        f"‚úÖ {table_name}: {len(df):,} –∑–∞–ø–∏—Å–µ–π ‚Üí {parquet_file}"
                    )

            # –°–æ–∑–¥–∞–µ–º –æ—Å–Ω–æ–≤–Ω–æ–π Parquet —Ñ–∞–π–ª
            if parquet_files:
                main_parquet = "complete_1c_database.parquet"
                logger.info(f"üìä –°–æ–∑–¥–∞–Ω–∏–µ –æ—Å–Ω–æ–≤–Ω–æ–≥–æ Parquet —Ñ–∞–π–ª–∞: {main_parquet}")
                # –ó–¥–µ—Å—å –º–æ–∂–Ω–æ –æ–±—ä–µ–¥–∏–Ω–∏—Ç—å –≤—Å–µ —Ç–∞–±–ª–∏—Ü—ã –≤ –æ–¥–∏–Ω —Ñ–∞–π–ª

        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –≤ Parquet: {str(e)}")

    def save_to_duckdb(self, results: Dict[str, List[Dict[str, Any]]]) -> None:
        """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤ DuckDB"""
        if not PARQUET_DUCKDB_AVAILABLE:
            logger.error("‚ùå Parquet/DuckDB –Ω–µ –¥–æ—Å—Ç—É–ø–Ω—ã")
            return

        try:
            logger.info("üíæ –°–æ–∑–¥–∞–Ω–∏–µ DuckDB –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö...")

            # –°–æ–∑–¥–∞–µ–º DuckDB —Å–æ–µ–¥–∏–Ω–µ–Ω–∏–µ
            conn = duckdb.connect("complete_1c_database.duckdb")

            for table_name, records in results.items():
                if not records:
                    continue

                logger.info(f"üìä –°–æ–∑–¥–∞–Ω–∏–µ —Ç–∞–±–ª–∏—Ü—ã {table_name} –≤ DuckDB...")

                # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –∑–∞–ø–∏—Å–∏ –≤ DataFrame
                df_data = []
                for record in records:
                    row_data = {
                        "id": record.get("id"),
                        "table_name": record.get("table_name"),
                        "row_index": record.get("row_index"),
                    }

                    # –î–æ–±–∞–≤–ª—è–µ–º –ø–æ–ª—è
                    for key, value in record.get("fields", {}).items():
                        row_data[f"field_{key}"] = value

                    # –î–æ–±–∞–≤–ª—è–µ–º BLOB –ø–æ–ª—è (—Ç–æ–ª—å–∫–æ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ)
                    for key, blob_data in record.get("blobs", {}).items():
                        row_data[f"blob_{key}_size"] = blob_data.get("size", 0)
                        row_data[f"blob_{key}_type"] = blob_data.get("value", {}).get(
                            "type", "unknown"
                        )

                    # –î–æ–±–∞–≤–ª—è–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –∏–∑–≤–ª–µ—á–µ–Ω–∏—è
                    stats = record.get("extraction_stats", {})
                    row_data["total_blobs"] = stats.get("total_blobs", 0)
                    row_data["successful_blobs"] = stats.get("successful", 0)
                    row_data["failed_blobs"] = stats.get("failed", 0)

                    df_data.append(row_data)

                if df_data:
                    df = pd.DataFrame(df_data)
                    # –°–æ–∑–¥–∞–µ–º —Ç–∞–±–ª–∏—Ü—É –≤ DuckDB
                    conn.register(f"df_{table_name}", df)
                    conn.execute(
                        f"CREATE TABLE {table_name} AS SELECT * FROM df_{table_name}"
                    )
                    logger.info(f"‚úÖ {table_name}: {len(df):,} –∑–∞–ø–∏—Å–µ–π ‚Üí DuckDB")

            # –°–æ–∑–¥–∞–µ–º –∏–Ω–¥–µ–∫—Å—ã –¥–ª—è –±—ã—Å—Ç—Ä–æ–≥–æ –ø–æ–∏—Å–∫–∞
            logger.info("üîç –°–æ–∑–¥–∞–Ω–∏–µ –∏–Ω–¥–µ–∫—Å–æ–≤...")
            for table_name in results.keys():
                try:
                    conn.execute(
                        f"CREATE INDEX IF NOT EXISTS idx_{table_name}_id ON {table_name}(id)"
                    )
                    conn.execute(
                        f"CREATE INDEX IF NOT EXISTS idx_{table_name}_table ON {table_name}(table_name)"
                    )
                except Exception as e:
                    logger.warning(
                        f"‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å —Å–æ–∑–¥–∞—Ç—å –∏–Ω–¥–µ–∫—Å –¥–ª—è {table_name}: {str(e)}"
                    )

            conn.close()
            logger.info("‚úÖ DuckDB –±–∞–∑–∞ –¥–∞–Ω–Ω—ã—Ö —Å–æ–∑–¥–∞–Ω–∞: complete_1c_database.duckdb")

        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ —Å–æ–∑–¥–∞–Ω–∏—è DuckDB: {str(e)}")


def main() -> None:
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –∞–¥–∞–ø—Ç–∏–≤–Ω–æ–≥–æ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è"""
    print("üîç –ê–¥–∞–ø—Ç–∏–≤–Ω–æ–µ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏—Ö —Ç–∞–±–ª–∏—Ü")
    print("=" * 60)

    try:
        db_file = open("data/raw/1Cv8.1CD", "rb")
        db = DatabaseReader(db_file)

        print("‚úÖ –ë–∞–∑–∞ –¥–∞–Ω–Ω—ã—Ö –æ—Ç–∫—Ä—ã—Ç–∞ —É—Å–ø–µ—à–Ω–æ!")

        # –°–æ–∑–¥–∞–µ–º –∞–¥–∞–ø—Ç–∏–≤–Ω—ã–π –∏–∑–≤–ª–µ–∫–∞—Ç–µ–ª—å
        extractor = AdaptiveExtractor()

        # –ò–∑–≤–ª–µ–∫–∞–µ–º –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏–µ —Ç–∞–±–ª–∏—Ü—ã
        print("\nüéØ –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏—Ö —Ç–∞–±–ª–∏—Ü...")
        results = extractor.extract_critical_tables(db)

        # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤ JSON
        output_file = "adaptive_extraction_results.json"
        with open(output_file, "w", encoding="utf-8") as f:
            json.dump(results, f, ensure_ascii=False, indent=2, default=str)

        print(f"\n‚úÖ –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ {output_file}")

        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ Parquet –∏ DuckDB
        print("\nüíæ –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤ –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ —Ñ–æ—Ä–º–∞—Ç—ã...")
        extractor.save_to_parquet(results)
        extractor.save_to_duckdb(results)

        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        total_records = sum(len(records) for records in results.values())
        print("\nüìä –ò–¢–û–ì–û–í–ê–Ø –°–¢–ê–¢–ò–°–¢–ò–ö–ê:")
        print(f"   üìÑ –ò–∑–≤–ª–µ—á–µ–Ω–æ {total_records:,} –∑–∞–ø–∏—Å–µ–π –∏–∑ {len(results)} —Ç–∞–±–ª–∏—Ü")

        for table_name, records in results.items():
            print(f"   üìÑ {table_name}: {len(records):,} –∑–∞–ø–∏—Å–µ–π")

        print("\n‚úÖ –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ —É—Å–ø–µ—à–Ω–æ")
        print("üìÅ –°–æ–∑–¥–∞–Ω–Ω—ã–µ —Ñ–∞–π–ª—ã:")
        print("   üìÑ adaptive_extraction_results.json - –ø–æ–ª–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ")
        print("   üìä complete_1c_database_*.parquet - –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ —Ç–∞–±–ª–∏—Ü—ã")
        print("   üóÑÔ∏è complete_1c_database.duckdb - –∏–Ω–¥–µ–∫—Å–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –±–∞–∑–∞ –¥–∞–Ω–Ω—ã—Ö")

    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞: {str(e)}")
        import traceback

        traceback.print_exc()
    finally:
        # –ó–∞–∫—Ä—ã–≤–∞–µ–º —Ñ–∞–π–ª
        if "db_file" in locals():
            db_file.close()


if __name__ == "__main__":
    main()
