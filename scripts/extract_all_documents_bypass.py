#!/usr/bin/env python3
# -*- coding: utf-8 -*-

import json
import os
import sys
from datetime import datetime
import struct
from typing import Optional, List, Any, Dict, Collection

def extract_all_documents_bypass() -> None:
    """
    –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –≤—Å–µ—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤, –æ–±—Ö–æ–¥—è –ø—Ä–æ–±–ª–µ–º—É —Å onec_dtools
    –ò—Å–ø–æ–ª—å–∑—É–µ–º –ø—Ä—è–º–æ–µ —á—Ç–µ–Ω–∏–µ —Ñ–∞–π–ª–∞ –∏ –ø–æ–∏—Å–∫ –ø–æ –ø–∞—Ç—Ç–µ—Ä–Ω–∞–º
    """
    print("üîç –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –í–°–ï–• –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ (–æ–±—Ö–æ–¥ onec_dtools)")
    print("üéØ –¶–ï–õ–¨: –ù–∞–π—Ç–∏ –≤—Å–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã, —Å—á–µ—Ç–∞-—Ñ–∞–∫—Ç—É—Ä—ã, –Ω–∞–∫–ª–∞–¥–Ω—ã–µ, –∞–∫—Ç—ã")
    print("=" * 60)

    results: Dict[str, Any] = {
        "all_documents": [],
        "document_types": {
            "acts": [],
            "invoices": [],
            "waybills": [],
            "retail_sales": [],
            "other_documents": []
        },
        "metadata": {
            "extraction_date": datetime.now().isoformat(),
            "source_file": "data/raw/1Cv8.1CD",
            "total_documents_found": 0,
            "extraction_method": "bypass_onec_dtools"
        },
    }

    try:
        # –ß–∏—Ç–∞–µ–º —Ñ–∞–π–ª –Ω–∞–ø—Ä—è–º—É—é
        with open("data/raw/1Cv8.1CD", "rb") as f:
            print("‚úÖ –§–∞–π–ª –æ—Ç–∫—Ä—ã—Ç –¥–ª—è –ø—Ä—è–º–æ–≥–æ —á—Ç–µ–Ω–∏—è")
            
            # –ß–∏—Ç–∞–µ–º –ø–µ—Ä–≤—ã–µ 100MB –¥–ª—è –ø–æ–∏—Å–∫–∞ –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤
            chunk_size = 1024 * 1024  # 1MB chunks
            total_read = 0
            max_read = 100 * 1024 * 1024  # 100MB
            
            while total_read < max_read:
                chunk = f.read(chunk_size)
                if not chunk:
                    break
                
                total_read += len(chunk)
                print(f"üìä –ü—Ä–æ—á–∏—Ç–∞–Ω–æ: {total_read // (1024*1024)}MB")
                
                # –ò—â–µ–º –ø–∞—Ç—Ç–µ—Ä–Ω—ã –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
                found_documents = search_document_patterns(chunk, total_read - len(chunk))
                results["all_documents"].extend(found_documents)
                
                # –ò—â–µ–º –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–µ —Ç–∏–ø—ã –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
                search_specific_document_types(chunk, results, total_read - len(chunk))

    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞: {e}")
        import traceback
        traceback.print_exc()
        return None

    # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
    with open('data/results/all_documents_bypass.json', 'w', encoding='utf-8') as f:
        json.dump(results, f, ensure_ascii=False, indent=2)

    print(f"\nüìÑ –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤: data/results/all_documents_bypass.json")
    print(f"üìä –í—Å–µ–≥–æ –Ω–∞–π–¥–µ–Ω–æ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤: {len(results['all_documents'])}")
    
    # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ —Ç–∏–ø–∞–º
    document_types = results["document_types"]
    if isinstance(document_types, dict):
        for doc_type, docs in document_types.items():
            if isinstance(docs, list) and docs:
                print(f"   üìã {doc_type}: {len(docs)} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤")

    return None


def search_document_patterns(chunk: bytes, offset: int) -> List[Dict[str, Any]]:
    """
    –ü–æ–∏—Å–∫ –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –≤ –±–∏–Ω–∞—Ä–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
    """
    found_documents: List[Dict[str, Any]] = []
    
    # –ü–∞—Ç—Ç–µ—Ä–Ω—ã –¥–ª—è –ø–æ–∏—Å–∫–∞
    patterns = [
        b"_DOCUMENT",
        b"_NUMBER",
        b"_DATE_TIME",
        b"_POSTED",
        b"_MARKED",
        b"_VERSION",
        b"_FLD",
        b"_IDRREF",
        b"_DOCUMENTTREF",
        b"_DOCUMENTRREF"
    ]
    
    for pattern in patterns:
        pos = 0
        while True:
            pos = chunk.find(pattern, pos)
            if pos == -1:
                break
            
            # –ò–∑–≤–ª–µ–∫–∞–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç –≤–æ–∫—Ä—É–≥ –Ω–∞–π–¥–µ–Ω–Ω–æ–≥–æ –ø–∞—Ç—Ç–µ—Ä–Ω–∞
            start = max(0, pos - 100)
            end = min(len(chunk), pos + 200)
            context = chunk[start:end]
            
            document = {
                "pattern": pattern.decode('utf-8', errors='ignore'),
                "offset": offset + pos,
                "context": context.decode('utf-8', errors='ignore'),
                "context_hex": context.hex()
            }
            found_documents.append(document)
            
            pos += len(pattern)
    
    return found_documents


def search_specific_document_types(chunk: bytes, results: Dict[str, Any], offset: int) -> None:
    """
    –ü–æ–∏—Å–∫ –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã—Ö —Ç–∏–ø–æ–≤ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
    """
    # –ü–æ–∏—Å–∫ –∞–∫—Ç–æ–≤ –≤—ã–ø–æ–ª–Ω–µ–Ω–Ω—ã—Ö —Ä–∞–±–æ—Ç
    if b"_DOCUMENT163" in chunk:
        results["document_types"]["acts"].append({
            "type": "act",
            "table": "_DOCUMENT163",
            "offset": offset,
            "found_in_chunk": True
        })
    
    # –ü–æ–∏—Å–∫ —Å—á–µ—Ç–æ–≤-—Ñ–∞–∫—Ç—É—Ä
    if b"_DOCUMENT184" in chunk:
        results["document_types"]["invoices"].append({
            "type": "invoice",
            "table": "_DOCUMENT184", 
            "offset": offset,
            "found_in_chunk": True
        })
    
    # –ü–æ–∏—Å–∫ –Ω–∞–∫–ª–∞–¥–Ω—ã—Ö
    if b"_DOCUMENT154" in chunk:
        results["document_types"]["waybills"].append({
            "type": "waybill",
            "table": "_DOCUMENT154",
            "offset": offset,
            "found_in_chunk": True
        })
    
    # –ü–æ–∏—Å–∫ —Ä–æ–∑–Ω–∏—á–Ω—ã—Ö –ø—Ä–æ–¥–∞–∂
    if b"_DOCUMENT184" in chunk and b"Roznichnaya" in chunk:
        results["document_types"]["retail_sales"].append({
            "type": "retail_sale",
            "table": "_DOCUMENT184",
            "offset": offset,
            "found_in_chunk": True
        })
    
    # –ü–æ–∏—Å–∫ –¥—Ä—É–≥–∏—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
    other_patterns = [b"_DOCUMENT137", b"_DOCUMENT12259", b"_DOCUMENT13139"]
    for pattern in other_patterns:
        if pattern in chunk:
            results["document_types"]["other_documents"].append({
                "type": "other",
                "table": pattern.decode('utf-8'),
                "offset": offset,
                "found_in_chunk": True
            })


if __name__ == "__main__":
    extract_all_documents_bypass()
