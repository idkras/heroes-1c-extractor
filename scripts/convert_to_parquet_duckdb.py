#!/usr/bin/env python3

"""
–ö–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è JSON –¥–∞–Ω–Ω—ã—Ö –≤ Parquet + DuckDB –¥–ª—è –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞
"""

import json
import os
import sys
from typing import Any

import duckdb
import pandas as pd

# –î–æ–±–∞–≤–ª—è–µ–º –ø—É—Ç—å –∫ src
sys.path.insert(0, os.path.join(os.path.dirname(__file__), "..", "src"))


def load_json_data(json_file: str) -> dict[str, Any]:
    """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –¥–∞–Ω–Ω—ã–µ –∏–∑ JSON —Ñ–∞–π–ª–∞"""
    print(f"üìÅ –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö –∏–∑ {json_file}...")

    if not os.path.exists(json_file):
        print(f"‚ùå –§–∞–π–ª {json_file} –Ω–µ –Ω–∞–π–¥–µ–Ω")
        return {}

    try:
        with open(json_file, encoding="utf-8") as f:
            data = json.load(f)
        print(f"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(data.get('documents', []))} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤")
        return data
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ {json_file}: {e}")
        return {}


def convert_documents_to_dataframe(documents: list[dict[str, Any]]) -> pd.DataFrame:
    """–ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ—Ç –¥–æ–∫—É–º–µ–Ω—Ç—ã –≤ DataFrame"""
    print("üîÑ –ö–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –≤ DataFrame...")

    rows = []
    for doc in documents:
        # –û—Å–Ω–æ–≤–Ω—ã–µ –ø–æ–ª—è
        row = {
            "id": doc.get("id", ""),
            "table_name": doc.get("table_name", ""),
            "row_index": doc.get("row_index", 0),
        }

        # –ò–°–ü–†–ê–í–õ–ï–ù–û: –°–æ–∑–¥–∞–µ–º –±–∏–∑–Ω–µ—Å-–ø–æ–ª—è –∏–∑ —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏—Ö –ø–æ–ª–µ–π
        if "fields" in doc:
            fields = doc["fields"]

            # –ò–∑–≤–ª–µ–∫–∞–µ–º –±–∏–∑–Ω–µ—Å-–ø–æ–ª—è –∏–∑ —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏—Ö –ø–æ–ª–µ–π
            row["document_number"] = fields.get("_NUMBER", "N/A")

            # –ò–°–ü–†–ê–í–õ–ï–ù–û: –ò—â–µ–º –¥–∞—Ç—É –≤ —Ä–∞–∑–Ω—ã—Ö –ø–æ–ª—è—Ö
            date_value = "N/A"
            for field_name, field_value in fields.items():
                if isinstance(field_value, str) and len(field_value) > 10:
                    # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —Å–æ–¥–µ—Ä–∂–∏—Ç –ª–∏ –ø–æ–ª–µ –¥–∞—Ç—É
                    if any(char.isdigit() for char in field_value) and any(
                        char in field_value for char in [".", "-", "/"]
                    ):
                        date_value = field_value
                        break
            row["document_date"] = date_value

            row["total_amount"] = fields.get("_FLD4239", 0)
            row["quantity"] = fields.get("_FLD4238", 0)
            row["unit_measure"] = fields.get("_FLD4240", 1)
            row["posted"] = fields.get("_POSTED", False)
            row["marked"] = fields.get("_MARKED", False)

            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ç–∏–ø –¥–æ–∫—É–º–µ–Ω—Ç–∞ –ø–æ table_name
            table_name = doc.get("table_name", "")
            if "JOURNAL" in table_name:
                row["document_type"] = "–ñ–£–†–ù–ê–õ"
            elif "DOCUMENT138" in table_name:
                row["document_type"] = "–ü–ï–†–ï–ú–ï–©–ï–ù–ò–ï"
            elif "DOCUMENT156" in table_name:
                row["document_type"] = "–†–ï–ê–õ–ò–ó–ê–¶–ò–Ø"
            elif "DOCUMENT184" in table_name:
                row["document_type"] = "–°–ß–ï–¢-–§–ê–ö–¢–£–†–ê"
            else:
                row["document_type"] = "–î–û–ö–£–ú–ï–ù–¢"

            # –ò–°–ü–†–ê–í–õ–ï–ù–û: –ò–∑–≤–ª–µ–∫–∞–µ–º –º–∞–≥–∞–∑–∏–Ω –∏–∑ BLOB –ø–æ–ª–µ–π
            row["store_name"] = "N/A"
            if "blobs" in doc:
                for blob_name, blob_data in doc["blobs"].items():
                    if isinstance(blob_data, dict):
                        # –ò—â–µ–º —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ –≤ value.content
                        if "value" in blob_data and isinstance(
                            blob_data["value"],
                            dict,
                        ):
                            content = blob_data["value"].get("content", "")
                        elif "content" in blob_data:
                            content = blob_data["content"]
                        else:
                            continue

                        if isinstance(content, str) and content.strip():
                            # –ò—â–µ–º –º–∞–≥–∞–∑–∏–Ω—ã –≤ —Å–æ–¥–µ—Ä–∂–∏–º–æ–º
                            if "–ü–¶022" in content:
                                row["store_name"] = "–ü–¶022 (–ß–µ—Ö–æ–≤—Å–∫–∏–π)"
                            elif "–ü–¶036" in content:
                                row["store_name"] = "–ü–¶036 (–Æ–∂–Ω—ã–π)"
                            elif "–ë—Ä–∞—Ç–∏—Å–ª–∞–≤—Å–∫–∏–π" in content:
                                row["store_name"] = "–ë—Ä–∞—Ç–∏—Å–ª–∞–≤—Å–∫–∏–π"
                            elif "–Æ–∂–Ω—ã–π" in content:
                                row["store_name"] = "–Æ–∂–Ω—ã–π"
                            elif "–ß–µ—Ö–æ–≤—Å–∫–∏–π" in content:
                                row["store_name"] = "–ß–µ—Ö–æ–≤—Å–∫–∏–π"
                            break

        # BLOB –ø–æ–ª—è –¥–ª—è –ø–æ–∏—Å–∫–∞
        if "blobs" in doc:
            blob_content = ""
            for blob_name, blob_data in doc["blobs"].items():
                if isinstance(blob_data, dict):
                    # –ò–°–ü–†–ê–í–õ–ï–ù–û: –ü—Ä–∞–≤–∏–ª—å–Ω–æ–µ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ BLOB —Å–æ–¥–µ—Ä–∂–∏–º–æ–≥–æ
                    if "value" in blob_data and isinstance(blob_data["value"], dict):
                        value_data = blob_data["value"]
                        if "content" in value_data and isinstance(
                            value_data["content"],
                            str,
                        ):
                            content = value_data["content"]
                            if content and content.strip():
                                blob_content += content + " "
                    elif "content" in blob_data and isinstance(
                        blob_data["content"],
                        str,
                    ):
                        content = blob_data["content"]
                        if content and content.strip():
                            blob_content += content + " "
            row["blob_content"] = blob_content.strip()
        else:
            row["blob_content"] = ""

        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è
        if "extraction_stats" in doc:
            stats = doc["extraction_stats"]
            row["total_blobs"] = stats.get("total_blobs", 0)
            row["successful_blobs"] = stats.get("successful", 0)
            row["failed_blobs"] = stats.get("failed", 0)

        rows.append(row)

    df = pd.DataFrame(rows)
    print(f"‚úÖ –°–æ–∑–¥–∞–Ω DataFrame —Å {len(df)} —Å—Ç—Ä–æ–∫–∞–º–∏ –∏ {len(df.columns)} —Å—Ç–æ–ª–±—Ü–∞–º–∏")
    return df


def save_to_parquet(df: pd.DataFrame, output_file: str) -> None:
    """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç DataFrame –≤ Parquet —Ñ–æ—Ä–º–∞—Ç"""
    print(f"üíæ –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤ Parquet: {output_file}...")

    try:
        # –°–æ–∑–¥–∞–µ–º –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é –µ—Å–ª–∏ –Ω–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç
        os.makedirs(os.path.dirname(output_file), exist_ok=True)

        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ Parquet
        df.to_parquet(output_file, compression="snappy", index=False)

        # –ü–æ–ª—É—á–∞–µ–º —Ä–∞–∑–º–µ—Ä —Ñ–∞–π–ª–∞
        file_size = os.path.getsize(output_file)
        print(f"‚úÖ Parquet —Ñ–∞–π–ª —Å–æ—Ö—Ä–∞–Ω–µ–Ω: {file_size / 1024 / 1024:.2f} MB")

    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è Parquet: {e}")


def create_duckdb_database(parquet_file: str, db_file: str) -> None:
    """–°–æ–∑–¥–∞–µ—Ç DuckDB –±–∞–∑—É –¥–∞–Ω–Ω—ã—Ö –∏–∑ Parquet —Ñ–∞–π–ª–∞"""
    print(f"üóÑÔ∏è –°–æ–∑–¥–∞–Ω–∏–µ DuckDB –±–∞–∑—ã: {db_file}...")

    try:
        # –ü–æ–¥–∫–ª—é—á–∞–µ–º—Å—è –∫ DuckDB
        conn = duckdb.connect(db_file)

        # –°–æ–∑–¥–∞–µ–º —Ç–∞–±–ª–∏—Ü—É –∏–∑ Parquet —Ñ–∞–π–ª–∞
        conn.execute(
            f"""
            CREATE TABLE documents AS 
            SELECT * FROM read_parquet('{parquet_file}')
        """,
        )

        # –°–æ–∑–¥–∞–µ–º –∏–Ω–¥–µ–∫—Å—ã –¥–ª—è –±—ã—Å—Ç—Ä–æ–≥–æ –ø–æ–∏—Å–∫–∞
        print("üîç –°–æ–∑–¥–∞–Ω–∏–µ –∏–Ω–¥–µ–∫—Å–æ–≤...")

        # –ò–Ω–¥–µ–∫—Å –ø–æ table_name
        conn.execute("CREATE INDEX idx_table_name ON documents(table_name)")

        # –ò–Ω–¥–µ–∫—Å –ø–æ row_index
        conn.execute("CREATE INDEX idx_row_index ON documents(row_index)")

        # –ò–Ω–¥–µ–∫—Å –ø–æ BLOB –ø–æ–ª—è–º (–µ—Å–ª–∏ –µ—Å—Ç—å)
        blob_columns = [
            col
            for col in conn.execute("DESCRIBE documents").fetchall()
            if col[0].startswith("blob_")
        ]
        for col_name, _, _, _, _, _ in blob_columns:
            try:
                conn.execute(f"CREATE INDEX idx_{col_name} ON documents({col_name})")
            except:
                pass  # –ò–≥–Ω–æ—Ä–∏—Ä—É–µ–º –æ—à–∏–±–∫–∏ —Å–æ–∑–¥–∞–Ω–∏—è –∏–Ω–¥–µ–∫—Å–æ–≤ –¥–ª—è BLOB –ø–æ–ª–µ–π

        # –ó–∞–∫—Ä—ã–≤–∞–µ–º —Å–æ–µ–¥–∏–Ω–µ–Ω–∏–µ
        conn.close()

        # –ü–æ–ª—É—á–∞–µ–º —Ä–∞–∑–º–µ—Ä —Ñ–∞–π–ª–∞
        file_size = os.path.getsize(db_file)
        print(f"‚úÖ DuckDB –±–∞–∑–∞ —Å–æ–∑–¥–∞–Ω–∞: {file_size / 1024 / 1024:.2f} MB")

    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ —Å–æ–∑–¥–∞–Ω–∏—è DuckDB: {e}")


def create_analysis_queries(db_file: str) -> None:
    """–°–æ–∑–¥–∞–µ—Ç SQL –∑–∞–ø—Ä–æ—Å—ã –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –¥–∞–Ω–Ω—ã—Ö"""
    print("üìä –°–æ–∑–¥–∞–Ω–∏–µ SQL –∑–∞–ø—Ä–æ—Å–æ–≤ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞...")

    try:
        conn = duckdb.connect(db_file)

        # –ê–Ω–∞–ª–∏–∑ –ø–æ —Ç–∞–±–ª–∏—Ü–∞–º
        print("\nüìã –ê–Ω–∞–ª–∏–∑ –ø–æ —Ç–∞–±–ª–∏—Ü–∞–º:")
        result = conn.execute(
            """
            SELECT 
                table_name,
                COUNT(*) as total_records,
                COUNT(DISTINCT id) as unique_documents
            FROM documents 
            GROUP BY table_name 
            ORDER BY total_records DESC
            LIMIT 10
        """,
        ).fetchall()

        for row in result:
            print(f"  {row[0]}: {row[1]:,} –∑–∞–ø–∏—Å–µ–π, {row[2]:,} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤")

        # –ê–Ω–∞–ª–∏–∑ BLOB –ø–æ–ª–µ–π
        print("\nüìù –ê–Ω–∞–ª–∏–∑ BLOB –ø–æ–ª–µ–π:")
        blob_columns = [
            col
            for col in conn.execute("DESCRIBE documents").fetchall()
            if col[0].startswith("blob_")
        ]
        for col_name, _, _, _, _, _ in blob_columns:
            result = conn.execute(
                f"""
                SELECT 
                    COUNT(*) as total_records,
                    COUNT({col_name}) as non_null_records,
                    AVG(LENGTH({col_name})) as avg_length
                FROM documents 
                WHERE {col_name} IS NOT NULL
            """,
            ).fetchone()

            if result:
                print(
                    f"  {col_name}: {result[0]:,} –∑–∞–ø–∏—Å–µ–π, {result[1]:,} –Ω–µ –ø—É—Å—Ç—ã—Ö, —Å—Ä–µ–¥–Ω—è—è –¥–ª–∏–Ω–∞ {result[2]:.1f}",
                )

        # –ü–æ–∏—Å–∫ —Ü–≤–µ—Ç–æ–≤ –≤ BLOB –ø–æ–ª—è—Ö
        print("\nüå∏ –ü–æ–∏—Å–∫ —Ü–≤–µ—Ç–æ–≤ –≤ BLOB –ø–æ–ª—è—Ö:")
        colors = ["—Ä–æ–∑–æ–≤", "–≥–æ–ª—É–±", "–∫—Ä–∞—Å–Ω", "–±–µ–ª", "–∂–µ–ª—Ç"]
        for color in colors:
            result = conn.execute(
                f"""
                SELECT COUNT(*) as count
                FROM documents 
                WHERE blob_content LIKE '%{color}%'
            """,
            ).fetchone()

            if result and result[0] > 0:
                print(f"  {color}: {result[0]:,} —É–ø–æ–º–∏–Ω–∞–Ω–∏–π")

        # –ü–æ–∏—Å–∫ —Ç–∏–ø–æ–≤ –±—É–∫–µ—Ç–æ–≤
        print("\nüíê –ü–æ–∏—Å–∫ —Ç–∏–ø–æ–≤ –±—É–∫–µ—Ç–æ–≤:")
        bouquet_types = ["—Ñ–ª–æ—Ä–∏—Å—Ç–∏—á–µ—Å–∫–∏–π", "–º–æ–Ω–æ", "—è–Ω–¥–µ–∫—Å", "–∫–æ–º–ø–æ–∑–∏—Ü–∏—è"]
        for bouquet_type in bouquet_types:
            result = conn.execute(
                f"""
                SELECT COUNT(*) as count
                FROM documents 
                WHERE blob_content LIKE '%{bouquet_type}%'
            """,
            ).fetchone()

            if result and result[0] > 0:
                print(f"  {bouquet_type}: {result[0]:,} —É–ø–æ–º–∏–Ω–∞–Ω–∏–π")

        # –ü–æ–∏—Å–∫ –º–∞–≥–∞–∑–∏–Ω–æ–≤
        print("\nüè™ –ü–æ–∏—Å–∫ –º–∞–≥–∞–∑–∏–Ω–æ–≤:")
        stores = ["–ü–¶022", "–ü–¶036", "–ë—Ä–∞—Ç–∏—Å–ª–∞–≤—Å–∫–∏–π", "–Æ–∂–Ω—ã–π"]
        for store in stores:
            result = conn.execute(
                f"""
                SELECT COUNT(*) as count
                FROM documents 
                WHERE blob_content LIKE '%{store}%'
            """,
            ).fetchone()

            if result and result[0] > 0:
                print(f"  {store}: {result[0]:,} —É–ø–æ–º–∏–Ω–∞–Ω–∏–π")

        conn.close()

    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ —Å–æ–∑–¥–∞–Ω–∏—è SQL –∑–∞–ø—Ä–æ—Å–æ–≤: {e}")


def main():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è"""
    print("üöÄ –ö–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è JSON –≤ Parquet + DuckDB")
    print("=" * 50)

    # –ü—É—Ç–∏ –∫ —Ñ–∞–π–ª–∞–º
    json_file = "all_available_data.json"
    parquet_file = "data/results/heroes_1c_data.parquet"
    db_file = "data/results/heroes_1c_data.duckdb"

    # –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–∞–Ω–Ω—ã–µ
    data = load_json_data(json_file)
    if not data:
        print("‚ùå –ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –∫–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏–∏")
        return

    # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –¥–æ–∫—É–º–µ–Ω—Ç—ã
    documents = data.get("documents", [])
    if not documents:
        print("‚ùå –ù–µ—Ç –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –¥–ª—è –∫–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏–∏")
        return

    # –°–æ–∑–¥–∞–µ–º DataFrame
    df = convert_documents_to_dataframe(documents)

    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ Parquet
    save_to_parquet(df, parquet_file)

    # –°–æ–∑–¥–∞–µ–º DuckDB –±–∞–∑—É
    create_duckdb_database(parquet_file, db_file)

    # –°–æ–∑–¥–∞–µ–º SQL –∑–∞–ø—Ä–æ—Å—ã –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
    create_analysis_queries(db_file)

    print("\n‚úÖ –ö–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞!")
    print(f"üìÅ Parquet —Ñ–∞–π–ª: {parquet_file}")
    print(f"üóÑÔ∏è DuckDB –±–∞–∑–∞: {db_file}")
    print("\nüîç –î–ª—è –∞–Ω–∞–ª–∏–∑–∞ –∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ:")
    print(
        f"   python -c \"import duckdb; conn = duckdb.connect('{db_file}'); print(conn.execute('SELECT COUNT(*) FROM documents').fetchone())\"",
    )


if __name__ == "__main__":
    main()
