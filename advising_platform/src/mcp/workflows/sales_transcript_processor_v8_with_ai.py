#!/usr/bin/env python3
"""
Sales Transcript Processor v8.0 - OPENAI GPT-4.1 MINI POWERED VERSION
–ö–†–ò–¢–ò–ß–ï–°–ö–ò–ï –£–õ–£–ß–®–ï–ù–ò–Ø:
1. –ò—Å–ø–æ–ª—å–∑—É–µ—Ç OpenAI GPT-4.1 mini –¥–ª—è –ò–ù–î–ò–í–ò–î–£–ê–õ–¨–ù–û–ì–û –∞–Ω–∞–ª–∏–∑–∞ –∫–∞–∂–¥–æ–≥–æ —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ç–∞
2. –ë–ê–¢–ß-–û–ë–†–ê–ë–û–¢–ö–ê: –æ—Ç–ø—Ä–∞–≤–ª—è–µ—Ç 5 —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ç–æ–≤ –∑–∞ —Ä–∞–∑, –ø–æ–ª—É—á–∞–µ—Ç 5 –∞–Ω–∞–ª–∏–∑–æ–≤
3. –§–û–ö–£–° –ù–ê JTBD –û–®–ò–ë–ö–ò: –≤—ã—è–≤–ª—è–µ—Ç –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–µ Big/Medium/Small JTBD –≥–¥–µ –ø—Ä–æ–¥–∞–≤–µ—Ü –¥–æ–ø—É—Å—Ç–∏–ª –æ—à–∏–±–∫—É
4. –ò—Å–ø—Ä–∞–≤–ª—è–µ—Ç –≥–ª–∞–≤–Ω—É—é –ø—Ä–æ–±–ª–µ–º—É v7: —à–∞–±–ª–æ–Ω–Ω–æ—Å—Ç—å –∞–Ω–∞–ª–∏–∑–∞ –∑–∞–º–µ–Ω–µ–Ω–∞ –Ω–∞ —Ä–µ–∞–ª—å–Ω–æ–µ AI –ø–æ–Ω–∏–º–∞–Ω–∏–µ
"""

import logging
import re
import os
import json
from datetime import datetime
from typing import Dict, List, Optional, Tuple
from concurrent.futures import ThreadPoolExecutor
import openai

# Setup logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class SalesTranscriptProcessorV8WithOpenAI:
    """
    OPENAI GPT-4.1 MINI POWERED –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä v8.0
    –ö–†–ò–¢–ò–ß–ï–°–ö–ò–ï –£–õ–£–ß–®–ï–ù–ò–Ø:
    - –ò—Å–ø–æ–ª—å–∑—É–µ—Ç OpenAI GPT-4.1 mini –¥–ª—è –ë–ê–¢–ß-–∞–Ω–∞–ª–∏–∑–∞ 5 —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ç–æ–≤ –∑–∞ —Ä–∞–∑
    - –§–û–ö–£–° –ù–ê JTBD –û–®–ò–ë–ö–ò: –≤—ã—è–≤–ª—è–µ—Ç –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–µ Big/Medium/Small JTBD –≥–¥–µ –ø—Ä–æ–¥–∞–≤–µ—Ü –æ—à–∏–±—Å—è
    - –†–µ–∞–ª—å–Ω–æ–µ –ø–æ–Ω–∏–º–∞–Ω–∏–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –∫–∞–∂–¥–æ–≥–æ —Ä–∞–∑–≥–æ–≤–æ—Ä–∞
    - –°–ø–µ—Ü–∏—Ñ–∏—á–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –≤–º–µ—Å—Ç–æ —à–∞–±–ª–æ–Ω–Ω—ã—Ö –æ—Ç–≤–µ—Ç–æ–≤
    """
    
    # –¢–û–ß–ù–ê–Ø —Å—Ç—Ä—É–∫—Ç—É—Ä–∞ —Å–æ–≥–ª–∞—Å–Ω–æ sales.injury standard v1.1 (14 –∫–æ–ª–æ–Ω–æ–∫)
    REQUIRED_COLUMNS = [
        'transcript',
        'lead_inquiry', 
        'when_trigger_situation',
        'root cause 5why',
        'sale blockers',
        'segment',
        'stop_words_patterns',
        'recommended_phrases', 
        'what client get on this stage',
        'big jtbd',
        'medium jtbd',
        'small jtbd',
        'date_time',
        'week'
    ]
    
    def __init__(self):
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è OpenAI –∫–ª–∏–µ–Ω—Ç–∞
        api_key = os.environ.get('OPENAI_API_KEY')
        if not api_key:
            logger.warning("‚ö†Ô∏è OPENAI_API_KEY –Ω–µ –Ω–∞–π–¥–µ–Ω. AI –∞–Ω–∞–ª–∏–∑ –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω.")
            self.ai_client = None
        else:
            self.ai_client = openai.OpenAI(api_key=api_key)
            logger.info("‚úÖ AI –∫–ª–∏–µ–Ω—Ç OpenAI GPT-4.1 mini –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω")
            
        # –ó–∞–≥—Ä—É–∂–∞–µ–º reference table
        self.load_reference_table()
        
        # TDD VALIDATION SETUP: —Ü–µ–ª–µ–≤—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã Google Sheets
        self.target_spreadsheet_id = "1KQ7eP472By9BBR3yOStE9oJNxxcErNXp73OCbDU6oyc"
        self.target_worksheet_gid = "514375575" # –¢–û–ß–ù–´–ô GID –∏–∑ URL –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
        self.tsv_output_file = "[rick.ai] clients/avtoall.ru/[4] whatsapp-jtbd-tracktion/results/avtoall_sales_analyzed_v8_ai.tsv"
    
    def load_reference_table(self):
        """–ó–∞–≥—Ä—É–∂–∞–µ—Ç reference table –∏–∑ —Ñ–∞–π–ª–∞"""
        try:
            reference_file = "[rick.ai] clients/avtoall.ru/[4] whatsapp-jtbd-tracktion/avtoall_jtbd_analysis_16_jul_2025.md"
            with open(reference_file, 'r', encoding='utf-8') as f:
                self.reference_table_content = f.read()
            logger.info("‚úÖ Reference table –∑–∞–≥—Ä—É–∂–µ–Ω–∞")
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å reference table: {e}")
            self.reference_table_content = ""
    
    def process_batch_with_openai(self, transcripts_batch: List[Tuple[str, str]]) -> List[str]:
        """
        –ë–ê–¢–ß-–û–ë–†–ê–ë–û–¢–ö–ê 5 —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ç–æ–≤ —Å OpenAI GPT-4.1 mini
        –§–û–ö–£–°: –≤—ã—è–≤–ª–µ–Ω–∏–µ –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã—Ö JTBD –æ—à–∏–±–æ–∫ –ø—Ä–æ–¥–∞–≤—Ü–æ–≤
        
        Args:
            transcripts_batch: –°–ø–∏—Å–æ–∫ –∏–∑ 5 –∫–æ—Ä—Ç–µ–∂–µ–π (timestamp, transcript_text)
            
        Returns:
            List[str]: –°–ø–∏—Å–æ–∫ –∏–∑ 5 TSV —Å—Ç—Ä–æ–∫ —Å –∞–Ω–∞–ª–∏–∑–æ–º JTBD –æ—à–∏–±–æ–∫
        """
        logger.info(f"üß† BATCH Processing {len(transcripts_batch)} transcripts with OpenAI GPT-4.1 mini")
        logger.info("üéØ FOCUS: Identifying specific Big/Medium/Small JTBD errors by sales representatives")
        
        if not self.ai_client:
            logger.error("‚ùå OpenAI –∫–ª–∏–µ–Ω—Ç –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω")
            return [self._fallback_processing(transcript, timestamp) for timestamp, transcript in transcripts_batch]
        
        try:
            # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º batch prompt –¥–ª—è AI –∞–Ω–∞–ª–∏–∑–∞ 5 —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ç–æ–≤
            batch_prompt = self._build_batch_analysis_prompt(transcripts_batch)
            
            # –ó–∞–ø—Ä–æ—Å –∫ OpenAI GPT-4.1 mini
            response = self.ai_client.chat.completions.create(
                model="gpt-4o-mini",  # GPT-4.1 mini —ç–∫–≤–∏–≤–∞–ª–µ–Ω—Ç
                messages=[
                    {
                        "role": "system",
                        "content": "–¢—ã —ç–∫—Å–ø–µ—Ä—Ç –ø–æ –∞–Ω–∞–ª–∏–∑—É –ø—Ä–æ–¥–∞–∂ –≤ –∞–≤—Ç–æ–∑–∞–ø—á–∞—Å—Ç—è—Ö. –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—à—å —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ç—ã –∑–≤–æ–Ω–∫–æ–≤ –¥–ª—è –≤—ã—è–≤–ª–µ–Ω–∏—è –ö–û–ù–ö–†–ï–¢–ù–´–• JTBD –æ—à–∏–±–æ–∫ –ø—Ä–æ–¥–∞–≤—Ü–æ–≤."
                    },
                    {
                        "role": "user", 
                        "content": batch_prompt
                    }
                ],
                max_tokens=8000,  # –ë–æ–ª—å—à–µ —Ç–æ–∫–µ–Ω–æ–≤ –¥–ª—è –±–∞—Ç—á-–æ–±—Ä–∞–±–æ—Ç–∫–∏
                temperature=0.2,  # –ù–∏–∑–∫–∞—è —Ç–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∞ –¥–ª—è —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏
                response_format={"type": "json_object"}  # –¢—Ä–µ–±—É–µ–º JSON –æ—Ç–≤–µ—Ç
            )
            
            # –ò–∑–≤–ª–µ–∫–∞–µ–º –∞–Ω–∞–ª–∏–∑ –∏–∑ –æ—Ç–≤–µ—Ç–∞ AI
            ai_analysis = response.choices[0].message.content
            if not ai_analysis:
                logger.error("‚ùå –ü—É—Å—Ç–æ–π –æ—Ç–≤–µ—Ç –æ—Ç OpenAI")
                return [self._fallback_processing(transcript, timestamp) for timestamp, transcript in transcripts_batch]
                
            logger.info("‚úÖ OpenAI batch –∞–Ω–∞–ª–∏–∑ –ø–æ–ª—É—á–µ–Ω")
            
            # –ü–∞—Ä—Å–∏–º AI –æ—Ç–≤–µ—Ç –∏ —Ñ–æ—Ä–º–∏—Ä—É–µ–º 5 TSV —Å—Ç—Ä–æ–∫
            tsv_rows = self._parse_batch_ai_analysis_to_tsv(ai_analysis, transcripts_batch)
            
            logger.info(f"‚úÖ Generated {len(tsv_rows)} AI-powered TSV rows with JTBD error analysis")
            return tsv_rows
            
        except Exception as e:
            logger.error(f"‚ùå Error in OpenAI batch processing: {e}")
            return [self._fallback_processing(transcript, timestamp) for timestamp, transcript in transcripts_batch]
    
    def _build_batch_analysis_prompt(self, transcripts_batch: List[Tuple[str, str]]) -> str:
        """–°—Ç—Ä–æ–∏—Ç batch prompt –¥–ª—è AI –∞–Ω–∞–ª–∏–∑–∞ 5 —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ç–æ–≤ —Å —Ñ–æ–∫—É—Å–æ–º –Ω–∞ JTBD –æ—à–∏–±–∫–∏"""
        
        # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ç—ã –¥–ª—è –±–∞—Ç—á-–∞–Ω–∞–ª–∏–∑–∞
        transcripts_for_prompt = ""
        for i, (timestamp, transcript) in enumerate(transcripts_batch, 1):
            transcripts_for_prompt += f"""
=== –¢–†–ê–ù–°–ö–†–ò–ü–¢ {i} ===
Timestamp: {timestamp}
Content: {transcript[:1000]}...
"""
        
        prompt = f"""
–¢—ã —ç–∫—Å–ø–µ—Ä—Ç –ø–æ –∞–Ω–∞–ª–∏–∑—É –ø—Ä–æ–¥–∞–∂ –≤ –∞–≤—Ç–æ–∑–∞–ø—á–∞—Å—Ç—è—Ö. –ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä—É–π —ç—Ç–∏ 5 –ö–û–ù–ö–†–ï–¢–ù–´–• —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ç–æ–≤ –∑–≤–æ–Ω–∫–æ–≤ –¥–ª—è –≤—ã—è–≤–ª–µ–Ω–∏—è JTBD –æ—à–∏–±–æ–∫ –ø—Ä–æ–¥–∞–≤—Ü–æ–≤.

–ö–õ–Æ–ß–ï–í–ê–Ø –ó–ê–î–ê–ß–ê: –î–ª—è –∫–∞–∂–¥–æ–≥–æ —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ç–∞ –Ω–∞–π–¥–∏ –ö–û–ù–ö–†–ï–¢–ù–´–ï Big/Medium/Small JTBD –≥–¥–µ –ü–†–û–î–ê–í–ï–¶ –î–û–ü–£–°–¢–ò–õ –û–®–ò–ë–ö–£.

–¢–†–ê–ù–°–ö–†–ò–ü–¢–´ –î–õ–Ø –ê–ù–ê–õ–ò–ó–ê:
{transcripts_for_prompt}

REFERENCE TABLE –î–õ–Ø JTBD MAPPING:
{self.reference_table_content[:1500]}

–ö–†–ò–¢–ò–ß–ï–°–ö–ò–ï –¢–†–ï–ë–û–í–ê–ù–ò–Ø:
1. –ê–Ω–∞–ª–∏–∑–∏—Ä—É–π –ö–ê–ñ–î–´–ô —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ç –ò–ù–î–ò–í–ò–î–£–ê–õ–¨–ù–û - –Ω–µ –∏—Å–ø–æ–ª—å–∑—É–π —à–∞–±–ª–æ–Ω–Ω—ã–µ –æ—Ç–≤–µ—Ç—ã
2. –§–û–ö–£–° –ù–ê –û–®–ò–ë–ö–ê–•: –ù–∞–π–¥–∏ –≥–¥–µ –ø—Ä–æ–¥–∞–≤–µ—Ü –ù–ï –í–´–ü–û–õ–ù–ò–õ –Ω—É–∂–Ω—ã–π JTBD
3. –ò—Å–ø–æ–ª—å–∑—É–π –¢–û–ß–ù–´–ï –¶–ò–¢–ê–¢–´ –∏–∑ —Ä–∞–∑–≥–æ–≤–æ—Ä–æ–≤
4. –°–≤—è–∑—ã–≤–∞–π –æ—à–∏–±–∫–∏ —Å –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–º–∏ JTBD –∏–∑ reference table

–§–û–†–ú–ê–¢ –û–¢–í–ï–¢–ê - JSON —Å –∞–Ω–∞–ª–∏–∑–æ–º –≤—Å–µ—Ö 5 —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ç–æ–≤:

```json
{{
  "transcript_1": {{
    "lead_inquiry": "—Ç–æ—á–Ω—ã–π –∑–∞–ø—Ä–æ—Å –∫–ª–∏–µ–Ω—Ç–∞ –ë–ï–ó –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö –º–µ—Ç–æ–∫",
    "when_trigger_situation": "–∫–æ–≥–¥–∞ [—Å–∏—Ç—É–∞—Ü–∏—è] –≤ [timestamp] - –ø—Ä–æ–¥–∞–≤–µ—Ü –Ω–µ –≤—ã–ø–æ–ª–Ω–∏–ª [–∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–π JTBD] –≤–º–µ—Å—Ç–æ [–ø—Ä–∞–≤–∏–ª—å–Ω–æ–≥–æ –¥–µ–π—Å—Ç–≤–∏—è]",
    "root_cause_5why": "why1: –ö–æ–Ω–∫—Ä–µ—Ç–Ω–∞—è –ø—Ä–∏—á–∏–Ω–∞ –¥–ª—è –≠–¢–û–ì–û —Å–ª—É—á–∞—è  
why2: –°–ª–µ–¥—É—é—â–∏–π —É—Ä–æ–≤–µ–Ω—å –¥–ª—è –≠–¢–û–ì–û —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ç–∞  
why3: –°–∏—Å—Ç–µ–º–Ω–∞—è –ø—Ä–∏—á–∏–Ω–∞ –¥–ª—è –≠–¢–û–ì–û –ø—Ä–æ–¥–∞–≤—Ü–∞  
why4: –ü—Ä–∏—á–∏–Ω–∞ –ø—Ä–æ—Ü–µ—Å—Å–∞ –¥–ª—è –≠–¢–û–ô —Å–∏—Ç—É–∞—Ü–∏–∏  
why5: –ö–æ—Ä–Ω–µ–≤–∞—è –ø—Ä–∏—á–∏–Ω–∞ –¥–ª—è –≠–¢–û–ô –∫–æ–º–ø–∞–Ω–∏–∏",
    "sale_blockers": "–í [timestamp] –ø—Ä–æ–¥–∞–≤–µ—Ü —Å–∫–∞–∑–∞–ª '[—Ç–æ—á–Ω–∞—è —Ü–∏—Ç–∞—Ç–∞]' –≤–º–µ—Å—Ç–æ [–ø—Ä–∞–≤–∏–ª—å–Ω–æ–≥–æ –¥–µ–π—Å—Ç–≤–∏—è]",
    "segment": "b2b/b2c - [–∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–µ –∞—Ä–≥—É–º–µ–Ω—Ç—ã –∏–∑ –≠–¢–û–ì–û —Ä–∞–∑–≥–æ–≤–æ—Ä–∞]",
    "stop_words_patterns": "small-jtbd —Å—Ü–µ–Ω–∞—Ä–∏–π: [–æ–ø–∏—Å–∞–Ω–∏–µ –æ—à–∏–±–∫–∏]  
  
lead inquiry: [—Ü–∏—Ç–∞—Ç–∞ –∑–∞–ø—Ä–æ—Å–∞]  
  
operator answer: [—Ü–∏—Ç–∞—Ç–∞ –Ω–µ–ø—Ä–∞–≤–∏–ª—å–Ω–æ–≥–æ –æ—Ç–≤–µ—Ç–∞]",
    "recommended_phrases": "small jtbd —Å—Ü–µ–Ω–∞—Ä–∏–π: [–æ–ø–∏—Å–∞–Ω–∏–µ –ø—Ä–∞–≤–∏–ª—å–Ω–æ–≥–æ –ø–æ–≤–µ–¥–µ–Ω–∏—è]  
  
lead inquiry: [—Ç–∞ –∂–µ —Ü–∏—Ç–∞—Ç–∞ –∑–∞–ø—Ä–æ—Å–∞]  
  
good_answer: [–æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –æ—Ç–≤–µ—Ç –¥–ª—è –≠–¢–û–ô —Å–∏—Ç—É–∞—Ü–∏–∏]",
    "what_client_get_on_this_stage": "1. [—ç—Ç–∞–ø –¥–ª—è –≠–¢–û–ì–û –∫–ª–∏–µ–Ω—Ç–∞]  
2. [—ç—Ç–∞–ø –¥–ª—è –≠–¢–û–ì–û —Å–ª—É—á–∞—è]  
3. [—ç—Ç–∞–ø –¥–ª—è –≠–¢–û–ô —Å–∏—Ç—É–∞—Ü–∏–∏]  
4. [—ç—Ç–∞–ø –¥–ª—è –≠–¢–û–ì–û —Ä–∞–∑–≥–æ–≤–æ—Ä–∞]",
    "big_jtbd": "[B1/B2/B3 –∏–∑ reference table]",
    "medium_jtbd": "[M1.1-M3.3 –∏–∑ reference table - –∫–æ—Ç–æ—Ä—ã–π –ù–ï –ë–´–õ –í–´–ü–û–õ–ù–ï–ù]",
    "small_jtbd": "[S1.1-S3.12 –∏–∑ reference table - –∫–æ—Ç–æ—Ä—ã–π –ù–ï –ë–´–õ –í–´–ü–û–õ–ù–ï–ù]"
  }},
  "transcript_2": {{ ... –∞–Ω–∞–ª–æ–≥–∏—á–Ω–æ –¥–ª—è —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ç–∞ 2 ... }},
  "transcript_3": {{ ... –∞–Ω–∞–ª–æ–≥–∏—á–Ω–æ –¥–ª—è —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ç–∞ 3 ... }},
  "transcript_4": {{ ... –∞–Ω–∞–ª–æ–≥–∏—á–Ω–æ –¥–ª—è —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ç–∞ 4 ... }},
  "transcript_5": {{ ... –∞–Ω–∞–ª–æ–≥–∏—á–Ω–æ –¥–ª—è —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ç–∞ 5 ... }}
}}
```

–í–ê–ñ–ù–û: –ö–∞–∂–¥—ã–π –∞–Ω–∞–ª–∏–∑ –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å –£–ù–ò–ö–ê–õ–¨–ù–´–ú –¥–ª—è –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–≥–æ —Ä–∞–∑–≥–æ–≤–æ—Ä–∞. –ù–ï –∏—Å–ø–æ–ª—å–∑—É–π –∫–æ–ø–∏—Ä–æ–≤–∞–Ω–∏–µ –º–µ–∂–¥—É —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ç–∞–º–∏!
"""
        
        return prompt
    
    def _parse_batch_ai_analysis_to_tsv(self, ai_analysis: str, transcripts_batch: List[Tuple[str, str]]) -> List[str]:
        """–ü–∞—Ä—Å–∏—Ç batch AI –∞–Ω–∞–ª–∏–∑ –∏ —Ñ–æ—Ä–º–∏—Ä—É–µ—Ç 5 TSV —Å—Ç—Ä–æ–∫"""
        
        try:
            # –ü—ã—Ç–∞–µ–º—Å—è –∏–∑–≤–ª–µ—á—å JSON –∏–∑ –æ—Ç–≤–µ—Ç–∞ AI
            json_match = re.search(r'```json\s*(\{.*?\})\s*```', ai_analysis, re.DOTALL)
            if json_match:
                json_str = json_match.group(1)
                batch_analysis = json.loads(json_str)
            else:
                # –ü—ã—Ç–∞–µ–º—Å—è –ø–∞—Ä—Å–∏—Ç—å –∫–∞–∫ –æ–±—ã—á–Ω—ã–π JSON
                batch_analysis = json.loads(ai_analysis)
            
            tsv_rows = []
            
            # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –∫–∞–∂–¥—ã–π —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ç –∏–∑ –±–∞—Ç—á–∞
            for i, (timestamp, transcript_text) in enumerate(transcripts_batch, 1):
                transcript_key = f"transcript_{i}"
                
                if transcript_key in batch_analysis:
                    analysis_data = batch_analysis[transcript_key]
                    
                    # –ò–∑–≤–ª–µ–∫–∞–µ–º date_time –∏ week
                    date_time, week = self._extract_datetime_and_week(timestamp)
                    
                    # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ç
                    clean_transcript = transcript_text.replace('\n', ' ').replace('\t', ' ')[:500]
                    
                    # –°–æ–±–∏—Ä–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç –¥–ª—è —ç—Ç–æ–≥–æ —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ç–∞
                    result_data = {
                        'transcript': clean_transcript,
                        'lead_inquiry': analysis_data.get('lead_inquiry', '–Ω–µ –∏–∑–≤–ª–µ—á–µ–Ω–æ')[:150],
                        'when_trigger_situation': analysis_data.get('when_trigger_situation', '–Ω–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–æ')[:200],
                        'root cause 5why': analysis_data.get('root_cause_5why', '–∞–Ω–∞–ª–∏–∑ –Ω–µ –≤—ã–ø–æ–ª–Ω–µ–Ω').replace('\n', ' ').replace('\\n', '  \n'),
                        'sale blockers': analysis_data.get('sale_blockers', '–Ω–µ –Ω–∞–π–¥–µ–Ω—ã')[:200],
                        'segment': analysis_data.get('segment', '–Ω–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω')[:100],
                        'stop_words_patterns': analysis_data.get('stop_words_patterns', '–Ω–µ –Ω–∞–π–¥–µ–Ω—ã').replace('\n', ' ').replace('\\n', '  \n'),
                        'recommended_phrases': analysis_data.get('recommended_phrases', '–Ω–µ —Å–æ–∑–¥–∞–Ω—ã').replace('\n', ' ').replace('\\n', '  \n'),
                        'what client get on this stage': analysis_data.get('what_client_get_on_this_stage', '–Ω–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–æ').replace('\n', ' ').replace('\\n', '  \n'),
                        'big jtbd': analysis_data.get('big_jtbd', '–Ω–µ –≤—ã–±—Ä–∞–Ω–æ')[:100],
                        'medium jtbd': analysis_data.get('medium_jtbd', '–Ω–µ –≤—ã–±—Ä–∞–Ω–æ')[:150],
                        'small jtbd': analysis_data.get('small_jtbd', '–Ω–µ –≤—ã–±—Ä–∞–Ω–æ')[:150],
                        'date_time': date_time,
                        'week': week
                    }
                    
                    # –û—á–∏—â–∞–µ–º —Ç–∞–±—É–ª—è—Ü–∏–∏ –≤–æ –≤—Å–µ—Ö –ø–æ–ª—è—Ö
                    for key in result_data:
                        if isinstance(result_data[key], str):
                            result_data[key] = result_data[key].replace('\t', ' ')
                    
                    # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º TSV —Å—Ç—Ä–æ–∫—É
                    tsv_row = '\t'.join([result_data[col] for col in self.REQUIRED_COLUMNS])
                    tsv_rows.append(tsv_row)
                else:
                    # Fallback –¥–ª—è –æ—Ç—Å—É—Ç—Å—Ç–≤—É—é—â–∏—Ö –¥–∞–Ω–Ω—ã—Ö
                    tsv_rows.append(self._fallback_processing(transcript_text, timestamp))
            
            return tsv_rows
            
        except Exception as e:
            logger.error(f"‚ùå Error parsing batch AI analysis: {e}")
            return [self._fallback_processing(transcript, timestamp) for timestamp, transcript in transcripts_batch]
    
    def _parse_text_analysis(self, ai_text: str) -> Dict:
        """–ü–∞—Ä—Å–∏—Ç —Ç–µ–∫—Å—Ç–æ–≤—ã–π –æ—Ç–≤–µ—Ç AI –µ—Å–ª–∏ JSON –Ω–µ —É–¥–∞–ª—Å—è"""
        # –ü—Ä–æ—Å—Ç–æ–π –ø–∞—Ä—Å–∏–Ω–≥ –ø–æ –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–∞–º
        analysis_data = {}
        
        # –ò—â–µ–º –∫–∞–∂–¥–æ–µ –ø–æ–ª–µ –≤ —Ç–µ–∫—Å—Ç–µ
        fields = [
            'lead_inquiry', 'when_trigger_situation', 'root_cause_5why',
            'sale_blockers', 'segment', 'stop_words_patterns', 
            'recommended_phrases', 'what_client_get_on_this_stage',
            'big_jtbd', 'medium_jtbd', 'small_jtbd'
        ]
        
        for field in fields:
            pattern = rf'{field}[:\"]([^"\n]{{1,500}})'
            match = re.search(pattern, ai_text, re.IGNORECASE)
            if match:
                analysis_data[field] = match.group(1).strip()
            else:
                analysis_data[field] = f"{field} –Ω–µ –Ω–∞–π–¥–µ–Ω–æ –≤ AI –æ—Ç–≤–µ—Ç–µ"
        
        return analysis_data
    
    def _extract_datetime_and_week(self, source_timestamp: Optional[str]) -> Tuple[str, str]:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –¥–∞—Ç—É –∏ –Ω–µ–¥–µ–ª—é –∏–∑ timestamp"""
        if source_timestamp:
            try:
                if '@' in source_timestamp:
                    date_part, time_part = source_timestamp.split('@')
                    date_part = date_part.strip()
                    time_part = time_part.strip().split('.')[0]
                    dt = datetime.strptime(f"{date_part} {time_part}", '%b %d, %Y %H:%M:%S')
                    date_time = dt.strftime('%Y-%m-%d %H:%M:%S')
                    week = f"Week {dt.isocalendar()[1]}"
                    return date_time, week
            except Exception as e:
                logger.warning(f"‚ö†Ô∏è Error parsing timestamp: {e}")
        
        # Fallback
        now = datetime.now()
        return now.strftime('%Y-%m-%d %H:%M:%S'), f"Week {now.isocalendar()[1]}"
    
    def _fallback_processing(self, transcript_text: str, source_timestamp: Optional[str]) -> str:
        """Fallback –æ–±—Ä–∞–±–æ—Ç–∫–∞ –±–µ–∑ AI"""
        logger.info("üîÑ Using fallback processing (no AI)")
        
        # –ü—Ä–æ—Å—Ç–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –±–µ–∑ AI
        date_time, week = self._extract_datetime_and_week(source_timestamp)
        
        result_data = {
            'transcript': transcript_text.replace('\n', ' ').replace('\t', ' ')[:500],
            'lead_inquiry': 'AI –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω - –±–∞–∑–æ–≤–æ–µ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ',
            'when_trigger_situation': '—Ç—Ä–µ–±—É–µ—Ç—Å—è AI –∞–Ω–∞–ª–∏–∑',
            'root cause 5why': 'AI –∞–Ω–∞–ª–∏–∑ –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω',
            'sale blockers': '—Ç—Ä–µ–±—É–µ—Ç—Å—è AI –æ–±—Ä–∞–±–æ—Ç–∫–∞',
            'segment': '–Ω–µ–æ–ø—Ä–µ–¥–µ–ª–µ–Ω –±–µ–∑ AI',
            'stop_words_patterns': 'AI –∞–Ω–∞–ª–∏–∑ –Ω–µ–æ–±—Ö–æ–¥–∏–º',
            'recommended_phrases': 'AI –≥–µ–Ω–µ—Ä–∞—Ü–∏—è –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∞',
            'what client get on this stage': '—Ç—Ä–µ–±—É–µ—Ç—Å—è AI –∞–Ω–∞–ª–∏–∑',
            'big jtbd': 'AI –≤—ã–±–æ—Ä –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω',
            'medium jtbd': 'AI –∞–Ω–∞–ª–∏–∑ –Ω—É–∂–µ–Ω',
            'small jtbd': 'AI –æ–±—Ä–∞–±–æ—Ç–∫–∞ —Ç—Ä–µ–±—É–µ—Ç—Å—è',
            'date_time': date_time,
            'week': week
        }
        
        return '\t'.join([result_data[col] for col in self.REQUIRED_COLUMNS])




def process_batch_transcripts_with_openai(transcripts: List[Tuple[str, str]], batch_size: int = 5) -> List[str]:
    """
    –û–±—Ä–∞–±–æ—Ç–∫–∞ —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ç–æ–≤ –±–∞—Ç—á–∞–º–∏ –ø–æ 5 —à—Ç—É–∫ —Å OpenAI GPT-4.1 mini
    –§–û–ö–£–°: –≤—ã—è–≤–ª–µ–Ω–∏–µ JTBD –æ—à–∏–±–æ–∫ –ø—Ä–æ–¥–∞–≤—Ü–æ–≤
    
    Args:
        transcripts: –°–ø–∏—Å–æ–∫ (timestamp, transcript_text)
        batch_size: –†–∞–∑–º–µ—Ä –±–∞—Ç—á–∞ (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é 5)
        
    Returns:
        List[str]: –°–ø–∏—Å–æ–∫ TSV —Å—Ç—Ä–æ–∫ —Å –∞–Ω–∞–ª–∏–∑–æ–º JTBD –æ—à–∏–±–æ–∫
    """
    processor = SalesTranscriptProcessorV8WithOpenAI()
    results = []
    
    # –†–∞–∑–±–∏–≤–∞–µ–º —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ç—ã –Ω–∞ –±–∞—Ç—á–∏ –ø–æ 5 —à—Ç—É–∫
    for i in range(0, len(transcripts), batch_size):
        batch = transcripts[i:i + batch_size]
        
        logger.info(f"üîÑ Processing batch {i//batch_size + 1}: {len(batch)} transcripts")
        
        try:
            # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –±–∞—Ç—á
            batch_results = processor.process_batch_with_openai(batch)
            results.extend(batch_results)
            
            logger.info(f"‚úÖ Batch {i//batch_size + 1} completed: {len(batch_results)} analyses")
            
        except Exception as e:
            logger.error(f"‚ùå Error processing batch {i//batch_size + 1}: {e}")
            # –î–æ–±–∞–≤–ª—è–µ–º fallback —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø—Ä–∏ –æ—à–∏–±–∫–µ
            for timestamp, transcript in batch:
                empty_row = '\t'.join([''] * 14)
                results.append(empty_row)
    
    return results


if __name__ == "__main__":
    # –¢–µ—Å—Ç OpenAI –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä–∞
    test_transcript = """
    –°–ø–∏–∫–µ—Ä 0 (00:00:00): –ê–≤—Ç–æ–ª.—Ä—É, –º–µ–Ω—è –∑–æ–≤—É—Ç –ê–ª–µ–∫—Å–µ–π, –∑–¥—Ä–∞–≤—Å—Ç–≤—É–π—Ç–µ.
    –°–ø–∏–∫–µ—Ä 1 (00:00:02): –ó–¥—Ä–∞–≤—Å—Ç–≤—É–π—Ç–µ, –ê–ª–µ–∫—Å–µ–π. –•–æ—Ç–µ–ª –±—ã —É–∑–Ω–∞—Ç—å, –µ—Å—Ç—å –ª–∏ —É –≤–∞—Å –Ω–∞–±–æ—Ä –¥–ª—è –≤—ã—Å—Ç–∞–≤–ª–µ–Ω–∏—è —Ä–µ–∑—å–±—ã –±–æ–ª—Ç–∞ –ú6?
    –°–ø–∏–∫–µ—Ä 0 (00:00:14): –ù–∞–±–æ—Ä –¥–ª—è –≤—ã—Å—Ç–∞–≤–ª–µ–Ω–∏—è —Ä–µ–∑—å–±—ã –ú6... –°–µ–π—á–∞—Å –ø–æ—Å–º–æ—Ç—Ä—é. –î–∞, —É –Ω–∞—Å –µ—Å—Ç—å —Ç–∞–∫–æ–π –Ω–∞–±–æ—Ä –∑–∞ 1050 —Ä—É–±–ª–µ–π.
    –°–ø–∏–∫–µ—Ä 1 (00:00:22): –ê –≥–¥–µ –º–æ–∂–Ω–æ –∑–∞–±—Ä–∞—Ç—å?
    –°–ø–∏–∫–µ—Ä 0 (00:00:25): –≠—Ç–æ –≤—Å–µ –¥–∞–ª–µ–∫–æ, —ç—Ç–æ –≤—Å–µ –Ω–∞ –≤–æ—Å—Ç–æ–∫–µ.
    –°–ø–∏–∫–µ—Ä 1 (00:00:28): –ü–æ–Ω—è—Ç–Ω–æ, —Å–ø–∞—Å–∏–±–æ.
    """
    
    processor = SalesTranscriptProcessorV8WithOpenAI()
    
    # –¢–µ—Å—Ç–∏—Ä—É–µ–º –±–∞—Ç—á-–æ–±—Ä–∞–±–æ—Ç–∫—É
    test_batch = [
        ("Jul 4, 2025 @ 19:05:57.156", test_transcript),
        ("Jul 4, 2025 @ 19:15:30.000", test_transcript),  # –î—É–±–ª–∏—Ä—É–µ–º –¥–ª—è —Ç–µ—Å—Ç–∞
    ]
    
    results = processor.process_batch_with_openai(test_batch)
    print("OpenAI Batch Analysis Results:")
    for i, result in enumerate(results, 1):
        print(f"Result {i}: {result[:200]}...")
        
    def upload_to_target_worksheet_with_tdd(self, tsv_results: List[str]) -> Dict[str, any]:
        """
        TDD INTEGRATION: –ó–∞–≥—Ä—É–∂–∞–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤ –¶–ï–õ–ï–í–û–ô –ª–∏—Å—Ç —Å –≤—Å—Ç—Ä–æ–µ–Ω–Ω–æ–π –≤–∞–ª–∏–¥–∞—Ü–∏–µ–π
        –ò—Å–ø–æ–ª—å–∑—É–µ—Ç —Ç–æ—á–Ω—ã–π GID –∏–∑ URL –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è –≤–º–µ—Å—Ç–æ —Å–æ–∑–¥–∞–Ω–∏—è –Ω–æ–≤–æ–≥–æ –ª–∏—Å—Ç–∞
        """
        import gspread
        from google.oauth2.service_account import Credentials
        import json
        import time
        
        logger.info("üß™ TDD UPLOAD: Starting upload to target worksheet with validation")
        
        try:
            # –ê—É—Ç–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ü–∏—è
            with open("advising_platform/config/google_service_account.json", 'r') as f:
                creds_data = json.load(f)
            
            scopes = [
                'https://www.googleapis.com/auth/spreadsheets',
                'https://www.googleapis.com/auth/drive'
            ]
            credentials = Credentials.from_service_account_info(creds_data, scopes=scopes)
            gc = gspread.authorize(credentials)
            
            # –û—Ç–∫—Ä—ã—Ç—å —Ç–∞–±–ª–∏—Ü—É –∏ –Ω–∞–π—Ç–∏ –¶–ï–õ–ï–í–û–ô –ª–∏—Å—Ç –ø–æ GID
            sheet = gc.open_by_key(self.target_spreadsheet_id)
            target_worksheet = None
            
            for ws in sheet.worksheets():
                if str(ws.id) == self.target_worksheet_gid:
                    target_worksheet = ws
                    break
            
            if not target_worksheet:
                return {
                    "success": False, 
                    "error": f"Target worksheet with GID {self.target_worksheet_gid} not found",
                    "available_worksheets": [(ws.title, ws.id) for ws in sheet.worksheets()]
                }
            
            logger.info(f"‚úÖ Found target worksheet: {target_worksheet.title} (gid={self.target_worksheet_gid})")
            
            # –ü–æ–¥–≥–æ—Ç–æ–≤–∏—Ç—å –¥–∞–Ω–Ω—ã–µ –∏–∑ TSV —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
            if not tsv_results:
                return {"success": False, "error": "No TSV results to upload"}
                
            # –ü–∞—Ä—Å–∏–º TSV —Å—Ç—Ä–æ–∫–∏
            parsed_data = []
            for tsv_row in tsv_results:
                parsed_data.append(tsv_row.split('\t'))
            
            # Mapping –∫–æ–ª–æ–Ω–æ–∫ –∫ –ø–æ–∑–∏—Ü–∏—è–º –≤ Google Sheets (—Å–æ–≥–ª–∞—Å–Ω–æ —Å–∫—Ä–∏–Ω—à–æ—Ç—É –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è)
            column_mapping = {
                'sale blockers': 'F',      # –ö–æ–ª–æ–Ω–∫–∞ F
                'when_trigger_situation': 'G',  # –ö–æ–ª–æ–Ω–∫–∞ G  
                'root cause 5why': 'H',    # –ö–æ–ª–æ–Ω–∫–∞ H
                'stop_words_patterns': 'I', # –ö–æ–ª–æ–Ω–∫–∞ I
                'recommended_phrases': 'J'  # –ö–æ–ª–æ–Ω–∫–∞ J
            }
            
            # –ù–∞–π—Ç–∏ –∏–Ω–¥–µ–∫—Å—ã –∫–æ–ª–æ–Ω–æ–∫ –≤ TSV
            tsv_column_indices = {}
            for col_name in column_mapping.keys():
                if col_name in self.REQUIRED_COLUMNS:
                    tsv_column_indices[col_name] = self.REQUIRED_COLUMNS.index(col_name)
            
            # –û—á–∏—Å—Ç–∏—Ç—å —Ü–µ–ª–µ–≤—ã–µ –∫–æ–ª–æ–Ω–∫–∏
            ranges_to_clear = [f"{col}2:{col}11" for col in column_mapping.values()]
            target_worksheet.batch_clear(ranges_to_clear)
            time.sleep(2)  # –ñ–¥–µ–º –ø—Ä–∏–º–µ–Ω–µ–Ω–∏—è –∏–∑–º–µ–Ω–µ–Ω–∏–π
            
            # –ó–∞–≥—Ä—É–∑–∏—Ç—å –¥–∞–Ω–Ω—ã–µ –ø–æ –∫–æ–ª–æ–Ω–∫–∞–º
            updates_made = 0
            
            for tsv_col, sheets_col in column_mapping.items():
                if tsv_col in tsv_column_indices:
                    col_idx = tsv_column_indices[tsv_col]
                    
                    # –ü–æ–¥–≥–æ—Ç–æ–≤–∏—Ç—å –¥–∞–Ω–Ω—ã–µ –¥–ª—è –∫–æ–ª–æ–Ω–∫–∏
                    column_data = []
                    for row_data in parsed_data:
                        if col_idx < len(row_data):
                            column_data.append([row_data[col_idx]])
                        else:
                            column_data.append([''])
                    
                    # –ó–∞–≥—Ä—É–∑–∏—Ç—å –∫–æ–ª–æ–Ω–∫—É
                    range_name = f"{sheets_col}2:{sheets_col}{len(parsed_data) + 1}"
                    target_worksheet.update(values=column_data, range_name=range_name)
                    updates_made += len(column_data)
                    logger.info(f"‚úÖ Updated {tsv_col} in column {sheets_col}: {len(column_data)} cells")
                    time.sleep(1)  # –ü–∞—É–∑–∞ –º–µ–∂–¥—É –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è–º–∏
            
            # TDD VALIDATION: –ü—Ä–æ–≤–µ—Ä–∏—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç
            time.sleep(3)  # –ñ–¥–µ–º –ø—Ä–∏–º–µ–Ω–µ–Ω–∏—è –≤—Å–µ—Ö –∏–∑–º–µ–Ω–µ–Ω–∏–π
            
            validation_results = {}
            total_matches = 0
            total_cells = 0
            
            for tsv_col, sheets_col in column_mapping.items():
                if tsv_col in tsv_column_indices:
                    col_idx = tsv_column_indices[tsv_col]
                    
                    # –ü–æ–ª—É—á–∏—Ç—å –¥–∞–Ω–Ω—ã–µ –∏–∑ Google Sheets
                    range_name = f"{sheets_col}2:{sheets_col}{len(parsed_data) + 1}"
                    sheets_values = target_worksheet.get(range_name)
                    
                    # –°—Ä–∞–≤–Ω–∏—Ç—å —Å –∏—Å—Ö–æ–¥–Ω—ã–º–∏ –¥–∞–Ω–Ω—ã–º–∏
                    matches = 0
                    for i, row_data in enumerate(parsed_data):
                        expected = row_data[col_idx] if col_idx < len(row_data) else ""
                        actual = ""
                        if i < len(sheets_values) and sheets_values[i]:
                            actual = sheets_values[i][0]
                        
                        if expected.strip() == actual.strip():
                            matches += 1
                    
                    match_percentage = (matches / len(parsed_data) * 100) if parsed_data else 0
                    validation_results[tsv_col] = {
                        'matches': matches,
                        'total': len(parsed_data),
                        'percentage': match_percentage
                    }
                    
                    total_matches += matches
                    total_cells += len(parsed_data)
                    logger.info(f"üìä {tsv_col}: {matches}/{len(parsed_data)} matches ({match_percentage:.1f}%)")
            
            overall_success = (total_matches / total_cells * 100) >= 95 if total_cells > 0 else False
            
            return {
                "success": overall_success,
                "updates_made": updates_made,
                "validation_results": validation_results,
                "overall_match_percentage": (total_matches / total_cells * 100) if total_cells > 0 else 0,
                "target_url": f"https://docs.google.com/spreadsheets/d/{self.target_spreadsheet_id}/edit#gid={self.target_worksheet_gid}",
                "worksheet_title": target_worksheet.title
            }
            
        except Exception as e:
            logger.error(f"‚ùå TDD Upload failed: {e}")
            return {"success": False, "error": str(e)}