#!/usr/bin/env python3
"""
Sales Transcript Processor v2.0 - Multiple Rows per Transcript
–ò—Å–ø—Ä–∞–≤–ª–µ–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π –º–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö JTBD –Ω–∞ –æ–¥–∏–Ω —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ç
"""

import pandas as pd
import json
from pathlib import Path
from typing import List, Dict, Any, Tuple
import re

class SalesTranscriptProcessorV2:
    """–û–±—Ä–∞–±–æ—Ç—á–∏–∫ —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ç–æ–≤ —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π –º–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö JTBD"""
    
    def __init__(self):
        self.column_mapping = {
            # –û—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–µ –∫–æ–ª–æ–Ω–∫–∏ ‚Üí –ù–æ–≤—ã–µ –∞–Ω–≥–ª–∏–π—Å–∫–∏–µ –Ω–∞–∑–≤–∞–Ω–∏—è
            '–∫–∞–∫ –Ω–µ –Ω—É–∂–Ω–æ –≥–æ–≤–æ—Ä–∏—Ç—å, —á—Ç–æ —Å—Ç–æ–ø-—Å–ª–æ–≤–∞': 'stop_words_patterns',
            '–∫–∞–∫ —Ö–æ—Ç–∏–º, —á—Ç–æ–±—ã –≥–æ–≤–æ—Ä–∏–ª–∏': 'recommended_phrases', 
            '–≤–æ–ø—Ä–æ—Å—ã –Ω–∞ –∫–∞–∂–¥–æ–π —Å—Ç–∞–¥–∏–∏, —á—Ç–æ –Ω—É–∂–Ω–æ –ø–æ–Ω–∏–º–∞—Ç—å –∫–ª–∏–µ–Ω—Ç': 'client_understanding_criteria'
        }
    
    def load_original_data(self, tsv_path: str) -> pd.DataFrame:
        """–ó–∞–≥—Ä—É–∑–∫–∞ –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö"""
        return pd.read_csv(tsv_path, sep='\t')
    
    def extract_multiple_jtbd(self, row: pd.Series) -> List[Dict[str, Any]]:
        """
        –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –º–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö JTBD –∏–∑ –æ–¥–Ω–æ–π —Å—Ç—Ä–æ–∫–∏ —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ç–∞
        
        –°–æ–≥–ª–∞—Å–Ω–æ sales.injury standard:
        - 1 transcript –º–æ–∂–µ—Ç —Å–æ–¥–µ—Ä–∂–∞—Ç—å –Ω–µ—Å–∫–æ–ª—å–∫–æ Big/Medium/Small JTBD
        - –ö–∞–∂–¥—ã–π JTBD = –æ—Ç–¥–µ–ª—å–Ω–∞—è —Å—Ç—Ä–æ–∫–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞
        """
        
        base_data = {
            'transcript': row['transcript'],
            'lead_inquiry': row['lead_inquiry'],
            'root_cause_5why': row['root cause 5why'],
            'sales_blockers': row['sales_blockers'],
            'segment': row['segment'],
            'processing_time_sec': row['processing_time_sec'],
            'quality_score': row['quality_score'],
            'validation_passed': row['validation_passed']
        }
        
        # –ò–∑–≤–ª–µ–∫–∞–µ–º JTBD –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã
        big_jtbd = str(row['big_jtbd']) if pd.notna(row['big_jtbd']) else ""
        medium_jtbd = str(row['medium_jtbd']) if pd.notna(row['medium_jtbd']) else ""
        small_jtbd = str(row['small_jtbd']) if pd.notna(row['small_jtbd']) else ""
        when_situation = str(row['when —Å–∏—Ç—É–∞—Ü–∏—è –≤ big_jtbd']) if pd.notna(row['when —Å–∏—Ç—É–∞—Ü–∏—è –≤ big_jtbd']) else ""
        qualified_triggers = str(row['qualified_triggers']) if pd.notna(row['qualified_triggers']) else ""
        
        # –ö–æ–º–º—É–Ω–∏–∫–∞—Ü–∏–æ–Ω–Ω—ã–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã
        stop_words = str(row['–∫–∞–∫ –Ω–µ –Ω—É–∂–Ω–æ –≥–æ–≤–æ—Ä–∏—Ç—å, —á—Ç–æ —Å—Ç–æ–ø-—Å–ª–æ–≤–∞']) if pd.notna(row['–∫–∞–∫ –Ω–µ –Ω—É–∂–Ω–æ –≥–æ–≤–æ—Ä–∏—Ç—å, —á—Ç–æ —Å—Ç–æ–ø-—Å–ª–æ–≤–∞']) else ""
        recommended_phrases = str(row['–∫–∞–∫ —Ö–æ—Ç–∏–º, —á—Ç–æ–±—ã –≥–æ–≤–æ—Ä–∏–ª–∏']) if pd.notna(row['–∫–∞–∫ —Ö–æ—Ç–∏–º, —á—Ç–æ–±—ã –≥–æ–≤–æ—Ä–∏–ª–∏']) else ""
        understanding_criteria = str(row['–≤–æ–ø—Ä–æ—Å—ã –Ω–∞ –∫–∞–∂–¥–æ–π —Å—Ç–∞–¥–∏–∏, —á—Ç–æ –Ω—É–∂–Ω–æ –ø–æ–Ω–∏–º–∞—Ç—å –∫–ª–∏–µ–Ω—Ç']) if pd.notna(row['–≤–æ–ø—Ä–æ—Å—ã –Ω–∞ –∫–∞–∂–¥–æ–π —Å—Ç–∞–¥–∏–∏, —á—Ç–æ –Ω—É–∂–Ω–æ –ø–æ–Ω–∏–º–∞—Ç—å –∫–ª–∏–µ–Ω—Ç']) else ""
        
        # –õ–æ–≥–∏–∫–∞ —Å–æ–∑–¥–∞–Ω–∏—è –º–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö —Å—Ç—Ä–æ–∫
        jtbd_entries = []
        
        # –ü–∞—Ä—Å–∏–º –≤–æ–∑–º–æ–∂–Ω—ã–µ –º–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ JTBD
        big_jobs = self._split_jtbd_items(big_jtbd)
        medium_jobs = self._split_jtbd_items(medium_jtbd)  
        small_jobs = self._split_jtbd_items(small_jtbd)
        
        # –°–æ–∑–¥–∞–µ–º –∫–æ–º–±–∏–Ω–∞—Ü–∏–∏ (–∫–∞–∂–¥—ã–π Big –º–æ–∂–µ—Ç –∏–º–µ—Ç—å –Ω–µ—Å–∫–æ–ª—å–∫–æ Medium/Small)
        max_items = max(len(big_jobs), len(medium_jobs), len(small_jobs), 1)
        
        for i in range(max_items):
            entry = base_data.copy()
            entry.update({
                'stop_words_patterns': stop_words,
                'recommended_phrases': recommended_phrases,
                'client_understanding_criteria': understanding_criteria,
                'when_situation_big_jtbd': when_situation,
                'big_jtbd': big_jobs[i] if i < len(big_jobs) else big_jobs[-1] if big_jobs else "",
                'medium_jtbd': medium_jobs[i] if i < len(medium_jobs) else medium_jobs[-1] if medium_jobs else "",
                'small_jtbd': small_jobs[i] if i < len(small_jobs) else small_jobs[-1] if small_jobs else "",
                'qualified_triggers': qualified_triggers,
                'jtbd_sequence_number': i + 1,
                'total_jtbd_count': max_items
            })
            jtbd_entries.append(entry)
        
        return jtbd_entries
    
    def _split_jtbd_items(self, jtbd_text: str) -> List[str]:
        """–†–∞–∑–¥–µ–ª–µ–Ω–∏–µ JTBD –Ω–∞ –æ—Ç–¥–µ–ª—å–Ω—ã–µ —ç–ª–µ–º–µ–Ω—Ç—ã"""
        if not jtbd_text or jtbd_text == "":
            return []
        
        # –ü–æ–ø—Ä–æ–±—É–µ–º —Ä–∞–∑–ª–∏—á–Ω—ã–µ —Ä–∞–∑–¥–µ–ª–∏—Ç–µ–ª–∏
        separators = [';', '\n', '|', '‚Ä¢', '‚Üí']
        items = [jtbd_text]  # –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é –æ–¥–∏–Ω —ç–ª–µ–º–µ–Ω—Ç
        
        for sep in separators:
            if sep in jtbd_text:
                items = [item.strip() for item in jtbd_text.split(sep) if item.strip()]
                break
        
        return items if items else [jtbd_text]
    
    def process_tsv_to_multiple_rows(self, input_tsv: str, output_tsv: str) -> Tuple[int, int]:
        """
        –û–±—Ä–∞–±–æ—Ç–∫–∞ TSV —Å —Å–æ–∑–¥–∞–Ω–∏–µ–º –º–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö —Å—Ç—Ä–æ–∫
        
        Returns:
            Tuple[original_rows, processed_rows]
        """
        
        print(f"üîÑ Loading data from {input_tsv}")
        df = self.load_original_data(input_tsv)
        original_count = len(df)
        
        print(f"üìä Processing {original_count} transcripts for multiple JTBD extraction")
        
        all_processed_entries = []
        
        for index, row in df.iterrows():
            try:
                # –ò–∑–≤–ª–µ–∫–∞–µ–º –º–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ JTBD
                jtbd_entries = self.extract_multiple_jtbd(row)
                all_processed_entries.extend(jtbd_entries)
                
                if (index + 1) % 100 == 0:
                    print(f"‚úÖ Processed {index + 1}/{original_count} transcripts")
                    
            except Exception as e:
                print(f"‚ö†Ô∏è Error processing row {index}: {e}")
                continue
        
        # –°–æ–∑–¥–∞–µ–º –Ω–æ–≤—ã–π DataFrame —Å –æ–±–Ω–æ–≤–ª–µ–Ω–Ω—ã–º–∏ –∫–æ–ª–æ–Ω–∫–∞–º–∏
        processed_df = pd.DataFrame(all_processed_entries)
        processed_count = len(processed_df)
        
        # –ü–µ—Ä–µ—É–ø–æ—Ä—è–¥–æ—á–∏–≤–∞–µ–º –∫–æ–ª–æ–Ω–∫–∏ —Å–æ–≥–ª–∞—Å–Ω–æ sales.injury standard
        column_order = [
            'transcript',
            'lead_inquiry', 
            'root_cause_5why',
            'sales_blockers',
            'segment',
            'stop_words_patterns',
            'recommended_phrases',
            'client_understanding_criteria',
            'when_situation_big_jtbd',
            'big_jtbd',
            'medium_jtbd',
            'small_jtbd',
            'qualified_triggers',
            'jtbd_sequence_number',
            'total_jtbd_count',
            'processing_time_sec',
            'quality_score',
            'validation_passed'
        ]
        
        # –£–±–µ–¥–∏–º—Å—è —á—Ç–æ –≤—Å–µ –∫–æ–ª–æ–Ω–∫–∏ –ø—Ä–∏—Å—É—Ç—Å—Ç–≤—É—é—Ç
        for col in column_order:
            if col not in processed_df.columns:
                processed_df[col] = ""
        
        processed_df = processed_df[column_order]
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç
        processed_df.to_csv(output_tsv, sep='\t', index=False)
        
        print(f"üéØ Processing complete:")
        print(f"   üì• Original transcripts: {original_count}")
        print(f"   üì§ Processed rows: {processed_count}")
        print(f"   üìà Expansion factor: {processed_count/original_count:.1f}x")
        print(f"   üíæ Saved to: {output_tsv}")
        
        return original_count, processed_count


def main():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –æ–±—Ä–∞–±–æ—Ç–∫–∏"""
    
    processor = SalesTranscriptProcessorV2()
    
    input_file = "../[rick.ai] clients/avtoall.ru/[4] whatsapp-jtbd-tracktion/results/avtoall_sales_analyzed.tsv"
    output_file = "../[rick.ai] clients/avtoall.ru/[4] whatsapp-jtbd-tracktion/results/avtoall_sales_analyzed_v2.tsv"
    
    try:
        original_count, processed_count = processor.process_tsv_to_multiple_rows(
            input_file, output_file
        )
        
        print(f"\nüéâ SUCCESS! TSV processing completed")
        print(f"üìä Ready for Google Sheets upload: {processed_count} rows")
        
        return True
        
    except Exception as e:
        print(f"‚ùå Processing failed: {e}")
        import traceback
        traceback.print_exc()
        return False


if __name__ == "__main__":
    main()