#!/usr/bin/env python3
"""
–°–∫—Ä–∏–ø—Ç –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –ø–µ—Ä–µ—Å–µ—á–µ–Ω–∏–π —Å–æ–¥–µ—Ä–∂–∏–º–æ–≥–æ –º–µ–∂–¥—É —Å—Ç–∞–Ω–¥–∞—Ä—Ç–∞–º–∏.

–≠—Ç–æ—Ç —Å–∫—Ä–∏–ø—Ç –≤—ã—è–≤–ª—è–µ—Ç –ø–æ—Ö–æ–∂–∏–µ —Å—Ç–∞–Ω–¥–∞—Ä—Ç—ã –∏ —Å–æ–∑–¥–∞–µ—Ç –º–∞—Ç—Ä–∏—Ü—É –ø–µ—Ä–µ—Å–µ—á–µ–Ω–∏–π,
—á—Ç–æ –ø–æ–º–æ–≥–∞–µ—Ç –≤ –ø—Ä–æ—Ü–µ—Å—Å–µ –∫–æ–Ω—Å–æ–ª–∏–¥–∞—Ü–∏–∏ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–æ–≤.

–ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ:
    python analyze_standards_overlap.py [–¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—è]

–ê—Ä–≥—É–º–µ–Ω—Ç—ã:
    –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—è - –ø—É—Ç—å –∫ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏ —Å–æ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–∞–º–∏ (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é: —Ç–µ–∫—É—â–∞—è –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—è)

–ü—Ä–∏–º–µ—Ä—ã:
    python analyze_standards_overlap.py
    python analyze_standards_overlap.py "[standards .md]"
"""

import os
import re
import sys
from pathlib import Path
import difflib
import json
from collections import defaultdict
import argparse

# –¶–≤–µ—Ç–Ω—ã–µ –æ–±–æ–∑–Ω–∞—á–µ–Ω–∏—è –¥–ª—è –≤—ã–≤–æ–¥–∞
GREEN = '\033[92m'
YELLOW = '\033[93m'
RED = '\033[91m'
BLUE = '\033[94m'
ENDC = '\033[0m'
BOLD = '\033[1m'

# –†–µ–≥—É–ª—è—Ä–Ω—ã–µ –≤—ã—Ä–∞–∂–µ–Ω–∏—è –¥–ª—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏
TITLE_REGEX = r'^#\s+(.+?)$'
PROTECTED_SECTION_BEGIN = r'<!--\s*üîí\s*PROTECTED SECTION:\s*BEGIN\s*-->'
PROTECTED_SECTION_END = r'<!--\s*üîí\s*PROTECTED SECTION:\s*END\s*-->'
STANDARD_ID_REGEX = r'standard_id:\s*([^\s]+)'
LOGICAL_ID_REGEX = r'logical_id:\s*([^\s]+)'


def extract_metadata(content):
    """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –∏–∑ –∑–∞—â–∏—â–µ–Ω–Ω–æ–≥–æ —Ä–∞–∑–¥–µ–ª–∞ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–∞."""
    metadata = {}
    
    # –ò–∑–≤–ª–µ–∫–∞–µ–º –∑–∞–≥–æ–ª–æ–≤–æ–∫ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–∞
    title_match = re.search(TITLE_REGEX, content, re.MULTILINE)
    if title_match:
        metadata['title'] = title_match.group(1).strip()
    
    # –ò–∑–≤–ª–µ–∫–∞–µ–º –∑–∞—â–∏—â–µ–Ω–Ω—ã–π —Ä–∞–∑–¥–µ–ª
    protected_match = re.search(
        f"{PROTECTED_SECTION_BEGIN}(.*?){PROTECTED_SECTION_END}",
        content, 
        re.DOTALL
    )
    
    if protected_match:
        protected_content = protected_match.group(1)
        
        # –ò–∑–≤–ª–µ–∫–∞–µ–º standard_id
        standard_id_match = re.search(STANDARD_ID_REGEX, protected_content)
        if standard_id_match:
            metadata['standard_id'] = standard_id_match.group(1).strip()
        
        # –ò–∑–≤–ª–µ–∫–∞–µ–º logical_id
        logical_id_match = re.search(LOGICAL_ID_REGEX, protected_content)
        if logical_id_match:
            metadata['logical_id'] = logical_id_match.group(1).strip()
        
        # –ò–∑–≤–ª–µ–∫–∞–µ–º –≤—Å–µ —Å—Ç—Ä–æ–∫–∏ —Å related to –∏ integrated
        metadata['related'] = []
        metadata['integrated'] = []
        
        for line in protected_content.split('\n'):
            if 'related to:' in line:
                metadata['related'].append(line.strip())
            elif 'integrated:' in line:
                metadata['integrated'].append(line.strip())
    
    return metadata


def extract_sections(content):
    """–ò–∑–≤–ª–µ–∫–∞–µ—Ç —Ä–∞–∑–¥–µ–ª—ã —Å—Ç–∞–Ω–¥–∞—Ä—Ç–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ –∑–∞–≥–æ–ª–æ–≤–∫–æ–≤."""
    sections = {}
    
    # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –∑–∞—â–∏—â–µ–Ω–Ω—ã–π —Ä–∞–∑–¥–µ–ª
    if PROTECTED_SECTION_BEGIN in content and PROTECTED_SECTION_END in content:
        parts = content.split(PROTECTED_SECTION_END, 1)
        if len(parts) > 1:
            content = parts[1]
    
    # –ù–∞—Ö–æ–¥–∏–º –≤—Å–µ –∑–∞–≥–æ–ª–æ–≤–∫–∏ –≤—Ç–æ—Ä–æ–≥–æ —É—Ä–æ–≤–Ω—è (##)
    section_pattern = r'##\s+(.+?)\n(.*?)(?=##|\Z)'
    for match in re.finditer(section_pattern, content, re.DOTALL):
        section_title = match.group(1).strip()
        section_content = match.group(2).strip()
        sections[section_title] = section_content
    
    return sections


def calculate_similarity(text1, text2):
    """–†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ—Ç —Å—Ö–æ–¥—Å—Ç–≤–æ –º–µ–∂–¥—É –¥–≤—É–º—è —Ç–µ–∫—Å—Ç–∞–º–∏."""
    if not text1 or not text2:
        return 0.0
    
    # –ò—Å–ø–æ–ª—å–∑—É–µ–º SequenceMatcher –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è —Ç–µ–∫—Å—Ç–æ–≤
    sequence_matcher = difflib.SequenceMatcher(None, text1, text2)
    similarity = sequence_matcher.ratio()
    return similarity


def analyze_section_overlap(standards_data):
    """–ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç –ø–µ—Ä–µ—Å–µ—á–µ–Ω–∏–µ —Ä–∞–∑–¥–µ–ª–æ–≤ –º–µ–∂–¥—É —Å—Ç–∞–Ω–¥–∞—Ä—Ç–∞–º–∏."""
    section_overlap = defaultdict(lambda: defaultdict(dict))
    
    # –î–ª—è –∫–∞–∂–¥–æ–π –ø–∞—Ä—ã —Å—Ç–∞–Ω–¥–∞—Ä—Ç–æ–≤
    standard_ids = list(standards_data.keys())
    for i, standard1_id in enumerate(standard_ids):
        standard1 = standards_data[standard1_id]
        
        for j in range(i+1, len(standard_ids)):
            standard2_id = standard_ids[j]
            standard2 = standards_data[standard2_id]
            
            # –ù–∞—Ö–æ–¥–∏–º –æ–±—â–∏–µ —Ä–∞–∑–¥–µ–ª—ã
            common_sections = set(standard1['sections'].keys()) & set(standard2['sections'].keys())
            
            # –†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ–º —Å—Ö–æ–¥—Å—Ç–≤–æ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –æ–±—â–µ–≥–æ —Ä–∞–∑–¥–µ–ª–∞
            for section in common_sections:
                content1 = standard1['sections'][section]
                content2 = standard2['sections'][section]
                
                similarity = calculate_similarity(content1, content2)
                
                # –°–æ—Ö—Ä–∞–Ω—è–µ–º, –µ—Å–ª–∏ —Å—Ö–æ–¥—Å—Ç–≤–æ –±–æ–ª—å—à–µ –ø–æ—Ä–æ–≥–∞
                if similarity > 0.3:  # –º–æ–∂–Ω–æ –Ω–∞—Å—Ç—Ä–æ–∏—Ç—å –ø–æ—Ä–æ–≥
                    section_overlap[standard1_id][standard2_id][section] = similarity
                    section_overlap[standard2_id][standard1_id][section] = similarity
    
    return section_overlap


def analyze_overall_similarity(standards_data):
    """–ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç –æ–±—â–µ–µ —Å—Ö–æ–¥—Å—Ç–≤–æ –º–µ–∂–¥—É —Å—Ç–∞–Ω–¥–∞—Ä—Ç–∞–º–∏."""
    overall_similarity = defaultdict(dict)
    
    # –î–ª—è –∫–∞–∂–¥–æ–π –ø–∞—Ä—ã —Å—Ç–∞–Ω–¥–∞—Ä—Ç–æ–≤
    standard_ids = list(standards_data.keys())
    for i, standard1_id in enumerate(standard_ids):
        standard1 = standards_data[standard1_id]
        
        for j in range(i+1, len(standard_ids)):
            standard2_id = standard_ids[j]
            standard2 = standards_data[standard2_id]
            
            # –û–±—ä–µ–¥–∏–Ω—è–µ–º –≤—Å–µ —Ä–∞–∑–¥–µ–ª—ã
            content1 = " ".join(standard1['sections'].values())
            content2 = " ".join(standard2['sections'].values())
            
            # –†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ–º –æ–±—â–µ–µ —Å—Ö–æ–¥—Å—Ç–≤–æ
            similarity = calculate_similarity(content1, content2)
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç
            overall_similarity[standard1_id][standard2_id] = similarity
            overall_similarity[standard2_id][standard1_id] = similarity
    
    return overall_similarity


def identify_duplicates(overall_similarity, threshold=0.8):
    """–û–ø—Ä–µ–¥–µ–ª—è–µ—Ç –¥—É–±–ª–∏—Ä—É—é—â–∏–µ—Å—è —Å—Ç–∞–Ω–¥–∞—Ä—Ç—ã –Ω–∞ –æ—Å–Ω–æ–≤–µ –æ–±—â–µ–≥–æ —Å—Ö–æ–¥—Å—Ç–≤–∞."""
    duplicates = []
    
    for standard1_id, similarities in overall_similarity.items():
        for standard2_id, similarity in similarities.items():
            if similarity >= threshold and standard1_id < standard2_id:  # –ø—Ä–æ–≤–µ—Ä—è–µ–º —Ç–æ–ª—å–∫–æ –æ–¥–Ω–æ –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏–µ
                duplicates.append((standard1_id, standard2_id, similarity))
    
    # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ —É–±—ã–≤–∞–Ω–∏—é —Å—Ö–æ–¥—Å—Ç–≤–∞
    duplicates.sort(key=lambda x: x[2], reverse=True)
    return duplicates


def print_overlap_report(standards_data, section_overlap, overall_similarity, duplicates):
    """–í—ã–≤–æ–¥–∏—Ç –æ—Ç—á–µ—Ç –æ –ø–µ—Ä–µ—Å–µ—á–µ–Ω–∏—è—Ö –∏ –¥—É–±–ª–∏—Ä–æ–≤–∞–Ω–∏–∏ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–æ–≤."""
    print(f"\n{BOLD}== –û–¢–ß–ï–¢ –û –ü–ï–†–ï–°–ï–ß–ï–ù–ò–Ø–• –°–¢–ê–ù–î–ê–†–¢–û–í =={ENDC}\n")
    
    # –í—ã–≤–æ–¥–∏–º –¥—É–±–ª–∏–∫–∞—Ç—ã
    if duplicates:
        print(f"\n{BOLD}{RED}–û–ë–ù–ê–†–£–ñ–ï–ù–´ –ü–û–¢–ï–ù–¶–ò–ê–õ–¨–ù–´–ï –î–£–ë–õ–ò–ö–ê–¢–´:{ENDC}\n")
        for standard1_id, standard2_id, similarity in duplicates:
            print(f"  {RED}‚Ä¢ {standards_data[standard1_id]['file']} –∏ {standards_data[standard2_id]['file']}{ENDC}")
            print(f"    –°—Ö–æ–¥—Å—Ç–≤–æ: {similarity:.2%}")
            print(f"    –ù–∞–∑–≤–∞–Ω–∏–µ 1: {standards_data[standard1_id]['metadata'].get('title', '–ù–µ—Ç –∑–∞–≥–æ–ª–æ–≤–∫–∞')}")
            print(f"    –ù–∞–∑–≤–∞–Ω–∏–µ 2: {standards_data[standard2_id]['metadata'].get('title', '–ù–µ—Ç –∑–∞–≥–æ–ª–æ–≤–∫–∞')}")
            print()
    
    # –í—ã–≤–æ–¥–∏–º —Å—Ç–∞–Ω–¥–∞—Ä—Ç—ã —Å –≤—ã—Å–æ–∫–∏–º –ø–µ—Ä–µ—Å–µ—á–µ–Ω–∏–µ–º —Ä–∞–∑–¥–µ–ª–æ–≤
    print(f"\n{BOLD}{YELLOW}–°–¢–ê–ù–î–ê–†–¢–´ –° –í–´–°–û–ö–ò–ú –ü–ï–†–ï–°–ï–ß–ï–ù–ò–ï–ú –†–ê–ó–î–ï–õ–û–í:{ENDC}\n")
    for standard1_id, overlaps in section_overlap.items():
        for standard2_id, sections in overlaps.items():
            if sections and standard1_id < standard2_id:  # –ø—Ä–æ–≤–µ—Ä—è–µ–º —Ç–æ–ª—å–∫–æ –æ–¥–Ω–æ –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏–µ
                high_overlap_sections = {section: similarity for section, similarity in sections.items() if similarity > 0.6}
                if high_overlap_sections:
                    print(f"  {YELLOW}‚Ä¢ {standards_data[standard1_id]['file']} –∏ {standards_data[standard2_id]['file']}{ENDC}")
                    print(f"    –û–±—â–µ–µ —Å—Ö–æ–¥—Å—Ç–≤–æ: {overall_similarity[standard1_id][standard2_id]:.2%}")
                    print(f"    –†–∞–∑–¥–µ–ª—ã —Å –≤—ã—Å–æ–∫–∏–º –ø–µ—Ä–µ—Å–µ—á–µ–Ω–∏–µ–º:")
                    for section, similarity in high_overlap_sections.items():
                        print(f"      - {section}: {similarity:.2%}")
                    print()
    
    # –í—ã–≤–æ–¥–∏–º —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –ø–æ –∫–æ–Ω—Å–æ–ª–∏–¥–∞—Ü–∏–∏
    print(f"\n{BOLD}{GREEN}–†–ï–ö–û–ú–ï–ù–î–ê–¶–ò–ò –ü–û –ö–û–ù–°–û–õ–ò–î–ê–¶–ò–ò:{ENDC}\n")
    
    # –ì—Ä—É–ø–ø–∏—Ä–æ–≤–∫–∞ –ø–æ –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º
    category_groups = defaultdict(list)
    for standard_id, data in standards_data.items():
        file_path = data['file']
        parts = file_path.split('/')
        if len(parts) > 2:
            category = parts[-2]  # –ü—Ä–µ–¥–ø–æ–ª–∞–≥–∞–µ–º, —á—Ç–æ –∫–∞—Ç–µ–≥–æ—Ä–∏—è - —ç—Ç–æ –ø—Ä–µ–¥–ø–æ—Å–ª–µ–¥–Ω—è—è —á–∞—Å—Ç—å –ø—É—Ç–∏
            if "archive" not in category.lower():  # –ò—Å–∫–ª—é—á–∞–µ–º –∞—Ä—Ö–∏–≤–Ω—ã–µ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏
                category_groups[category].append(standard_id)
    
    # –î–ª—è –∫–∞–∂–¥–æ–π –∫–∞—Ç–µ–≥–æ—Ä–∏–∏
    for category, standards in category_groups.items():
        if len(standards) > 1:
            print(f"  {GREEN}‚Ä¢ –ö–∞—Ç–µ–≥–æ—Ä–∏—è: {category}{ENDC}")
            print(f"    –°—Ç–∞–Ω–¥–∞—Ä—Ç–æ–≤ –≤ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏: {len(standards)}")
            
            # –ù–∞—Ö–æ–¥–∏–º —Å–∏–ª—å–Ω–æ –ø–æ—Ö–æ–∂–∏–µ —Å—Ç–∞–Ω–¥–∞—Ä—Ç—ã –≤ —ç—Ç–æ–π –∫–∞—Ç–µ–≥–æ—Ä–∏–∏
            category_duplicates = []
            for i, standard1_id in enumerate(standards):
                for j in range(i+1, len(standards)):
                    standard2_id = standards[j]
                    if standard2_id in overall_similarity.get(standard1_id, {}):
                        similarity = overall_similarity[standard1_id][standard2_id]
                        if similarity > 0.4:  # –ü–æ—Ä–æ–≥ –¥–ª—è —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏—è
                            category_duplicates.append((standard1_id, standard2_id, similarity))
            
            # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ —Å—Ö–æ–¥—Å—Ç–≤—É
            category_duplicates.sort(key=lambda x: x[2], reverse=True)
            
            if category_duplicates:
                print(f"    –†–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è –æ–±—ä–µ–¥–∏–Ω–∏—Ç—å:")
                for standard1_id, standard2_id, similarity in category_duplicates:
                    print(f"      - {standards_data[standard1_id]['metadata'].get('title', standards_data[standard1_id]['file'])} –∏ {standards_data[standard2_id]['metadata'].get('title', standards_data[standard2_id]['file'])}")
                    print(f"        –°—Ö–æ–¥—Å—Ç–≤–æ: {similarity:.2%}")
            print()
    
    # –û–±—â–∏–µ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏
    if duplicates:
        print(f"  {BOLD}–†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è 1:{ENDC} –£–¥–∞–ª–∏—Ç—å —è–≤–Ω—ã–µ –¥—É–±–ª–∏–∫–∞—Ç—ã –∏ –æ—Å—Ç–∞–≤–∏—Ç—å —Ç–æ–ª—å–∫–æ –ø–æ—Å–ª–µ–¥–Ω–∏–µ –≤–µ—Ä—Å–∏–∏ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–æ–≤.")
    
    print(f"  {BOLD}–†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è 2:{ENDC} –û–±—ä–µ–¥–∏–Ω–∏—Ç—å —Å—Ç–∞–Ω–¥–∞—Ä—Ç—ã —Å –≤—ã—Å–æ–∫–∏–º –ø–µ—Ä–µ—Å–µ—á–µ–Ω–∏–µ–º —Å–æ–¥–µ—Ä–∂–∞–Ω–∏—è –≤ —Ä–∞–º–∫–∞—Ö –æ–¥–Ω–æ–π –∫–∞—Ç–µ–≥–æ—Ä–∏–∏.")
    print(f"  {BOLD}–†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è 3:{ENDC} –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å —Å–µ–∫—Ü–∏—é 'integrated' –¥–ª—è —É–∫–∞–∑–∞–Ω–∏—è –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π –≤–º–µ—Å—Ç–æ –¥—É–±–ª–∏—Ä–æ–≤–∞–Ω–∏—è –∫–æ–Ω—Ç–µ–Ω—Ç–∞.")
    print(f"  {BOLD}–†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è 4:{ENDC} –°–æ–∑–¥–∞—Ç—å –∏–µ—Ä–∞—Ä—Ö–∏—é —Å—Ç–∞–Ω–¥–∞—Ä—Ç–æ–≤, –≥–¥–µ –æ–±—â–∏–µ –ø—Ä–∏–Ω—Ü–∏–ø—ã –≤—ã–Ω–µ—Å–µ–Ω—ã –≤ —Ä–æ–¥–∏—Ç–µ–ª—å—Å–∫–∏–µ —Å—Ç–∞–Ω–¥–∞—Ä—Ç—ã.")
    
    print(f"\n{BOLD}–≠–ö–°–ü–û–†–¢ –†–ï–ó–£–õ–¨–¢–ê–¢–û–í:{ENDC}")
    export_file = "standards_overlap_analysis.json"
    with open(export_file, 'w', encoding='utf-8') as f:
        json.dump({
            'duplicates': duplicates,
            'section_overlap': {k: dict(v) for k, v in section_overlap.items()},
            'overall_similarity': {k: dict(v) for k, v in overall_similarity.items()}
        }, f, ensure_ascii=False, indent=2)
    print(f"  –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –∞–Ω–∞–ª–∏–∑–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ —Ñ–∞–π–ª: {export_file}")


def analyze_standards_directory(base_dir="."):
    """–ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç —Å—Ç–∞–Ω–¥–∞—Ä—Ç—ã –≤ —É–∫–∞–∑–∞–Ω–Ω–æ–π –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏."""
    # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –ø—É—Ç—å –≤ –æ–±—ä–µ–∫—Ç Path –¥–ª—è –∫—Ä–æ—Å—Å-–ø–ª–∞—Ç—Ñ–æ—Ä–º–µ–Ω–Ω–æ–π —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏
    base_path = Path(base_dir)
    
    print(f"\n{BOLD}–ê–Ω–∞–ª–∏–∑ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–æ–≤ –≤ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏: {base_path}{ENDC}\n")
    
    # –°–æ–±–∏—Ä–∞–µ–º –≤—Å–µ —Ñ–∞–π–ª—ã .md —Ä–µ–∫—É—Ä—Å–∏–≤–Ω–æ
    md_files = []
    import glob
    
    for file_path in glob.glob(str(base_path) + '/**/*.md', recursive=True):
        path = Path(file_path)
        # –ò—Å–∫–ª—é—á–∞–µ–º —Ñ–∞–π–ª—ã –≤ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—è—Ö archive
        if "archive" not in str(path).lower():
            md_files.append(path)
    
    print(f"–ù–∞–π–¥–µ–Ω–æ {len(md_files)} —Ñ–∞–π–ª–æ–≤ .md (–∏—Å–∫–ª—é—á–∞—è –∞—Ä—Ö–∏–≤—ã)")
    
    # –°–æ–±–∏—Ä–∞–µ–º –¥–∞–Ω–Ω—ã–µ –æ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–∞—Ö
    standards_data = {}
    
    for file_path in md_files:
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                content = f.read()
            
            # –ò–∑–≤–ª–µ–∫–∞–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –∏ —Ä–∞–∑–¥–µ–ª—ã
            metadata = extract_metadata(content)
            sections = extract_sections(content)
            
            # –ï—Å–ª–∏ –Ω–µ—Ç –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö –∏–ª–∏ —Ä–∞–∑–¥–µ–ª–æ–≤, –ø—Ä–æ–ø—É—Å–∫–∞–µ–º —Ñ–∞–π–ª
            if not metadata or not sections:
                print(f"  {YELLOW}–ü—Ä–æ–ø—É—Å–∫ —Ñ–∞–π–ª–∞ (–Ω–µ –ø–æ—Ö–æ–∂ –Ω–∞ —Å—Ç–∞–Ω–¥–∞—Ä—Ç): {file_path}{ENDC}")
                continue
            
            # –°–æ–∑–¥–∞–µ–º —É–Ω–∏–∫–∞–ª—å–Ω—ã–π –∏–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ç–æ—Ä –¥–ª—è —Å—Ç–∞–Ω–¥–∞—Ä—Ç–∞
            standard_id = metadata.get('standard_id') or metadata.get('logical_id') or str(file_path.name)
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –¥–∞–Ω–Ω—ã–µ –æ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–µ
            standards_data[standard_id] = {
                'file': str(file_path),
                'metadata': metadata,
                'sections': sections
            }
            
            print(f"  {GREEN}–û–±—Ä–∞–±–æ—Ç–∞–Ω —Å—Ç–∞–Ω–¥–∞—Ä—Ç: {standard_id} - {file_path.name}{ENDC}")
            
        except Exception as e:
            print(f"  {RED}–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ —Ñ–∞–π–ª–∞ {file_path}: {e}{ENDC}")
    
    print(f"\n–ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–æ {len(standards_data)} —Å—Ç–∞–Ω–¥–∞—Ä—Ç–æ–≤")
    
    # –ï—Å–ª–∏ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–æ–≤ –º–µ–Ω—å—à–µ 2, –Ω–µ—á–µ–≥–æ –∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å
    if len(standards_data) < 2:
        print(f"{YELLOW}–ù–∞–π–¥–µ–Ω–æ –º–µ–Ω—å—à–µ 2 —Å—Ç–∞–Ω–¥–∞—Ä—Ç–æ–≤. –ê–Ω–∞–ª–∏–∑ –ø–µ—Ä–µ—Å–µ—á–µ–Ω–∏–π –Ω–µ–≤–æ–∑–º–æ–∂–µ–Ω.{ENDC}")
        return
    
    # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –ø–µ—Ä–µ—Å–µ—á–µ–Ω–∏—è
    section_overlap = analyze_section_overlap(standards_data)
    overall_similarity = analyze_overall_similarity(standards_data)
    duplicates = identify_duplicates(overall_similarity)
    
    # –í—ã–≤–æ–¥–∏–º –æ—Ç—á–µ—Ç
    print_overlap_report(standards_data, section_overlap, overall_similarity, duplicates)


def main():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è."""
    parser = argparse.ArgumentParser(description='–ê–Ω–∞–ª–∏–∑ –ø–µ—Ä–µ—Å–µ—á–µ–Ω–∏–π —Å–æ–¥–µ—Ä–∂–∏–º–æ–≥–æ –º–µ–∂–¥—É —Å—Ç–∞–Ω–¥–∞—Ä—Ç–∞–º–∏')
    parser.add_argument('directory', nargs='?', default=".", help='–î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è —Å–æ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–∞–º–∏ (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é: —Ç–µ–∫—É—â–∞—è)')
    args = parser.parse_args()
    
    analyze_standards_directory(args.directory)


if __name__ == "__main__":
    main()