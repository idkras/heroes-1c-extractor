"""
Модуль верификации и синхронизации кеша с файловой системой.

Предоставляет инструменты для проверки согласованности между кешем и
файловой системой, а также для исправления обнаруженных несоответствий.

Автор: AI Assistant
Дата: 20 мая 2025
"""

import os
import json
import time
import logging
import glob
from typing import Dict, List, Set, Tuple, Any, Optional, TypedDict, TypeGuard, Union

# Настройка логирования
logger = logging.getLogger("cache_sync_verifier")
handler = logging.StreamHandler()
formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')
handler.setFormatter(formatter)
logger.addHandler(handler)
logger.setLevel(logging.INFO)


class CacheEntry(TypedDict, total=False):
    """Структура записи кеша для типизации."""
    size: int
    last_modified: float
    hash_value: Optional[str]
    cached_at: float
    metadata: Dict[str, Any]


class CacheSyncVerifier:
    """
    Верификатор синхронизации кеша с файловой системой.
    
    Проверяет соответствие между записями в кеше и реальными файлами,
    выявляет несоответствия и предоставляет механизмы для их исправления.
    """
    
    def __init__(
        self,
        cache_paths: List[str] = None,
        base_dir: str = ".",
        ignored_dirs: List[str] = None,
        ignored_files: List[str] = None,
        max_file_size: int = 10 * 1024 * 1024  # 10 МБ
    ):
        """
        Инициализация верификатора.
        
        Args:
            cache_paths: Список путей к файлам кеша
            base_dir: Базовая директория для поиска файлов
            ignored_dirs: Список директорий для игнорирования
            ignored_files: Список файлов для игнорирования
            max_file_size: Максимальный размер файла для проверки
        """
        self.cache_paths = cache_paths or [".cache_state.json"]
        self.base_dir = base_dir
        self.ignored_dirs = ignored_dirs or [".git", "__pycache__", "node_modules", ".venv", "venv"]
        self.ignored_files = ignored_files or [".DS_Store", "Thumbs.db", ".gitignore"]
        self.max_file_size = max_file_size
        
        # Кеши, загруженные из файлов
        self.caches: Dict[str, Dict[str, CacheEntry]] = {}
        
        # Результаты проверки
        self.missing_in_cache: List[str] = []
        self.missing_in_filesystem: List[str] = []
        self.metadata_mismatch: List[Tuple[str, Dict[str, Any], Dict[str, Any]]] = []
    
    def _load_cache(self, cache_path: str) -> Dict[str, CacheEntry]:
        """
        Загружает кеш из файла.
        
        Args:
            cache_path: Путь к файлу кеша
            
        Returns:
            Словарь с данными кеша или пустой словарь в случае ошибки
        """
        if not os.path.exists(cache_path):
            logger.warning(f"Файл кеша {cache_path} не существует")
            return {}
        
        try:
            with open(cache_path, "r", encoding="utf-8") as f:
                cache_data = json.load(f)
                
                # Проверяем, что загруженные данные имеют правильный формат
                if not isinstance(cache_data, dict):
                    logger.error(f"Некорректный формат кеша в файле {cache_path}")
                    return {}
                
                # Фильтруем неправильные записи
                valid_cache: Dict[str, CacheEntry] = {}
                for key, value in cache_data.items():
                    if isinstance(key, str) and isinstance(value, dict):
                        # Проверяем обязательные поля
                        if "size" in value and "last_modified" in value:
                            valid_cache[key] = value
                
                return valid_cache
        except (json.JSONDecodeError, IOError) as e:
            logger.error(f"Ошибка при чтении кеша из {cache_path}: {e}")
            return {}
    
    def _save_cache(self, cache_path: str, cache_data: Dict[str, CacheEntry]) -> bool:
        """
        Сохраняет кеш в файл.
        
        Args:
            cache_path: Путь к файлу кеша
            cache_data: Словарь с данными кеша
            
        Returns:
            True, если сохранение успешно, иначе False
        """
        try:
            # Создаем директорию, если её нет
            os.makedirs(os.path.dirname(os.path.abspath(cache_path)), exist_ok=True)
            
            # Сохраняем в файл
            with open(cache_path, "w", encoding="utf-8") as f:
                json.dump(cache_data, f, indent=2, ensure_ascii=False)
            
            return True
        except IOError as e:
            logger.error(f"Ошибка при сохранении кеша в {cache_path}: {e}")
            return False
    
    def _get_file_metadata(self, file_path: str) -> Optional[CacheEntry]:
        """
        Получает метаданные файла.
        
        Args:
            file_path: Путь к файлу
            
        Returns:
            Метаданные файла или None, если файл не существует
        """
        if not os.path.exists(file_path) or not os.path.isfile(file_path):
            return None
        
        try:
            stats = os.stat(file_path)
            
            # Проверяем размер файла
            if stats.st_size > self.max_file_size:
                logger.warning(f"Файл {file_path} слишком большой для проверки ({stats.st_size} байт)")
                # Для больших файлов возвращаем только базовые метаданные
                return {
                    "size": stats.st_size,
                    "last_modified": stats.st_mtime,
                    "cached_at": time.time()
                }
            
            # Для обычных файлов возвращаем полные метаданные
            return {
                "size": stats.st_size,
                "last_modified": stats.st_mtime,
                "cached_at": time.time()
            }
        except OSError as e:
            logger.error(f"Ошибка при получении метаданных файла {file_path}: {e}")
            return None
    
    def _get_files_in_filesystem(self) -> List[str]:
        """
        Получает список всех файлов в файловой системе (с учетом игнорируемых).
        
        Returns:
            Список путей к файлам
        """
        files = []
        
        # Нормализуем пути к игнорируемым директориям
        ignored_dirs_abs = [os.path.abspath(os.path.join(self.base_dir, d)) for d in self.ignored_dirs]
        
        # Обходим все файлы в базовой директории
        for root, dirs, filenames in os.walk(self.base_dir):
            # Исключаем игнорируемые директории
            dirs_to_remove = []
            for d in dirs:
                abs_dir = os.path.abspath(os.path.join(root, d))
                if any(abs_dir.startswith(ignored_dir) for ignored_dir in ignored_dirs_abs):
                    dirs_to_remove.append(d)
            
            for d in dirs_to_remove:
                dirs.remove(d)
            
            # Обрабатываем файлы
            for filename in filenames:
                if filename in self.ignored_files:
                    continue
                
                file_path = os.path.join(root, filename)
                
                # Проверяем, что это обычный файл
                if os.path.isfile(file_path):
                    # Нормализуем путь к файлу
                    file_path = os.path.normpath(file_path)
                    files.append(file_path)
        
        return files
    
    def _get_files_in_cache(self) -> Dict[str, Set[str]]:
        """
        Получает список всех файлов в кеше.
        
        Returns:
            Словарь {путь к кешу: множество файлов}
        """
        result = {}
        
        # Загружаем все кеши
        for cache_path in self.cache_paths:
            cache_data = self._load_cache(cache_path)
            self.caches[cache_path] = cache_data
            result[cache_path] = set(cache_data.keys())
        
        return result
    
    def verify_sync(self) -> Tuple[List[str], List[str], List[Tuple[str, Dict[str, Any], Dict[str, Any]]]]:
        """
        Проверяет синхронизацию между кешем и файловой системой.
        
        Returns:
            Кортеж (файлы, отсутствующие в кеше; файлы, отсутствующие в файловой системе; 
            файлы с несоответствием метаданных)
        """
        logger.info("Начало проверки синхронизации кеша с файловой системой")
        
        # Сбрасываем результаты предыдущей проверки
        self.missing_in_cache = []
        self.missing_in_filesystem = []
        self.metadata_mismatch = []
        
        # Получаем список всех файлов в файловой системе
        filesystem_files = set(self._get_files_in_filesystem())
        logger.info(f"Найдено {len(filesystem_files)} файлов в файловой системе")
        
        # Получаем список всех файлов в кеше
        cache_files = self._get_files_in_cache()
        
        # Объединяем все файлы из всех кешей
        all_cache_files = set()
        for files in cache_files.values():
            all_cache_files.update(files)
        
        logger.info(f"Найдено {len(all_cache_files)} записей во всех кешах")
        
        # Находим файлы, которые есть в файловой системе, но отсутствуют в кеше
        self.missing_in_cache = sorted(list(filesystem_files - all_cache_files))
        logger.info(f"Найдено {len(self.missing_in_cache)} файлов, отсутствующих в кеше")
        
        # Находим файлы, которые есть в кеше, но отсутствуют в файловой системе
        self.missing_in_filesystem = sorted(list(all_cache_files - filesystem_files))
        logger.info(f"Найдено {len(self.missing_in_filesystem)} файлов, отсутствующих в файловой системе")
        
        # Проверяем соответствие метаданных для файлов, которые есть и в кеше, и в файловой системе
        self.metadata_mismatch = []
        common_files = filesystem_files.intersection(all_cache_files)
        logger.info(f"Проверка соответствия метаданных для {len(common_files)} файлов")
        
        for file_path in common_files:
            # Получаем метаданные из файловой системы
            current_metadata = self._get_file_metadata(file_path)
            
            if current_metadata is None:
                logger.warning(f"Не удалось получить метаданные для файла {file_path}")
                continue
            
            # Проверяем метаданные в каждом кеше
            for cache_path, cache_data in self.caches.items():
                if file_path in cache_data:
                    cache_entry = cache_data[file_path]
                    
                    # Проверяем соответствие размера
                    if "size" in cache_entry and cache_entry["size"] != current_metadata["size"]:
                        self.metadata_mismatch.append((file_path, cache_entry, current_metadata))
                        break
                    
                    # Проверяем соответствие времени изменения (с точностью до 1 секунды)
                    if "last_modified" in cache_entry and abs(cache_entry["last_modified"] - current_metadata["last_modified"]) > 1.0:
                        self.metadata_mismatch.append((file_path, cache_entry, current_metadata))
                        break
        
        logger.info(f"Найдено {len(self.metadata_mismatch)} файлов с несоответствием метаданных")
        
        return self.missing_in_cache, self.missing_in_filesystem, self.metadata_mismatch
    
    def fix_sync_issues(self) -> bool:
        """
        Исправляет выявленные несоответствия между кешем и файловой системой.
        
        Returns:
            True, если все исправления выполнены успешно, иначе False
        """
        logger.info("Начало исправления несоответствий")
        
        # Если проверка не была выполнена, выполняем её
        if not self.missing_in_cache and not self.missing_in_filesystem and not self.metadata_mismatch:
            logger.info("Выполняем проверку перед исправлением")
            self.verify_sync()
        
        success = True
        
        # Исправляем каждый кеш отдельно
        for cache_path, cache_data in self.caches.items():
            # Создаем копию кеша для изменений
            updated_cache = cache_data.copy()
            
            # 1. Добавляем в кеш отсутствующие файлы
            for file_path in self.missing_in_cache:
                metadata = self._get_file_metadata(file_path)
                if metadata:
                    updated_cache[file_path] = metadata
                    logger.debug(f"Добавлен файл {file_path} в кеш {cache_path}")
            
            # 2. Удаляем из кеша несуществующие файлы
            for file_path in self.missing_in_filesystem:
                if file_path in updated_cache:
                    del updated_cache[file_path]
                    logger.debug(f"Удален файл {file_path} из кеша {cache_path}")
            
            # 3. Обновляем метаданные для файлов с несоответствиями
            for file_path, _, current_metadata in self.metadata_mismatch:
                if file_path in updated_cache:
                    updated_cache[file_path] = current_metadata
                    logger.debug(f"Обновлены метаданные для файла {file_path} в кеше {cache_path}")
            
            # Сохраняем обновленный кеш
            if not self._save_cache(cache_path, updated_cache):
                logger.error(f"Не удалось сохранить обновленный кеш {cache_path}")
                success = False
        
        if success:
            logger.info("Все несоответствия успешно исправлены")
            
            # Очищаем списки несоответствий, так как они исправлены
            self.missing_in_cache = []
            self.missing_in_filesystem = []
            self.metadata_mismatch = []
        else:
            logger.warning("Не все несоответствия удалось исправить")
        
        return success
    
    def get_sync_status(self) -> Dict[str, Any]:
        """
        Получает статус синхронизации.
        
        Returns:
            Словарь со статусом синхронизации
        """
        # Если проверка не была выполнена, выполняем её
        if not self.missing_in_cache and not self.missing_in_filesystem and not self.metadata_mismatch:
            self.verify_sync()
        
        # Определяем общий статус синхронизации
        is_synced = (len(self.missing_in_cache) + len(self.missing_in_filesystem) + len(self.metadata_mismatch)) == 0
        
        return {
            "is_synced": is_synced,
            "missing_in_cache_count": len(self.missing_in_cache),
            "missing_in_filesystem_count": len(self.missing_in_filesystem),
            "metadata_mismatch_count": len(self.metadata_mismatch),
            "missing_in_cache": self.missing_in_cache[:10],  # Только первые 10 для краткости
            "missing_in_cache_truncated": len(self.missing_in_cache) > 10,
            "missing_in_filesystem": self.missing_in_filesystem[:10],  # Только первые 10 для краткости
            "missing_in_filesystem_truncated": len(self.missing_in_filesystem) > 10,
            "metadata_mismatch": [
                {
                    "file_path": file_path,
                    "cache_entry": cache_entry,
                    "current_metadata": current_metadata
                }
                for file_path, cache_entry, current_metadata in self.metadata_mismatch[:10]  # Только первые 10
            ],
            "metadata_mismatch_truncated": len(self.metadata_mismatch) > 10,
            "caches": [
                {
                    "path": cache_path,
                    "entry_count": len(cache_data)
                }
                for cache_path, cache_data in self.caches.items()
            ]
        }
    
    def optimize_cache(self) -> bool:
        """
        Оптимизирует кеш, удаляя устаревшие записи и оптимизируя структуру.
        
        Returns:
            True, если оптимизация выполнена успешно, иначе False
        """
        logger.info("Начало оптимизации кеша")
        
        # Если кеши не были загружены, загружаем их
        if not self.caches:
            self._get_files_in_cache()
        
        success = True
        
        # Оптимизируем каждый кеш отдельно
        for cache_path, cache_data in self.caches.items():
            # Фильтруем устаревшие записи
            current_time = time.time()
            max_age = 30 * 24 * 60 * 60  # 30 дней
            
            # Создаем копию кеша для изменений
            optimized_cache = {}
            
            for file_path, entry in cache_data.items():
                # Проверяем, существует ли файл
                if os.path.exists(file_path) and os.path.isfile(file_path):
                    # Проверяем, не устарела ли запись
                    cached_at = entry.get("cached_at", 0)
                    if (current_time - cached_at) <= max_age:
                        # Добавляем актуальную запись в оптимизированный кеш
                        optimized_cache[file_path] = entry
            
            # Сохраняем оптимизированный кеш
            if not self._save_cache(cache_path, optimized_cache):
                logger.error(f"Не удалось сохранить оптимизированный кеш {cache_path}")
                success = False
            else:
                removed_entries = len(cache_data) - len(optimized_cache)
                logger.info(f"Удалено {removed_entries} устаревших записей из кеша {cache_path}")
                
                # Обновляем данные кеша
                self.caches[cache_path] = optimized_cache
        
        if success:
            logger.info("Оптимизация кеша успешно завершена")
        else:
            logger.warning("Не удалось полностью оптимизировать кеш")
        
        return success
    
    def rebuild_cache(self) -> bool:
        """
        Полностью перестраивает кеш на основе текущего состояния файловой системы.
        
        Returns:
            True, если перестроение выполнено успешно, иначе False
        """
        logger.info("Начало полного перестроения кеша")
        
        # Получаем список всех файлов в файловой системе
        filesystem_files = self._get_files_in_filesystem()
        logger.info(f"Найдено {len(filesystem_files)} файлов в файловой системе")
        
        success = True
        
        # Перестраиваем каждый кеш
        for cache_path in self.cache_paths:
            # Создаем новый пустой кеш
            new_cache = {}
            
            # Заполняем кеш метаданными файлов
            for file_path in filesystem_files:
                metadata = self._get_file_metadata(file_path)
                if metadata:
                    new_cache[file_path] = metadata
            
            # Сохраняем новый кеш
            if not self._save_cache(cache_path, new_cache):
                logger.error(f"Не удалось сохранить перестроенный кеш {cache_path}")
                success = False
            else:
                logger.info(f"Кеш {cache_path} успешно перестроен с {len(new_cache)} записями")
                
                # Обновляем данные кеша
                self.caches[cache_path] = new_cache
        
        if success:
            logger.info("Полное перестроение кеша успешно завершено")
            
            # Очищаем списки несоответствий, так как они исправлены
            self.missing_in_cache = []
            self.missing_in_filesystem = []
            self.metadata_mismatch = []
        else:
            logger.warning("Не удалось полностью перестроить кеш")
        
        return success
    
    def analyze_filesystem_changes(self) -> Dict[str, Any]:
        """
        Анализирует изменения в файловой системе по сравнению с кешем.
        
        Returns:
            Словарь с результатами анализа
        """
        logger.info("Начало анализа изменений в файловой системе")
        
        # Если проверка не была выполнена, выполняем её
        if not self.missing_in_cache and not self.missing_in_filesystem and not self.metadata_mismatch:
            self.verify_sync()
        
        # Анализируем изменения
        added_files = self.missing_in_cache
        removed_files = self.missing_in_filesystem
        modified_files = [file_path for file_path, _, _ in self.metadata_mismatch]
        
        # Группируем файлы по расширениям
        extensions = {}
        
        for file_path in added_files + removed_files + modified_files:
            ext = os.path.splitext(file_path)[1].lower()
            if ext not in extensions:
                extensions[ext] = {"added": 0, "removed": 0, "modified": 0}
            
            if file_path in added_files:
                extensions[ext]["added"] += 1
            elif file_path in removed_files:
                extensions[ext]["removed"] += 1
            elif file_path in modified_files:
                extensions[ext]["modified"] += 1
        
        # Группируем файлы по директориям
        directories = {}
        
        for file_path in added_files + removed_files + modified_files:
            directory = os.path.dirname(file_path)
            if directory not in directories:
                directories[directory] = {"added": 0, "removed": 0, "modified": 0}
            
            if file_path in added_files:
                directories[directory]["added"] += 1
            elif file_path in removed_files:
                directories[directory]["removed"] += 1
            elif file_path in modified_files:
                directories[directory]["modified"] += 1
        
        # Формируем отчет
        report = {
            "total_changes": len(added_files) + len(removed_files) + len(modified_files),
            "added_files_count": len(added_files),
            "removed_files_count": len(removed_files),
            "modified_files_count": len(modified_files),
            "added_files": added_files[:20],  # Только первые 20 для краткости
            "removed_files": removed_files[:20],  # Только первые 20 для краткости
            "modified_files": modified_files[:20],  # Только первые 20 для краткости
            "extensions": extensions,
            "directories": {
                directory: stats
                for directory, stats in sorted(
                    directories.items(),
                    key=lambda x: sum(x[1].values()),
                    reverse=True
                )[:20]  # Только первые 20 директорий с наибольшим количеством изменений
            }
        }
        
        return report


def is_callable(obj: object) -> TypeGuard[callable]:
    """Проверяет, является ли объект вызываемым."""
    return callable(obj)


def main():
    """Основная функция модуля."""
    import argparse
    
    # Настройка парсера аргументов
    parser = argparse.ArgumentParser(description="Инструменты для проверки и синхронизации кеша с файловой системой")
    
    # Добавляем подкоманды
    subparsers = parser.add_subparsers(dest="command", help="Команда для выполнения")
    
    # Подкоманда для проверки синхронизации
    verify_parser = subparsers.add_parser("verify", help="Проверить синхронизацию кеша с файловой системой")
    verify_parser.add_argument("--cache", default=".cache_state.json", help="Путь к файлу кеша")
    verify_parser.add_argument("--base-dir", default=".", help="Базовая директория для поиска файлов")
    verify_parser.add_argument("--verbose", action="store_true", help="Подробный вывод")
    
    # Подкоманда для исправления несоответствий
    fix_parser = subparsers.add_parser("fix", help="Исправить несоответствия между кешем и файловой системой")
    fix_parser.add_argument("--cache", default=".cache_state.json", help="Путь к файлу кеша")
    fix_parser.add_argument("--base-dir", default=".", help="Базовая директория для поиска файлов")
    
    # Подкоманда для полного перестроения кеша
    rebuild_parser = subparsers.add_parser("rebuild", help="Полностью перестроить кеш")
    rebuild_parser.add_argument("--cache", default=".cache_state.json", help="Путь к файлу кеша")
    rebuild_parser.add_argument("--base-dir", default=".", help="Базовая директория для поиска файлов")
    
    # Подкоманда для оптимизации кеша
    optimize_parser = subparsers.add_parser("optimize", help="Оптимизировать кеш")
    optimize_parser.add_argument("--cache", default=".cache_state.json", help="Путь к файлу кеша")
    
    # Подкоманда для получения статуса синхронизации
    status_parser = subparsers.add_parser("status", help="Получить статус синхронизации")
    status_parser.add_argument("--cache", default=".cache_state.json", help="Путь к файлу кеша")
    status_parser.add_argument("--base-dir", default=".", help="Базовая директория для поиска файлов")
    status_parser.add_argument("--json", action="store_true", help="Вывод в формате JSON")
    
    # Подкоманда для анализа изменений
    analyze_parser = subparsers.add_parser("analyze", help="Анализировать изменения в файловой системе")
    analyze_parser.add_argument("--cache", default=".cache_state.json", help="Путь к файлу кеша")
    analyze_parser.add_argument("--base-dir", default=".", help="Базовая директория для поиска файлов")
    analyze_parser.add_argument("--json", action="store_true", help="Вывод в формате JSON")
    
    # Разбор аргументов
    args = parser.parse_args()
    
    # Установка уровня логирования
    if getattr(args, "verbose", False):
        logger.setLevel(logging.DEBUG)
    
    # Выполняем команду
    if args.command == "verify":
        # Создаем верификатор
        verifier = CacheSyncVerifier(
            cache_paths=[args.cache],
            base_dir=args.base_dir
        )
        
        # Выполняем проверку
        missing_in_cache, missing_in_filesystem, metadata_mismatch = verifier.verify_sync()
        
        # Выводим результаты
        print(f"Файлы, отсутствующие в кеше: {len(missing_in_cache)}")
        if args.verbose and missing_in_cache:
            for file_path in missing_in_cache[:10]:
                print(f"  - {file_path}")
            if len(missing_in_cache) > 10:
                print(f"  ... и еще {len(missing_in_cache) - 10} файлов")
        
        print(f"Файлы, отсутствующие в файловой системе: {len(missing_in_filesystem)}")
        if args.verbose and missing_in_filesystem:
            for file_path in missing_in_filesystem[:10]:
                print(f"  - {file_path}")
            if len(missing_in_filesystem) > 10:
                print(f"  ... и еще {len(missing_in_filesystem) - 10} файлов")
        
        print(f"Файлы с несоответствием метаданных: {len(metadata_mismatch)}")
        if args.verbose and metadata_mismatch:
            for file_path, cache_entry, current_metadata in metadata_mismatch[:10]:
                print(f"  - {file_path}")
                print(f"    В кеше: размер={cache_entry.get('size', 'N/A')}, время={cache_entry.get('last_modified', 'N/A')}")
                print(f"    Текущие: размер={current_metadata.get('size', 'N/A')}, время={current_metadata.get('last_modified', 'N/A')}")
            if len(metadata_mismatch) > 10:
                print(f"  ... и еще {len(metadata_mismatch) - 10} файлов")
        
        print(f"\nВсего несоответствий: {len(missing_in_cache) + len(missing_in_filesystem) + len(metadata_mismatch)}")
    
    elif args.command == "fix":
        # Создаем верификатор
        verifier = CacheSyncVerifier(
            cache_paths=[args.cache],
            base_dir=args.base_dir
        )
        
        # Выполняем проверку
        missing_in_cache, missing_in_filesystem, metadata_mismatch = verifier.verify_sync()
        
        # Выводим результаты
        print(f"Найдено несоответствий: {len(missing_in_cache) + len(missing_in_filesystem) + len(metadata_mismatch)}")
        print(f"  - Файлы, отсутствующие в кеше: {len(missing_in_cache)}")
        print(f"  - Файлы, отсутствующие в файловой системе: {len(missing_in_filesystem)}")
        print(f"  - Файлы с несоответствием метаданных: {len(metadata_mismatch)}")
        
        # Исправляем несоответствия
        print("\nИсправление несоответствий...")
        if verifier.fix_sync_issues():
            print("Все несоответствия успешно исправлены")
        else:
            print("Не все несоответствия удалось исправить")
    
    elif args.command == "rebuild":
        # Создаем верификатор
        verifier = CacheSyncVerifier(
            cache_paths=[args.cache],
            base_dir=args.base_dir
        )
        
        # Перестраиваем кеш
        print("Полное перестроение кеша...")
        if verifier.rebuild_cache():
            print("Кеш успешно перестроен")
        else:
            print("Не удалось перестроить кеш")
    
    elif args.command == "optimize":
        # Создаем верификатор
        verifier = CacheSyncVerifier(
            cache_paths=[args.cache]
        )
        
        # Оптимизируем кеш
        print("Оптимизация кеша...")
        if verifier.optimize_cache():
            print("Кеш успешно оптимизирован")
        else:
            print("Не удалось оптимизировать кеш")
    
    elif args.command == "status":
        # Создаем верификатор
        verifier = CacheSyncVerifier(
            cache_paths=[args.cache],
            base_dir=args.base_dir
        )
        
        # Получаем статус
        status = verifier.get_sync_status()
        
        # Выводим статус
        if args.json:
            import json
            print(json.dumps(status, indent=2))
        else:
            print(f"Статус синхронизации: {'✅ Синхронизировано' if status['is_synced'] else '❌ Не синхронизировано'}")
            print(f"Файлы, отсутствующие в кеше: {status['missing_in_cache_count']}")
            print(f"Файлы, отсутствующие в файловой системе: {status['missing_in_filesystem_count']}")
            print(f"Файлы с несоответствием метаданных: {status['metadata_mismatch_count']}")
            
            for cache_info in status['caches']:
                print(f"Кеш {cache_info['path']}: {cache_info['entry_count']} записей")
    
    elif args.command == "analyze":
        # Создаем верификатор
        verifier = CacheSyncVerifier(
            cache_paths=[args.cache],
            base_dir=args.base_dir
        )
        
        # Анализируем изменения
        analysis = verifier.analyze_filesystem_changes()
        
        # Выводим результаты анализа
        if args.json:
            import json
            print(json.dumps(analysis, indent=2))
        else:
            print(f"Всего изменений: {analysis['total_changes']}")
            print(f"  - Добавлено файлов: {analysis['added_files_count']}")
            print(f"  - Удалено файлов: {analysis['removed_files_count']}")
            print(f"  - Изменено файлов: {analysis['modified_files_count']}")
            
            print("\nИзменения по расширениям:")
            for ext, counts in sorted(analysis['extensions'].items(), key=lambda x: sum(x[1].values()), reverse=True)[:10]:
                if not ext:
                    ext = "(без расширения)"
                print(f"  - {ext}: +{counts['added']}, -{counts['removed']}, ~{counts['modified']}")
            
            print("\nИзменения по директориям (топ-5):")
            dir_count = 0
            for directory, counts in analysis['directories'].items():
                if dir_count >= 5:
                    break
                print(f"  - {directory}: +{counts['added']}, -{counts['removed']}, ~{counts['modified']}")
                dir_count += 1
    
    else:
        parser.print_help()


if __name__ == "__main__":
    main()