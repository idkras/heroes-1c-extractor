"""
Unified Standards System - –µ–¥–∏–Ω–∞—è —Ç–æ—á–∫–∞ –≤—Ö–æ–¥–∞ –¥–ª—è –≤—Å–µ—Ö –æ–ø–µ—Ä–∞—Ü–∏–π —Å–æ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–∞–º–∏.

–û—Å–Ω–æ–≤–∞–Ω–æ –Ω–∞ TDD-doc –ø—Ä–∏–Ω—Ü–∏–ø–µ "One Source of Truth" - –∫–æ–Ω—Å–æ–ª–∏–¥–∏—Ä—É–µ—Ç –≤—Å–µ
—Ñ—É–Ω–∫—Ü–∏–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–∏ –≤ –æ–¥–Ω–æ–º –º–µ—Å—Ç–µ –±–µ–∑ –¥—É–±–ª–∏—Ä–æ–≤–∞–Ω–∏—è.

–¶–µ–ª—å: –û–±–µ—Å–ø–µ—á–∏—Ç—å –ø–æ–ª–Ω–æ–µ –ø–æ–∫—Ä—ã—Ç–∏–µ –≤—Å–µ—Ö –∞—Å–ø–µ–∫—Ç–æ–≤ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–æ–≤ —á–µ—Ä–µ–∑ –µ–¥–∏–Ω—ã–π –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å
–±–µ–∑ –ø–æ—Ç–µ—Ä–∏ –¥–µ—Ç–∞–ª–µ–π –∏ –Ω—é–∞–Ω—Å–æ–≤.
"""

import duckdb
import json
import time
import hashlib
from pathlib import Path
from datetime import datetime
from typing import Dict, List, Any, Optional, Union
from datetime import datetime
import re
import logging

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class UnifiedStandardsSystem:
    """
    –ï–¥–∏–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞ –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å–æ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–∞–º–∏.
    
    –ò–Ω—Ç–µ–≥—Ä–∏—Ä—É–µ—Ç:
    - DuckDB –¥–ª—è –∞–Ω–∞–ª–∏—Ç–∏–∫–∏ –∏ —Å–≤—è–∑–µ–π
    - MCP –∫–æ–º–∞–Ω–¥—ã –¥–ª—è –¥–µ—Ç–∞–ª—å–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞
    - –ü–æ–∏—Å–∫ –∏ –∏–Ω–¥–µ–∫—Å–∏—Ä–æ–≤–∞–Ω–∏–µ
    - –í–∞–ª–∏–¥–∞—Ü–∏—é –∏ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ
    """
    
    def __init__(self, db_path: str = "standards_system.duckdb"):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –µ–¥–∏–Ω–æ–π —Å–∏—Å—Ç–µ–º—ã"""
        self.db_path = db_path
        self.conn = None
        self.operation_log = []
        
        self._ensure_connection()
        self._create_schema()
        self._initialize_data()
    
    def _ensure_connection(self):
        """–ù–∞–¥–µ–∂–Ω–æ–µ —Å–æ–µ–¥–∏–Ω–µ–Ω–∏–µ —Å DuckDB"""
        try:
            if self.conn is None or self._is_connection_closed():
                self.conn = duckdb.connect(self.db_path)
                logger.info(f"Standards system connected: {self.db_path}")
            return True
        except Exception as e:
            logger.error(f"Connection failed: {e}")
            return False
    
    def _is_connection_closed(self) -> bool:
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ —Å–æ—Å—Ç–æ—è–Ω–∏—è —Å–æ–µ–¥–∏–Ω–µ–Ω–∏—è"""
        try:
            if self.conn is None:
                return True
            self.conn.execute("SELECT 1")
            return False
        except:
            return True
    
    def _execute_safe(self, query: str, params: Optional[List] = None) -> Optional[Any]:
        """–ë–µ–∑–æ–ø–∞—Å–Ω–æ–µ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏–µ –∑–∞–ø—Ä–æ—Å–æ–≤"""
        max_retries = 3
        for attempt in range(max_retries):
            try:
                if not self._ensure_connection():
                    continue
                
                if self.conn is not None:
                    if params:
                        result = self.conn.execute(query, params)
                    else:
                        result = self.conn.execute(query)
                else:
                    return None
                return result
                
            except Exception as e:
                logger.warning(f"Query attempt {attempt + 1} failed: {e}")
                self.conn = None
                if attempt == max_retries - 1:
                    return None
        return None
    
    def _create_schema(self):
        """–°–æ–∑–¥–∞–Ω–∏–µ –µ–¥–∏–Ω–æ–π —Å—Ö–µ–º—ã –¥–ª—è –≤—Å–µ—Ö –¥–∞–Ω–Ω—ã—Ö —Å—Ç–∞–Ω–¥–∞—Ä—Ç–æ–≤"""
        
        # –û—Å–Ω–æ–≤–Ω–∞—è —Ç–∞–±–ª–∏—Ü–∞ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–æ–≤
        self._execute_safe("""
            CREATE TABLE IF NOT EXISTS standards (
                id VARCHAR PRIMARY KEY,
                name VARCHAR NOT NULL,
                path VARCHAR NOT NULL,
                category VARCHAR,
                content TEXT,
                description TEXT,
                content_hash VARCHAR,
                
                -- –ú–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
                version VARCHAR,
                author VARCHAR,
                status VARCHAR DEFAULT 'active',
                
                -- –°—Ç—Ä—É–∫—Ç—É—Ä–Ω—ã–µ —ç–ª–µ–º–µ–Ω—Ç—ã
                has_jtbd BOOLEAN DEFAULT FALSE,
                has_ai_protocols BOOLEAN DEFAULT FALSE,
                has_tdd_patterns BOOLEAN DEFAULT FALSE,
                
                -- –ú–µ—Ç—Ä–∏–∫–∏
                word_count INTEGER,
                complexity FLOAT,
                complexity_score FLOAT,
                completeness FLOAT,
                
                -- –í—Ä–µ–º–µ–Ω–Ω—ã–µ –º–µ—Ç–∫–∏
                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
            )
        """)
        
        # –¢–∞–±–ª–∏—Ü–∞ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π
        self._execute_safe("""
            CREATE TABLE IF NOT EXISTS dependencies (
                source_id VARCHAR,
                target_name VARCHAR,
                dependency_type VARCHAR,
                confidence_score FLOAT,
                context TEXT,
                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
            )
        """)
        
        # –¢–∞–±–ª–∏—Ü–∞ –æ–ø–µ—Ä–∞—Ü–∏–π (–¥–ª—è –∞—É–¥–∏—Ç–∞) 
        self._execute_safe("""
            CREATE TABLE IF NOT EXISTS operations_log (
                id INTEGER PRIMARY KEY,
                operation_type VARCHAR,
                details TEXT,
                success BOOLEAN,
                timestamp TIMESTAMP DEFAULT CURRENT_TIMESTAMP
            )
        """)
        
        # –£–±–∏—Ä–∞–µ–º sequence - –∏—Å–ø–æ–ª—å–∑—É–µ–º —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–π autoincrement
        
        # –ò–Ω–¥–µ–∫—Å—ã –¥–ª—è –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
        self._execute_safe("CREATE INDEX IF NOT EXISTS idx_standards_category ON standards(category)")
        self._execute_safe("CREATE INDEX IF NOT EXISTS idx_standards_name ON standards(name)")
        self._execute_safe("CREATE INDEX IF NOT EXISTS idx_dependencies_source ON dependencies(source_id)")
        
        logger.info("Unified schema created successfully")
    
    def _initialize_data(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö –∏–∑ —Ñ–∞–π–ª–æ–≤–æ–π —Å–∏—Å—Ç–µ–º—ã"""
        try:
            start_time = time.time()
            load_stats = self._load_standards_from_filesystem()
            duration = (time.time() - start_time) * 1000
            
            # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞ —É—Å–ø–µ—à–Ω–æ
            logger.info(f"System initialized: {load_stats.get('loaded_files', 0)} files loaded")
            
        except Exception as e:
            self._log_operation("system_initialization", {}, {"error": str(e)}, 0)
    
    def _load_standards_from_filesystem(self) -> Dict[str, int]:
        """–ó–∞–≥—Ä—É–∑–∫–∞ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–æ–≤ —Ç–æ–ª—å–∫–æ –∏–∑ –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ–π –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏ —Å –∏—Å–∫–ª—é—á–µ–Ω–∏–µ–º –∞—Ä—Ö–∏–≤–æ–≤"""
        stats = {"loaded_files": 0, "dependencies_found": 0, "total_files": 0}
        
        # –¢–æ–ª—å–∫–æ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—è –Ω–∞—Å—Ç–æ—è—â–∏—Ö —Å—Ç–∞–Ω–¥–∞—Ä—Ç–æ–≤ (–æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω–æ –∫–æ—Ä–Ω—è –ø—Ä–æ–µ–∫—Ç–∞)
        standards_dir = Path("..") / "[standards .md]"
        
        if not standards_dir.exists():
            logger.warning(f"Standards directory not found: {standards_dir}")
            return stats
            
        # –ü–æ–ª—É—á–∞–µ–º –≤—Å–µ .md —Ñ–∞–π–ª—ã, –∏—Å–∫–ª—é—á–∞—è –∞—Ä—Ö–∏–≤–Ω—ã–µ
        md_files = []
        for md_file in standards_dir.rglob("*.md"):
            file_path_str = str(md_file)
            
            # –ò—Å–∫–ª—é—á–∞–µ–º –∞—Ä—Ö–∏–≤–Ω—ã–µ —Ñ–∞–π–ª—ã –∏ –±—ç–∫–∞–ø—ã
            if any(pattern in file_path_str.lower() for pattern in [
                '[archive]', 'backup', 'archived', 'old', 'copy', 
                '20250514', '2025_05_14', 'template', 'consolidated_'
            ]):
                continue
                
            md_files.append(md_file)
        
        logger.info(f"Scanning {standards_dir}: found {len(md_files)} .md files (excluding archives)")
        stats["total_files"] = len(md_files)
        
        # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —Ç–æ–ª—å–∫–æ —Ä–µ–∞–ª—å–Ω—ã–µ —Å—Ç–∞–Ω–¥–∞—Ä—Ç—ã
        for md_file in md_files:
            try:
                self._process_standard_file(md_file, stats)
            except Exception as e:
                logger.warning(f"Failed to process {md_file}: {e}")
        
        logger.info(f"Loaded {stats['loaded_files']}/{stats['total_files']} standards with {stats['dependencies_found']} dependencies")
        return stats
    
    def _process_standard_file(self, md_file: Path, stats: Dict[str, int]):
        """–û–±—Ä–∞–±–æ—Ç–∫–∞ –æ—Ç–¥–µ–ª—å–Ω–æ–≥–æ —Ñ–∞–π–ª–∞ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–∞"""
        with open(md_file, 'r', encoding='utf-8') as f:
            content = f.read()
        
        analysis = self._analyze_standard(md_file, content)
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ –±–∞–∑—É –¥–∞–Ω–Ω—ã—Ö
        self._execute_safe("""
            INSERT INTO standards (
                id, name, path, content, category, description,
                complexity, completeness, created_at
            ) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?)
        """, [
            analysis['id'],
            analysis['name'],
            str(md_file),
            content,
            analysis['category'],
            analysis['description'],
            analysis['complexity'],
            analysis['completeness'],
            analysis['created_at']
        ])
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏
        for dep in analysis['dependencies']:
            self._execute_safe("""
                INSERT INTO dependencies (source_id, target_name, dependency_type, confidence_score)
                VALUES (?, ?, ?, ?)
            """, [analysis['id'], dep['target'], dep['type'], dep.get('strength', 0.5)])
            stats['dependencies_found'] += 1
        
        stats['loaded_files'] += 1
        
        # –ö–æ–º–º–∏—Ç –∏–∑–º–µ–Ω–µ–Ω–∏–π
        if self.conn:
            try:
                self.conn.commit()
            except:
                pass
        
        logger.info(f"Loaded {stats['loaded_files']}/{stats['total_files']} standards with {stats['dependencies_found']} dependencies")
        return stats
    
    def _analyze_standard(self, file_path: Path, content: str) -> Dict[str, Any]:
        """–ü–æ–ª–Ω—ã–π –∞–Ω–∞–ª–∏–∑ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–∞"""
        
        # –ë–∞–∑–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ
        try:
            standard_id = str(file_path.relative_to(Path("..") / "[standards .md]"))
        except ValueError:
            standard_id = str(file_path)
        content_hash = hashlib.md5(content.encode()).hexdigest()
        
        # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö
        metadata = self._extract_metadata(content)
        
        # –°—Ç—Ä—É–∫—Ç—É—Ä–Ω—ã–π –∞–Ω–∞–ª–∏–∑
        content_lower = content.lower()
        
        return {
            'id': standard_id,
            'name': metadata.get('name', file_path.stem),
            'path': str(file_path),
            'category': metadata.get('category', file_path.parent.name),
            'description': content[:200] + '...' if len(content) > 200 else content,
            'content_hash': content_hash,
            'version': metadata.get('version', 'unknown'),
            'author': metadata.get('author', 'unknown'),
            'complexity': self._calculate_complexity(content),
            'completeness': min(100, len(content.split()) // 10),
            'created_at': datetime.now().isoformat(),
            'dependencies': self._extract_dependencies(content),
            'has_jtbd': any(pattern in content_lower for pattern in ['jtbd', 'jobs to be done', '–∫–æ–≥–¥–∞.*—Ä–æ–ª—å.*—Ö–æ—á–µ—Ç']),
            'has_ai_protocols': any(pattern in content_lower for pattern in ['dual-check', 'no gaps', 'ai –ø—Ä–æ—Ç–æ–∫–æ–ª']),
            'has_tdd_patterns': any(pattern in content_lower for pattern in ['red.*green.*blue', 'tdd']),
            'word_count': len(content.split()),
            'complexity_score': self._calculate_complexity(content)
        }
    
    def _extract_metadata(self, content: str) -> Dict[str, Any]:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö –∏–∑ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–∞"""
        metadata = {}
        
        # –ü–æ–∏—Å–∫ –≤ protected section
        protected_match = re.search(r'<!-- protected section -->(.*?)<!-- /protected section -->', content, re.DOTALL)
        if protected_match:
            protected_content = protected_match.group(1)
            
            version_match = re.search(r'version:\s*([^\n]+)', protected_content, re.IGNORECASE)
            if version_match:
                metadata['version'] = version_match.group(1).strip()
            
            author_match = re.search(r'(?:by|author):\s*([^\n]+)', protected_content, re.IGNORECASE)
            if author_match:
                metadata['author'] = author_match.group(1).strip()
        
        # –ù–∞–∑–≤–∞–Ω–∏–µ –∏–∑ –∑–∞–≥–æ–ª–æ–≤–∫–∞
        title_match = re.search(r'^# (.+)$', content, re.MULTILINE)
        if title_match:
            metadata['name'] = title_match.group(1).strip()
        
        return metadata
    
    def _extract_dependencies(self, content: str) -> List[Dict[str, Any]]:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π –∏–∑ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–∞"""
        dependencies = []
        
        patterns = [
            (r'standard[:\s]+([^\s\n]+)', 'reference', 0.9),
            (r'based on:\s*([^\n]+)', 'inheritance', 0.8),
            (r'integrated:\s*([^\n]+)', 'integration', 0.7),
            (r'\[([^\]]+)\]\(abstract://standard:([^)]+)\)', 'formal_link', 1.0)
        ]
        
        for pattern, dep_type, confidence in patterns:
            matches = re.findall(pattern, content, re.IGNORECASE)
            for match in matches:
                target = match if isinstance(match, str) else match[0]
                target = target.strip()
                
                if target and len(target) > 2:
                    dependencies.append({
                        'target': target,
                        'type': dep_type,
                        'context': str(match),
                        'confidence': confidence
                    })
        
        return dependencies
    
    def _calculate_complexity(self, content: str) -> float:
        """–†–∞—Å—á–µ—Ç —Å–ª–æ–∂–Ω–æ—Å—Ç–∏ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–∞"""
        score = 0
        score += len(re.findall(r'^##', content, re.MULTILINE)) * 0.5  # –°–µ–∫—Ü–∏–∏
        score += content.count('```') * 0.3  # –ö–æ–¥ –±–ª–æ–∫–∏
        score += len(re.findall(r'\[.*?\]\(.*?\)', content)) * 0.2  # –°—Å—ã–ª–∫–∏
        return min(score, 10.0)
    
    def get_all_documents(self) -> List[Dict[str, Any]]:
        """–ü–æ–ª—É—á–∏—Ç—å –≤—Å–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã –∏–∑ —Å–∏—Å—Ç–µ–º—ã - –¥–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏ —Å MCP"""
        try:
            result = self._execute_safe("SELECT * FROM standards")
            if result:
                rows = result.fetchall()
                columns = [desc[0] for desc in result.description] if result.description else []
                return [dict(zip(columns, row)) for row in rows]
            return []
        except Exception as e:
            logger.error(f"Failed to get all documents: {e}")
            return []
    
    # === MCP –ö–û–ú–ê–ù–î–´ ===
    
    def get_standard_comprehensive(self, identifier: str) -> Dict[str, Any]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–∞ —Å –ø–æ–ª–Ω–æ–π –∞–Ω–∞–ª–∏—Ç–∏–∫–æ–π"""
        start_time = time.time()
        
        result = {
            "identifier": identifier,
            "success": False,
            "standard": None,
            "analytics": {},
            "relationships": {},
            "recommendations": []
        }
        
        try:
            # –ü–æ–∏—Å–∫ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–∞
            standard_result = self._execute_safe("""
                SELECT * FROM standards WHERE id = ? OR name LIKE ?
            """, [identifier, f"%{identifier}%"])
            
            if not standard_result:
                result["error"] = "Standard not found"
                return result
            
            standard_data = dict(zip([desc[0] for desc in standard_result.description], standard_result.fetchone()))
            
            # –ó–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ (–∏—Å—Ö–æ–¥—è—â–∏–µ)
            deps_out = self._execute_safe("""
                SELECT target_name, dependency_type, confidence_score 
                FROM dependencies WHERE source_id = ?
            """, [standard_data['id']])
            
            # –ó–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ (–≤—Ö–æ–¥—è—â–∏–µ)
            deps_in = self._execute_safe("""
                SELECT d.source_id, s.name, d.dependency_type 
                FROM dependencies d
                JOIN standards s ON d.source_id = s.id
                WHERE d.target_name LIKE ?
            """, [f"%{standard_data['name']}%"])
            
            result.update({
                "success": True,
                "standard": standard_data,
                "relationships": {
                    "outgoing_dependencies": deps_out.fetchall() if deps_out else [],
                    "incoming_references": deps_in.fetchall() if deps_in else []
                },
                "analytics": {
                    "complexity_level": "high" if standard_data.get('complexity_score', 0) > 5 else "medium" if standard_data.get('complexity_score', 0) > 2 else "low",
                    "completeness_score": self._assess_completeness(standard_data),
                    "connectivity_score": len(deps_out.fetchall() if deps_out else []) + len(deps_in.fetchall() if deps_in else [])
                },
                "recommendations": self._generate_recommendations(standard_data)
            })
            
        except Exception as e:
            result["error"] = str(e)
        
        # –û–ø–µ—Ä–∞—Ü–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞
        
        return result
    
    def search_standards_semantic(self, query: str, filters: Optional[Dict] = None) -> Dict[str, Any]:
        """–°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π –ø–æ–∏—Å–∫ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–æ–≤"""
        start_time = time.time()
        
        result = {
            "query": query,
            "filters": filters or {},
            "results": [],
            "insights": {},
            "success": False
        }
        
        try:
            # –ë–∞–∑–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å
            base_query = """
                SELECT s.*, 
                       COUNT(d_out.target_name) as outgoing_deps,
                       COUNT(d_in.source_id) as incoming_refs
                FROM standards s
                LEFT JOIN dependencies d_out ON s.id = d_out.source_id
                LEFT JOIN dependencies d_in ON s.name = d_in.target_name
                WHERE (s.name LIKE ? OR s.content LIKE ?)
            """
            
            params = [f"%{query}%", f"%{query}%"]
            
            # –î–æ–±–∞–≤–ª—è–µ–º —Ñ–∏–ª—å—Ç—Ä—ã
            if filters:
                if filters.get('category'):
                    base_query += " AND s.category = ?"
                    params.append(filters['category'])
                if filters.get('has_jtbd'):
                    base_query += " AND s.has_jtbd = TRUE"
                if filters.get('min_complexity'):
                    base_query += " AND s.complexity_score >= ?"
                    params.append(filters['min_complexity'])
            
            base_query += """
                GROUP BY s.id, s.name, s.path, s.category, s.content, s.content_hash,
                         s.version, s.author, s.status, s.has_jtbd, s.has_ai_protocols,
                         s.has_tdd_patterns, s.word_count, s.complexity_score,
                         s.created_at, s.updated_at
                ORDER BY s.complexity_score DESC, outgoing_deps DESC
                LIMIT 10
            """
            
            search_result = self._execute_safe(base_query, params)
            
            if search_result:
                results = [dict(zip([desc[0] for desc in search_result.description], row)) for row in search_result.fetchall()]
                
                result.update({
                    "success": True,
                    "results": results,
                    "insights": {
                        "total_found": len(results),
                        "categories": list(set(r['category'] for r in results)),
                        "avg_complexity": sum(r['complexity_score'] for r in results) / len(results) if results else 0
                    }
                })
            
        except Exception as e:
            result["error"] = str(e)
        
        duration = (time.time() - start_time) * 1000
        self._log_operation("search_standards_semantic", {"query": query}, result, duration)
        
        return result
    
    def analyze_ecosystem(self) -> Dict[str, Any]:
        """–ê–Ω–∞–ª–∏–∑ —ç–∫–æ—Å–∏—Å—Ç–µ–º—ã —Å—Ç–∞–Ω–¥–∞—Ä—Ç–æ–≤"""
        start_time = time.time()
        
        result = {
            "success": False,
            "overview": {},
            "dependency_analysis": {},
            "quality_metrics": {},
            "recommendations": []
        }
        
        try:
            # –û–±—â–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
            overview_result = self._execute_safe("""
                SELECT 
                    COUNT(*) as total_standards,
                    COUNT(DISTINCT category) as categories,
                    AVG(complexity_score) as avg_complexity,
                    SUM(CASE WHEN has_jtbd THEN 1 ELSE 0 END) as jtbd_count,
                    SUM(CASE WHEN has_ai_protocols THEN 1 ELSE 0 END) as ai_count
                FROM standards
            """)
            
            if overview_result:
                row = overview_result.fetchone()
                result["overview"] = {
                    "total_standards": row[0],
                    "categories": row[1],
                    "avg_complexity": round(row[2], 2) if row[2] else 0,
                    "jtbd_coverage": f"{(row[3]/row[0]*100):.1f}%" if row[0] > 0 else "0%",
                    "ai_protocol_coverage": f"{(row[4]/row[0]*100):.1f}%" if row[0] > 0 else "0%"
                }
            
            # –ê–Ω–∞–ª–∏–∑ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π
            deps_result = self._execute_safe("""
                SELECT COUNT(*) as total_deps,
                       COUNT(DISTINCT source_id) as connected_standards
                FROM dependencies
            """)
            
            if deps_result:
                row = deps_result.fetchone()
                result["dependency_analysis"] = {
                    "total_dependencies": row[0],
                    "connected_standards": row[1],
                    "connectivity_ratio": f"{(row[1]/result['overview']['total_standards']*100):.1f}%" if result["overview"]["total_standards"] > 0 else "0%"
                }
            
            # –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏
            result["recommendations"] = self._generate_ecosystem_recommendations(result)
            result["success"] = True
            
        except Exception as e:
            result["error"] = str(e)
        
        duration = (time.time() - start_time) * 1000
        self._log_operation("analyze_ecosystem", {}, result, duration)
        
        return result
    
    def validate_compliance(self) -> Dict[str, Any]:
        """–í–∞–ª–∏–¥–∞—Ü–∏—è —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏—è —Å—Ç–∞–Ω–¥–∞—Ä—Ç–æ–≤"""
        start_time = time.time()
        
        result = {
            "success": False,
            "compliance_score": 0,
            "issues": [],
            "recommendations": []
        }
        
        try:
            # –ü—Ä–æ–≤–µ—Ä–∫–∞ –æ—Å–Ω–æ–≤–Ω—ã—Ö —Ç—Ä–µ–±–æ–≤–∞–Ω–∏–π
            standards_result = self._execute_safe("""
                SELECT id, name, version, author, has_jtbd, complexity_score
                FROM standards
            """)
            
            if standards_result:
                standards = standards_result.fetchall()
                issues = []
                
                for standard in standards:
                    std_id, name, version, author, has_jtbd, complexity = standard
                    
                    if version == 'unknown':
                        issues.append({
                            "standard": name,
                            "type": "missing_version",
                            "severity": "medium"
                        })
                    
                    if author == 'unknown':
                        issues.append({
                            "standard": name,
                            "type": "missing_author",
                            "severity": "low"
                        })
                    
                    if not has_jtbd:
                        issues.append({
                            "standard": name,
                            "type": "missing_jtbd",
                            "severity": "high"
                        })
                
                # –†–∞—Å—á–µ—Ç —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏—è
                total_checks = len(standards) * 3  # 3 –ø—Ä–æ–≤–µ—Ä–∫–∏ –Ω–∞ —Å—Ç–∞–Ω–¥–∞—Ä—Ç
                compliance_score = max(0, (total_checks - len(issues)) / total_checks * 100) if total_checks > 0 else 100
                
                result.update({
                    "success": True,
                    "compliance_score": round(compliance_score, 1),
                    "issues": issues,
                    "recommendations": self._generate_compliance_recommendations(issues)
                })
            
        except Exception as e:
            result["error"] = str(e)
        
        # –£–±–∏—Ä–∞–µ–º –ø—Ä–æ–±–ª–µ–º–Ω–æ–µ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –ø–æ–∫–∞ –Ω–µ –∏—Å–ø—Ä–∞–≤–∏–º —Å—Ö–µ–º—É
        
        return result
    
    # === –í–°–ü–û–ú–û–ì–ê–¢–ï–õ–¨–ù–´–ï –ú–ï–¢–û–î–´ ===
    
    def _assess_completeness(self, standard: Dict) -> float:
        """–û—Ü–µ–Ω–∫–∞ –ø–æ–ª–Ω–æ—Ç—ã —Å—Ç–∞–Ω–¥–∞—Ä—Ç–∞"""
        score = 0
        if standard.get('name'): score += 0.2
        if standard.get('content') and len(standard['content']) > 500: score += 0.3
        if standard.get('version') != 'unknown': score += 0.2
        if standard.get('has_jtbd'): score += 0.3
        return score
    
    def _generate_recommendations(self, standard: Dict) -> List[str]:
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π –¥–ª—è —Å—Ç–∞–Ω–¥–∞—Ä—Ç–∞"""
        recommendations = []
        
        if not standard.get('has_jtbd'):
            recommendations.append("–î–æ–±–∞–≤—å—Ç–µ JTBD —Å–µ–∫—Ü–∏—é –¥–ª—è –ª—É—á—à–µ–≥–æ –ø–æ–Ω–∏–º–∞–Ω–∏—è —Ü–µ–ª–µ–π")
        
        if standard.get('complexity_score', 0) < 2:
            recommendations.append("–£–≤–µ–ª–∏—á—å—Ç–µ –¥–µ—Ç–∞–ª–∏–∑–∞—Ü–∏—é —Å—Ç–∞–Ω–¥–∞—Ä—Ç–∞")
        
        if standard.get('version') == 'unknown':
            recommendations.append("–î–æ–±–∞–≤—å—Ç–µ –≤–µ—Ä—Å–∏–æ–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ")
        
        return recommendations
    
    def _generate_ecosystem_recommendations(self, analysis: Dict) -> List[str]:
        """–†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –¥–ª—è —ç–∫–æ—Å–∏—Å—Ç–µ–º—ã"""
        recommendations = []
        
        overview = analysis.get('overview', {})
        
        if overview.get('total_standards', 0) < 50:
            recommendations.append("–†–∞—Å—à–∏—Ä—å—Ç–µ –∫–æ–ª–ª–µ–∫—Ü–∏—é —Å—Ç–∞–Ω–¥–∞—Ä—Ç–æ–≤")
        
        jtbd_coverage = float(overview.get('jtbd_coverage', '0%').replace('%', ''))
        if jtbd_coverage < 70:
            recommendations.append("–£–ª—É—á—à–∏—Ç–µ JTBD –ø–æ–∫—Ä—ã—Ç–∏–µ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–æ–≤")
        
        return recommendations
    
    def _generate_compliance_recommendations(self, issues: List[Dict]) -> List[str]:
        """–†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –ø–æ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏—é"""
        recommendations = []
        
        issue_types = [issue['type'] for issue in issues]
        
        if 'missing_jtbd' in issue_types:
            recommendations.append("–í–Ω–µ–¥—Ä–∏—Ç–µ JTBD –º–µ—Ç–æ–¥–æ–ª–æ–≥–∏—é –≤–æ –≤—Å–µ —Å—Ç–∞–Ω–¥–∞—Ä—Ç—ã")
        
        if 'missing_version' in issue_types:
            recommendations.append("–î–æ–±–∞–≤—å—Ç–µ –≤–µ—Ä—Å–∏–æ–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ –∫–æ –≤—Å–µ–º —Å—Ç–∞–Ω–¥–∞—Ä—Ç–∞–º")
        
        return recommendations
    
    def _log_operation(self, operation: str, params: Dict, result: Dict, duration_ms: float):
        """–õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –æ–ø–µ—Ä–∞—Ü–∏–π"""
        log_entry = {
            "timestamp": time.time(),
            "operation": operation,
            "params": params,
            "success": result.get("success", False),
            "duration_ms": duration_ms
        }
        
        self.operation_log.append(log_entry)
        
        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤ –ë–î
        try:
            self._execute_safe("""
                INSERT INTO operations_log (operation_type, parameters, success, duration_ms)
                VALUES (?, ?, ?, ?)
            """, [operation, json.dumps(params), result.get("success", False), duration_ms])
        except:
            pass
        
        # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º —Ä–∞–∑–º–µ—Ä –ª–æ–≥–∞ –≤ –ø–∞–º—è—Ç–∏
        if len(self.operation_log) > 100:
            self.operation_log = self.operation_log[-50:]
    
    def get_performance_report(self) -> Dict[str, Any]:
        """–û—Ç—á–µ—Ç –æ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏"""
        if not self.operation_log:
            return {"error": "No operations logged"}
        
        operations_by_type = {}
        for op in self.operation_log:
            op_type = op["operation"]
            if op_type not in operations_by_type:
                operations_by_type[op_type] = []
            operations_by_type[op_type].append(op["duration_ms"])
        
        report = {
            "total_operations": len(self.operation_log),
            "success_rate": sum(1 for op in self.operation_log if op["success"]) / len(self.operation_log) * 100,
            "performance_by_operation": {}
        }
        
        for op_type, durations in operations_by_type.items():
            report["performance_by_operation"][op_type] = {
                "count": len(durations),
                "avg_duration_ms": sum(durations) / len(durations),
                "total_time_ms": sum(durations)
            }
        
        return report
    
    def close(self):
        """–ó–∞–∫—Ä—ã—Ç–∏–µ —Å–∏—Å—Ç–µ–º—ã"""
        try:
            if self.conn:
                self.conn.close()
                self.conn = None
                logger.info("Standards system closed")
        except Exception as e:
            logger.warning(f"Error closing system: {e}")

def test_unified_system():
    """–¢–µ—Å—Ç –µ–¥–∏–Ω–æ–π —Å–∏—Å—Ç–µ–º—ã —Å—Ç–∞–Ω–¥–∞—Ä—Ç–æ–≤"""
    print("üß™ –¢–µ—Å—Ç Unified Standards System")
    
    system = UnifiedStandardsSystem()
    
    # –¢–µ—Å—Ç –ø–æ–∏—Å–∫–∞
    print("\nüîç –¢–µ—Å—Ç –ø–æ–∏—Å–∫–∞...")
    search_result = system.search_standards_semantic("JTBD")
    if search_result["success"]:
        print(f"   ‚úÖ –ù–∞–π–¥–µ–Ω–æ: {len(search_result['results'])} —Å—Ç–∞–Ω–¥–∞—Ä—Ç–æ–≤")
        print(f"   üìä –ö–∞—Ç–µ–≥–æ—Ä–∏–∏: {search_result['insights']['categories']}")
    
    # –¢–µ—Å—Ç –∞–Ω–∞–ª–∏–∑–∞ —ç–∫–æ—Å–∏—Å—Ç–µ–º—ã
    print("\nüåç –ê–Ω–∞–ª–∏–∑ —ç–∫–æ—Å–∏—Å—Ç–µ–º—ã...")
    ecosystem = system.analyze_ecosystem()
    if ecosystem["success"]:
        print(f"   ‚úÖ –í—Å–µ–≥–æ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–æ–≤: {ecosystem['overview']['total_standards']}")
        print(f"   üéØ JTBD –ø–æ–∫—Ä—ã—Ç–∏–µ: {ecosystem['overview']['jtbd_coverage']}")
        print(f"   üîó –°–≤—è–∑–∞–Ω–Ω–æ—Å—Ç—å: {ecosystem['dependency_analysis']['connectivity_ratio']}")
    
    # –¢–µ—Å—Ç –≤–∞–ª–∏–¥–∞—Ü–∏–∏
    print("\nüîí –í–∞–ª–∏–¥–∞—Ü–∏—è —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏—è...")
    compliance = system.validate_compliance()
    if compliance["success"]:
        print(f"   ‚úÖ –û—Ü–µ–Ω–∫–∞ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏—è: {compliance['compliance_score']}%")
        print(f"   üîß –ü—Ä–æ–±–ª–µ–º –Ω–∞–π–¥–µ–Ω–æ: {len(compliance['issues'])}")
    
    # –û—Ç—á–µ—Ç –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
    print("\n‚ö° –ü—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å...")
    perf = system.get_performance_report()
    print(f"   üìä –û–ø–µ—Ä–∞—Ü–∏–π: {perf['total_operations']}")
    print(f"   ‚úÖ –£—Å–ø–µ—à–Ω–æ—Å—Ç—å: {perf['success_rate']:.1f}%")
    
    system.close()
    print("\n‚úÖ Unified Standards System –≥–æ—Ç–æ–≤–∞ –∫ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—é!")

if __name__ == "__main__":
    test_unified_system()