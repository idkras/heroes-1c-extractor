#!/usr/bin/env python3
"""
–†–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–π —Ç—Ä–∏–≥–≥–µ—Ä —Å—Ç–∞–Ω–¥–∞—Ä—Ç–æ–≤ —Å –¥–µ—Ç–∞–ª—å–Ω–æ–π —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–æ–π –ø–æ —Ç–∏–ø–∞–º –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –≤ –∫–µ—à–µ.

–¶–µ–ª—å: –ü–æ–∫–∞–∑—ã–≤–∞—Ç—å –ø–æ–ª–Ω—É—é –∞–Ω–∞–ª–∏—Ç–∏–∫—É —Å–æ—Å—Ç–∞–≤–∞ –∫–µ—à–∞ –¥–ª—è –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞ –∏ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ —Å–∏—Å—Ç–µ–º—ã:
- –°—Ç–∞–Ω–¥–∞—Ä—Ç—ã –ø–æ –ø–∞–ø–∫–∞–º
- –ó–∞–¥–∞—á–∏ (–≤–∫–ª—é—á–∞—è –≥–∏–ø–æ—Ç–µ–∑—ã/–∑–∞–¥–∞—á–∏-–≥–∏–ø–æ—Ç–µ–∑—ã)
- –ò–Ω—Ü–∏–¥–µ–Ω—Ç—ã
- –î–µ—Ç–∞–ª—å–Ω–∞—è —Ä–∞–∑–±–∏–≤–∫–∞ –ø–æ –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º

–ê–≤—Ç–æ—Ä: AI Assistant  
–î–∞—Ç–∞: 22 May 2025
–ó–∞–¥–∞—á–∞: T012
"""

import os
import json
import time
from pathlib import Path
from typing import Dict, Any, List, Optional
from datetime import datetime
import logging

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logger = logging.getLogger(__name__)

class DocumentTypeAnalyzer:
    """–ê–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä —Ç–∏–ø–æ–≤ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –¥–ª—è –¥–µ—Ç–∞–ª—å–Ω–æ–π —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –∫–µ—à–∞."""
    
    def __init__(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä–∞."""
        self.base_path = Path(".")
        self.cache_managers = []
        self._initialize_cache_managers()
    
    def _initialize_cache_managers(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ—Ç –¥–æ—Å—Ç—É–ø–Ω—ã–µ –º–µ–Ω–µ–¥–∂–µ—Ä—ã –∫–µ—à–∞."""
        try:
            # –ü—ã—Ç–∞–µ–º—Å—è –ø–æ–¥–∫–ª—é—á–∏—Ç—å —Ä–∞–∑–ª–∏—á–Ω—ã–µ –∫–µ—à-–º–æ–¥—É–ª–∏ —Å –ø—Ä–∞–≤–∏–ª—å–Ω—ã–º–∏ –ø—É—Ç—è–º–∏
            import sys
            sys.path.append('.')
            
            from src.cache.real_inmemory_cache import get_cache
            
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º –Ω–æ–≤—ã–π –µ–¥–∏–Ω—ã–π RealInMemoryCache
            self.cache = get_cache()
            self.cache_managers = [
                ('real_inmemory_cache', self.cache)
            ]
            logger.info(f"–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–æ {len(self.cache_managers)} –∫–µ—à-–º–µ–Ω–µ–¥–∂–µ—Ä–æ–≤")
            
        except ImportError as e:
            logger.warning(f"–ù–µ —É–¥–∞–ª–æ—Å—å –∏–º–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞—Ç—å –∫–µ—à-–º–æ–¥—É–ª–∏: {e}")
            # –°–æ–∑–¥–∞–µ–º –ø—É—Å—Ç–æ–π —Å–ø–∏—Å–æ–∫, –∞–Ω–∞–ª–∏–∑ —Ñ–∞–π–ª–æ–≤–æ–π —Å–∏—Å—Ç–µ–º—ã –≤—Å–µ —Ä–∞–≤–Ω–æ —Ä–∞–±–æ—Ç–∞–µ—Ç
            self.cache_managers = []
    
    def analyze_file_type(self, file_path: str) -> Dict[str, Any]:
        """
        –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç —Ç–∏–ø —Ñ–∞–π–ª–∞ –∏ –∏–∑–≤–ª–µ–∫–∞–µ—Ç –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ.
        
        Args:
            file_path: –ü—É—Ç—å –∫ —Ñ–∞–π–ª—É
            
        Returns:
            Dict —Å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–µ–π –æ —Ç–∏–ø–µ –∏ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö
        """
        path_lower = file_path.lower()
        
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ç–∏–ø –¥–æ–∫—É–º–µ–Ω—Ç–∞
        if '[standards .md]' in file_path:
            return self._analyze_standard(file_path)
        elif '[todo ¬∑ incidents]' in file_path and 'todo' in path_lower:
            return self._analyze_task_or_hypothesis(file_path)
        elif '[todo ¬∑ incidents]' in file_path and 'incident' in path_lower:
            return self._analyze_incident(file_path)
        elif 'projects' in path_lower:
            return self._analyze_project(file_path)
        else:
            return {
                'type': 'other',
                'category': 'other',
                'subcategory': 'unknown',
                'size': self._get_file_size(file_path)
            }
    
    def _analyze_standard(self, file_path: str) -> Dict[str, Any]:
        """–ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç —Ñ–∞–π–ª —Å—Ç–∞–Ω–¥–∞—Ä—Ç–∞ —Å —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–µ–π –∞—Ä—Ö–∏–≤–æ–≤ –∏ –±—ç–∫–∞–ø–æ–≤."""
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —è–≤–ª—è–µ—Ç—Å—è –ª–∏ —ç—Ç–æ –∞—Ä—Ö–∏–≤–æ–º –∏–ª–∏ –±—ç–∫–∞–ø–æ–º
        path_lower = file_path.lower()
        
        # –ò—Å–∫–ª—é—á–∞–µ–º –∞—Ä—Ö–∏–≤—ã –∏ –±—ç–∫–∞–ø—ã
        archive_keywords = [
            'archive', 'backup', 'consolidated', 'template', 
            '20250512', '20250513', '20250514', '20250510',
            'backups_', 'ik', 'ai never use', 'rename_backup'
        ]
        
        for keyword in archive_keywords:
            if keyword in path_lower:
                return {
                    'type': 'archive',
                    'category': 'archives',
                    'subcategory': 'archived_standard',
                    'folder': self._extract_folder_name(file_path),
                    'size': self._get_file_size(file_path)
                }
        
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –ø–æ–¥–∫–∞—Ç–µ–≥–æ—Ä–∏—é –∞–∫—Ç–∏–≤–Ω–æ–≥–æ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–∞ –ø–æ —Å—Ç—Ä—É–∫—Ç—É—Ä–µ –ø–∞–ø–æ–∫
        path_parts = file_path.split('/')
        subcategory = 'general'
        
        # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –∞–∫—Ç–∏–≤–Ω—ã–µ –ø–∞–ø–∫–∏ —Å–æ–≥–ª–∞—Å–Ω–æ —Å—Ç—Ä—É–∫—Ç—É—Ä–µ
        for part in path_parts:
            part_lower = part.lower()
            
            if '0. core' in part_lower:
                subcategory = 'core_standards'
            elif '1. process' in part_lower or 'goalmap' in part_lower:
                subcategory = 'process_standards'
            elif '2. projects' in part_lower or 'context' in part_lower:
                subcategory = 'project_standards'
            elif '3. scenario' in part_lower or 'jtbd' in part_lower:
                subcategory = 'scenario_standards'
            elif '4. dev' in part_lower or 'design' in part_lower:
                subcategory = 'dev_standards'
            elif '6. advising' in part_lower or 'review' in part_lower:
                subcategory = 'advising_standards'
            elif '8. auto' in part_lower or 'n8n' in part_lower:
                subcategory = 'automation_standards'
            elif 'registry' in part_lower:
                subcategory = 'registry_standards'
            elif 'task' in part_lower or 'master' in part_lower:
                subcategory = 'task_master_standards'
        
        return {
            'type': 'standard',
            'category': 'standards',
            'subcategory': subcategory,
            'folder': self._extract_folder_name(file_path),
            'size': self._get_file_size(file_path),
            'is_active': True
        }
    
    def _analyze_task_or_hypothesis(self, file_path: str) -> Dict[str, Any]:
        """–ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç —Ñ–∞–π–ª –∑–∞–¥–∞—á–∏ –∏–ª–∏ –≥–∏–ø–æ—Ç–µ–∑—ã."""
        content = self._read_file_content(file_path)
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —è–≤–ª—è–µ—Ç—Å—è –ª–∏ —ç—Ç–æ –≥–∏–ø–æ—Ç–µ–∑–æ–π
        is_hypothesis = False
        if content:
            content_lower = content.lower()
            if any(keyword in content_lower for keyword in [
                '–≥–∏–ø–æ—Ç–µ–∑', 'hypothesis', '–ø—Ä–µ–¥–ø–æ–ª–æ–∂', 'assume', '—ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç'
            ]):
                is_hypothesis = True
        
        # –ò–∑–≤–ª–µ–∫–∞–µ–º –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç –∑–∞–¥–∞—á–∏
        priority = self._extract_task_priority(content or '')
        
        return {
            'type': 'hypothesis' if is_hypothesis else 'task',
            'category': 'tasks',
            'subcategory': 'hypothesis' if is_hypothesis else 'regular_task',
            'priority': priority,
            'size': self._get_file_size(file_path)
        }
    
    def _analyze_incident(self, file_path: str) -> Dict[str, Any]:
        """–ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç —Ñ–∞–π–ª –∏–Ω—Ü–∏–¥–µ–Ω—Ç–∞."""
        content = self._read_file_content(file_path)
        
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Å–µ—Ä—å–µ–∑–Ω–æ—Å—Ç—å –∏–Ω—Ü–∏–¥–µ–Ω—Ç–∞
        severity = 'medium'
        if content:
            content_lower = content.lower()
            if any(keyword in content_lower for keyword in [
                '–∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏–π', 'critical', 'blocker', 'urgent'
            ]):
                severity = 'high'
            elif any(keyword in content_lower for keyword in [
                'minor', '–Ω–∏–∑–∫–∏–π', 'small'
            ]):
                severity = 'low'
        
        return {
            'type': 'incident',
            'category': 'incidents',
            'subcategory': f'{severity}_severity',
            'severity': severity,
            'size': self._get_file_size(file_path)
        }
    
    def _analyze_project(self, file_path: str) -> Dict[str, Any]:
        """–ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç —Ñ–∞–π–ª –ø—Ä–æ–µ–∫—Ç–∞."""
        return {
            'type': 'project',
            'category': 'projects',
            'subcategory': 'project_file',
            'size': self._get_file_size(file_path)
        }
    
    def _extract_task_priority(self, content: str) -> str:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç –∑–∞–¥–∞—á–∏ –∏–∑ —Å–æ–¥–µ—Ä–∂–∏–º–æ–≥–æ."""
        if not content:
            return 'unknown'
        
        content_lower = content.lower()
        
        if 'üö®' in content or 'alarm' in content_lower:
            return 'alarm'
        elif 'üî¥' in content or 'blocker' in content_lower:
            return 'blocker'
        elif 'üü†' in content or 'asap' in content_lower:
            return 'asap'
        elif 'üîç' in content or 'research' in content_lower:
            return 'research'
        elif '‚≠ê' in content or 'exciter' in content_lower:
            return 'exciter'
        elif 'üü¢' in content or 'small' in content_lower:
            return 'small'
        else:
            return 'normal'
    
    def _extract_folder_name(self, file_path: str) -> str:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –∏–º—è –ø–∞–ø–∫–∏ –∏–∑ –ø—É—Ç–∏."""
        path_parts = file_path.split('/')
        for part in path_parts:
            if '[' in part and ']' in part:
                return part
        return 'root'
    
    def _get_file_size(self, file_path: str) -> int:
        """–ü–æ–ª—É—á–∞–µ—Ç —Ä–∞–∑–º–µ—Ä —Ñ–∞–π–ª–∞."""
        try:
            return os.path.getsize(file_path)
        except (OSError, FileNotFoundError):
            return 0
    
    def _read_file_content(self, file_path: str) -> Optional[str]:
        """–ß–∏—Ç–∞–µ—Ç —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ —Ñ–∞–π–ª–∞."""
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                return f.read()
        except (OSError, UnicodeDecodeError):
            return None


class EnhancedStandardsTrigger:
    """–†–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–π —Ç—Ä–∏–≥–≥–µ—Ä —Å—Ç–∞–Ω–¥–∞—Ä—Ç–æ–≤ —Å –¥–µ—Ç–∞–ª—å–Ω–æ–π —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–æ–π."""
    
    def __init__(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —Ç—Ä–∏–≥–≥–µ—Ä–∞."""
        self.analyzer = DocumentTypeAnalyzer()
        self.statistics = {}
    
    def generate_detailed_cache_statistics(self) -> Dict[str, Any]:
        """
        –ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –¥–µ—Ç–∞–ª—å–Ω—É—é —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –ø–æ —Ç–∏–ø–∞–º –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –≤ –∫–µ—à–µ.
        
        Returns:
            –î–µ—Ç–∞–ª—å–Ω–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –∫–µ—à–∞
        """
        logger.info("üîç –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –¥–µ—Ç–∞–ª—å–Ω–æ–π —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –∫–µ—à–∞...")
        
        statistics = {
            'timestamp': datetime.now().isoformat(),
            'summary': {
                'total_documents': 0,
                'total_size_bytes': 0
            },
            'by_type': {
                'standards': {'count': 0, 'size': 0, 'subcategories': {}},
                'tasks': {'count': 0, 'size': 0, 'subcategories': {}},
                'hypotheses': {'count': 0, 'size': 0, 'subcategories': {}},
                'incidents': {'count': 0, 'size': 0, 'subcategories': {}},
                'projects': {'count': 0, 'size': 0, 'subcategories': {}},
                'others': {'count': 0, 'size': 0, 'subcategories': {}}
            },
            'by_folder': {},
            'by_priority': {},
            'by_severity': {},
            'cache_sources': []
        }
        
        # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º —Ñ–∞–π–ª—ã —á–µ—Ä–µ–∑ —Ä–∞–∑–ª–∏—á–Ω—ã–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∏
        self._analyze_filesystem_directly(statistics)
        self._analyze_cache_managers(statistics)
        
        # –í—ã—á–∏—Å–ª—è–µ–º –ø—Ä–æ—Ü–µ–Ω—Ç—ã
        self._calculate_percentages(statistics)
        
        return statistics
    
    def _analyze_filesystem_directly(self, statistics: Dict[str, Any]):
        """–ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç —Ñ–∞–π–ª–æ–≤—É—é —Å–∏—Å—Ç–µ–º—É –Ω–∞–ø—Ä—è–º—É—é."""
        logger.debug("üìÅ –ê–Ω–∞–ª–∏–∑ —Ñ–∞–π–ª–æ–≤–æ–π —Å–∏—Å—Ç–µ–º—ã...")
        
        key_directories = [
            '[standards .md]',
            '[todo ¬∑ incidents]',
            'projects',
            'cache'
        ]
        
        for directory in key_directories:
            if os.path.exists(directory):
                self._scan_directory(directory, statistics)
    
    def _scan_directory(self, directory: str, statistics: Dict[str, Any]):
        """–°–∫–∞–Ω–∏—Ä—É–µ—Ç –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é –∏ –æ–±–Ω–æ–≤–ª—è–µ—Ç —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É."""
        try:
            for root, dirs, files in os.walk(directory):
                for file in files:
                    if file.endswith(('.md', '.txt', '.json')):
                        file_path = os.path.join(root, file)
                        self._process_file(file_path, statistics)
        except Exception as e:
            logger.warning(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–∫–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏–∏ {directory}: {e}")
    
    def _process_file(self, file_path: str, statistics: Dict[str, Any]):
        """–û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –æ–¥–∏–Ω —Ñ–∞–π–ª –∏ –æ–±–Ω–æ–≤–ª—è–µ—Ç —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É."""
        analysis = self.analyzer.analyze_file_type(file_path)
        
        # –û–±–Ω–æ–≤–ª—è–µ–º –æ–±—â—É—é —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
        statistics['summary']['total_documents'] += 1
        statistics['summary']['total_size_bytes'] += analysis.get('size', 0)
        
        # –û–±–Ω–æ–≤–ª—è–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –ø–æ —Ç–∏–ø–∞–º
        doc_type = analysis.get('type', 'other')
        category_key = doc_type + 's' if doc_type != 'other' else 'others'
        
        if category_key in statistics['by_type']:
            statistics['by_type'][category_key]['count'] += 1
            statistics['by_type'][category_key]['size'] += analysis.get('size', 0)
            
            # –ü–æ–¥–∫–∞—Ç–µ–≥–æ—Ä–∏–∏
            subcategory = analysis.get('subcategory', 'unknown')
            subcats = statistics['by_type'][category_key]['subcategories']
            if subcategory not in subcats:
                subcats[subcategory] = {'count': 0, 'size': 0}
            subcats[subcategory]['count'] += 1
            subcats[subcategory]['size'] += analysis.get('size', 0)
        
        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ –ø–∞–ø–∫–∞–º
        folder = analysis.get('folder', 'unknown')
        if folder not in statistics['by_folder']:
            statistics['by_folder'][folder] = {'count': 0, 'size': 0, 'types': {}}
        statistics['by_folder'][folder]['count'] += 1
        statistics['by_folder'][folder]['size'] += analysis.get('size', 0)
        
        if doc_type not in statistics['by_folder'][folder]['types']:
            statistics['by_folder'][folder]['types'][doc_type] = 0
        statistics['by_folder'][folder]['types'][doc_type] += 1
        
        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–∞–º (–¥–ª—è –∑–∞–¥–∞—á)
        if 'priority' in analysis:
            priority = analysis['priority']
            if priority not in statistics['by_priority']:
                statistics['by_priority'][priority] = {'count': 0, 'size': 0}
            statistics['by_priority'][priority]['count'] += 1
            statistics['by_priority'][priority]['size'] += analysis.get('size', 0)
        
        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ —Å–µ—Ä—å–µ–∑–Ω–æ—Å—Ç–∏ (–¥–ª—è –∏–Ω—Ü–∏–¥–µ–Ω—Ç–æ–≤)
        if 'severity' in analysis:
            severity = analysis['severity']
            if severity not in statistics['by_severity']:
                statistics['by_severity'][severity] = {'count': 0, 'size': 0}
            statistics['by_severity'][severity]['count'] += 1
            statistics['by_severity'][severity]['size'] += analysis.get('size', 0)
    
    def _analyze_cache_managers(self, statistics: Dict[str, Any]):
        """–ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç –¥–∞–Ω–Ω—ã–µ –∏–∑ –Ω–æ–≤–æ–≥–æ RealInMemoryCache."""
        logger.debug("üíæ –ê–Ω–∞–ª–∏–∑ RealInMemoryCache...")
        
        try:
            # –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–æ–∫—É–º–µ–Ω—Ç—ã –≤ –Ω–æ–≤—ã–π –∫–µ—à –µ—Å–ª–∏ –µ—â–µ –Ω–µ –∑–∞–≥—Ä—É–∂–µ–Ω—ã
            cache_stats = self.analyzer.cache.get_statistics()
            if cache_stats['total_documents'] == 0:
                logger.info("üì• –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–æ–∫—É–º–µ–Ω—Ç—ã –≤ RealInMemoryCache...")
                loaded = self.analyzer.cache.load_documents(['../[standards .md]', '../[todo ¬∑ incidents]'])
                logger.info(f"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–æ {loaded} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –≤ –∫–µ—à")
                cache_stats = self.analyzer.cache.get_statistics()
            
            # –û–±–Ω–æ–≤–ª—è–µ–º –æ–±—â—É—é —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
            statistics['summary']['total_documents'] += cache_stats['total_documents']
            statistics['summary']['total_size_bytes'] += int(cache_stats['memory_usage_mb'] * 1024 * 1024)
            
            # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –¥–æ–∫—É–º–µ–Ω—Ç—ã –ø–æ —Ç–∏–ø–∞–º –∏–∑ –∫–µ—à–∞
            for doc_type, count in cache_stats.get('document_types', {}).items():
                if doc_type == 'standard':
                    statistics['by_type']['standards']['count'] += count
                elif doc_type == 'task':
                    statistics['by_type']['tasks']['count'] += count
                elif doc_type == 'incident':
                    statistics['by_type']['incidents']['count'] += count
                else:
                    statistics['by_type']['others']['count'] += count
            
            # –î–æ–±–∞–≤–ª—è–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ–± –∏—Å—Ç–æ—á–Ω–∏–∫–µ –∫–µ—à–∞
            statistics['cache_sources'].append({
                'name': 'RealInMemoryCache',
                'total_files': cache_stats['total_documents'],
                'memory_usage_mb': cache_stats['memory_usage_mb'],
                'hit_rate_percent': cache_stats['hit_rate_percent'],
                'document_types': cache_stats.get('document_types', {}),
                'available': True,
                'status': 'active'
            })
            
            logger.info(f"‚úÖ RealInMemoryCache: {cache_stats['total_documents']} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤, {cache_stats['memory_usage_mb']:.2f}MB")
            
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –∞–Ω–∞–ª–∏–∑–∞ RealInMemoryCache: {e}")
            statistics['cache_sources'].append({
                'name': 'RealInMemoryCache',
                'available': False,
                'error': str(e)
            })
    
    def _get_cache_stats(self, cache_class, cache_name: str) -> Optional[Dict]:
        """–ü–æ–ª—É—á–∞–µ—Ç —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –∏–∑ –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–≥–æ –∫–µ—à–∞."""
        try:
            if cache_name == 'cache_storage':
                # CacheStorage —Ç—Ä–µ–±—É–µ—Ç –ø—É—Ç—å –∫ —Ñ–∞–π–ª—É
                cache_path = "cache/cache_entries.json"
                if os.path.exists(cache_path):
                    cache_instance = cache_class(cache_path)
                    return cache_instance.get_statistics()
            else:
                # –î—Ä—É–≥–∏–µ –∫–µ—à–∏ –º–æ–≥—É—Ç –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å—Å—è –±–µ–∑ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤
                cache_instance = cache_class()
                if hasattr(cache_instance, 'get_statistics'):
                    return cache_instance.get_statistics()
        except Exception as e:
            logger.debug(f"–ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –∏–∑ {cache_name}: {e}")
        return None
    
    def _calculate_percentages(self, statistics: Dict[str, Any]):
        """–í—ã—á–∏—Å–ª—è–µ—Ç –ø—Ä–æ—Ü–µ–Ω—Ç–Ω—ã–µ —Å–æ–æ—Ç–Ω–æ—à–µ–Ω–∏—è."""
        total = statistics['summary']['total_documents']
        if total == 0:
            return
        
        # –ü—Ä–æ—Ü–µ–Ω—Ç—ã –ø–æ —Ç–∏–ø–∞–º
        for type_name, type_data in statistics['by_type'].items():
            type_data['percentage'] = round((type_data['count'] / total) * 100, 1)
        
        # –ü—Ä–æ—Ü–µ–Ω—Ç—ã –ø–æ –ø–∞–ø–∫–∞–º
        for folder_name, folder_data in statistics['by_folder'].items():
            folder_data['percentage'] = round((folder_data['count'] / total) * 100, 1)
    
    def format_statistics_report(self, statistics: Dict[str, Any]) -> str:
        """
        –§–æ—Ä–º–∞—Ç–∏—Ä—É–µ—Ç —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –≤ —á–∏—Ç–∞–µ–º—ã–π –æ—Ç—á–µ—Ç.
        
        Args:
            statistics: –î–∞–Ω–Ω—ã–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏
            
        Returns:
            –§–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –æ—Ç—á–µ—Ç
        """
        report = []
        report.append("üìä === –î–ï–¢–ê–õ–¨–ù–ê–Ø –°–¢–ê–¢–ò–°–¢–ò–ö–ê –ö–ï–®–ê ===")
        report.append(f"üïí –í—Ä–µ–º—è: {statistics['timestamp']}")
        report.append("")
        
        # –û–±—â–∞—è —Å–≤–æ–¥–∫–∞
        summary = statistics['summary']
        total_size_mb = summary['total_size_bytes'] / (1024 * 1024)
        report.append(f"üìà –û–ë–©–ê–Ø –°–í–û–î–ö–ê:")
        report.append(f"   üìÑ –í—Å–µ–≥–æ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤: {summary['total_documents']}")
        report.append(f"   üíæ –û–±—â–∏–π —Ä–∞–∑–º–µ—Ä: {total_size_mb:.2f} MB")
        report.append("")
        
        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ —Ç–∏–ø–∞–º
        report.append("üìã –°–¢–ê–¢–ò–°–¢–ò–ö–ê –ü–û –¢–ò–ü–ê–ú –î–û–ö–£–ú–ï–ù–¢–û–í:")
        for type_name, type_data in statistics['by_type'].items():
            if type_data['count'] > 0:
                size_mb = type_data['size'] / (1024 * 1024)
                report.append(f"   {self._get_type_emoji(type_name)} {type_name.upper()}: {type_data['count']} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ ({type_data.get('percentage', 0)}%) - {size_mb:.2f} MB")
                
                # –ü–æ–¥–∫–∞—Ç–µ–≥–æ—Ä–∏–∏
                for subcat, subcat_data in type_data['subcategories'].items():
                    subcat_size_mb = subcat_data['size'] / (1024 * 1024)
                    report.append(f"      ‚îî‚îÄ {subcat}: {subcat_data['count']} ({subcat_size_mb:.2f} MB)")
        report.append("")
        
        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ –ø–∞–ø–∫–∞–º
        report.append("üìÅ –°–¢–ê–¢–ò–°–¢–ò–ö–ê –ü–û –ü–ê–ü–ö–ê–ú:")
        for folder_name, folder_data in statistics['by_folder'].items():
            if folder_data['count'] > 0:
                size_mb = folder_data['size'] / (1024 * 1024)
                report.append(f"   üìÇ {folder_name}: {folder_data['count']} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ ({folder_data.get('percentage', 0)}%) - {size_mb:.2f} MB")
                
                # –¢–∏–ø—ã –≤ –ø–∞–ø–∫–µ
                for doc_type, count in folder_data['types'].items():
                    report.append(f"      ‚îî‚îÄ {doc_type}: {count}")
        report.append("")
        
        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–∞–º
        if statistics['by_priority']:
            report.append("üéØ –°–¢–ê–¢–ò–°–¢–ò–ö–ê –ü–û –ü–†–ò–û–†–ò–¢–ï–¢–ê–ú –ó–ê–î–ê–ß:")
            for priority, priority_data in statistics['by_priority'].items():
                emoji = self._get_priority_emoji(priority)
                size_mb = priority_data['size'] / (1024 * 1024)
                report.append(f"   {emoji} {priority}: {priority_data['count']} –∑–∞–¥–∞—á - {size_mb:.2f} MB")
            report.append("")
        
        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ —Å–µ—Ä—å–µ–∑–Ω–æ—Å—Ç–∏ –∏–Ω—Ü–∏–¥–µ–Ω—Ç–æ–≤
        if statistics['by_severity']:
            report.append("‚ö†Ô∏è –°–¢–ê–¢–ò–°–¢–ò–ö–ê –ü–û –°–ï–†–¨–ï–ó–ù–û–°–¢–ò –ò–ù–¶–ò–î–ï–ù–¢–û–í:")
            for severity, severity_data in statistics['by_severity'].items():
                emoji = self._get_severity_emoji(severity)
                size_mb = severity_data['size'] / (1024 * 1024)
                report.append(f"   {emoji} {severity}: {severity_data['count']} –∏–Ω—Ü–∏–¥–µ–Ω—Ç–æ–≤ - {size_mb:.2f} MB")
            report.append("")
        
        # –ò—Å—Ç–æ—á–Ω–∏–∫–∏ –∫–µ—à–∞
        if statistics['cache_sources']:
            report.append("üíæ –ò–°–¢–û–ß–ù–ò–ö–ò –ö–ï–®–ê:")
            for source in statistics['cache_sources']:
                status = "‚úÖ" if source['available'] else "‚ùå"
                report.append(f"   {status} {source['name']}")
                if source['available'] and 'stats' in source:
                    stats = source['stats']
                    if isinstance(stats, dict):
                        for key, value in stats.items():
                            if isinstance(value, (int, float, str)):
                                report.append(f"      ‚îî‚îÄ {key}: {value}")
        
        return "\n".join(report)
    
    def _get_type_emoji(self, type_name: str) -> str:
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —ç–º–æ–¥–∑–∏ –¥–ª—è —Ç–∏–ø–∞ –¥–æ–∫—É–º–µ–Ω—Ç–∞."""
        emoji_map = {
            'standards': 'üìñ',
            'tasks': 'üìù',
            'hypotheses': 'üî¨',
            'incidents': 'üö®',
            'projects': 'üóÇÔ∏è',
            'others': 'üìÑ'
        }
        return emoji_map.get(type_name, 'üìÑ')
    
    def _get_priority_emoji(self, priority: str) -> str:
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —ç–º–æ–¥–∑–∏ –¥–ª—è –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–∞."""
        emoji_map = {
            'alarm': 'üö®',
            'blocker': 'üî¥',
            'asap': 'üü†',
            'research': 'üîç',
            'exciter': '‚≠ê',
            'small': 'üü¢',
            'normal': '‚ö™',
            'unknown': '‚ùì'
        }
        return emoji_map.get(priority, '‚ö™')
    
    def _get_severity_emoji(self, severity: str) -> str:
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —ç–º–æ–¥–∑–∏ –¥–ª—è —Å–µ—Ä—å–µ–∑–Ω–æ—Å—Ç–∏."""
        emoji_map = {
            'high': 'üî¥',
            'medium': 'üü°',
            'low': 'üü¢'
        }
        return emoji_map.get(severity, 'üü°')
    
    def run_statistics_trigger(self) -> str:
        """
        –ó–∞–ø—É—Å–∫–∞–µ—Ç —Ç—Ä–∏–≥–≥–µ—Ä –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏.
        
        Returns:
            –§–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –æ—Ç—á–µ—Ç
        """
        logger.info("üöÄ –ó–∞–ø—É—Å–∫ —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω–æ–≥–æ —Ç—Ä–∏–≥–≥–µ—Ä–∞ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–æ–≤...")
        
        try:
            # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
            statistics = self.generate_detailed_cache_statistics()
            
            # –§–æ—Ä–º–∞—Ç–∏—Ä—É–µ–º –æ—Ç—á–µ—Ç
            report = self.format_statistics_report(statistics)
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –≤ —Ñ–∞–π–ª
            self._save_statistics(statistics)
            
            logger.info("‚úÖ –¢—Ä–∏–≥–≥–µ—Ä —Å—Ç–∞–Ω–¥–∞—Ä—Ç–æ–≤ —É—Å–ø–µ—à–Ω–æ –≤—ã–ø–æ–ª–Ω–µ–Ω!")
            return report
            
        except Exception as e:
            error_msg = f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏–∏ —Ç—Ä–∏–≥–≥–µ—Ä–∞: {e}"
            logger.error(error_msg)
            return error_msg
    
    def _save_statistics(self, statistics: Dict[str, Any]):
        """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –≤ —Ñ–∞–π–ª."""
        try:
            cache_dir = Path("cache")
            cache_dir.mkdir(exist_ok=True)
            
            stats_file = cache_dir / "enhanced_cache_statistics.json"
            with open(stats_file, 'w', encoding='utf-8') as f:
                json.dump(statistics, f, indent=2, ensure_ascii=False)
            
            logger.info(f"üìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –≤ {stats_file}")
        except Exception as e:
            logger.warning(f"–ù–µ —É–¥–∞–ª–æ—Å—å —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É: {e}")


def main():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è –∑–∞–ø—É—Å–∫–∞ —Ç—Ä–∏–≥–≥–µ—Ä–∞."""
    trigger = EnhancedStandardsTrigger()
    report = trigger.run_statistics_trigger()
    print(report)
    return report


if __name__ == "__main__":
    main()