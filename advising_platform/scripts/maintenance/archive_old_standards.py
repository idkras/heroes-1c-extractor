#!/usr/bin/env python3
"""
–°–∫—Ä–∏–ø—Ç –¥–ª—è –∞—Ä—Ö–∏–≤–∞—Ü–∏–∏ —Å—Ç–∞—Ä—ã—Ö –∏–ª–∏ —É—Å—Ç–∞—Ä–µ–≤—à–∏—Ö —Å—Ç–∞–Ω–¥–∞—Ä—Ç–æ–≤.

–≠—Ç–æ—Ç —Å–∫—Ä–∏–ø—Ç –ø–æ–∑–≤–æ–ª—è–µ—Ç:
1. –ò–¥–µ–Ω—Ç–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞—Ç—å —Å—Ç–∞–Ω–¥–∞—Ä—Ç—ã-–∫–∞–Ω–¥–∏–¥–∞—Ç—ã –¥–ª—è –∞—Ä—Ö–∏–≤–∞—Ü–∏–∏
2. –û–ø—Ä–µ–¥–µ–ª–∏—Ç—å –¥—É–±–ª–∏—Ä—É—é—â–∏–µ —Å—Ç–∞–Ω–¥–∞—Ä—Ç—ã –Ω–∞ –æ—Å–Ω–æ–≤–µ –∏—Ö —Å–æ–¥–µ—Ä–∂–∏–º–æ–≥–æ
3. –ê—Ä—Ö–∏–≤–∏—Ä–æ–≤–∞—Ç—å –≤—ã–±—Ä–∞–Ω–Ω—ã–µ —Å—Ç–∞–Ω–¥–∞—Ä—Ç—ã
4. –û–±–Ω–æ–≤–∏—Ç—å —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –∞–∫—Ç–∏–≤–Ω—ã—Ö —Å—Ç–∞–Ω–¥–∞—Ä—Ç–æ–≤

–ê–≤—Ç–æ—Ä: AI Assistant
–î–∞—Ç–∞: 20 –º–∞—è 2025
"""

import os
import re
import sys
import json
import shutil
import logging
import datetime
import argparse
from pathlib import Path
from typing import List, Dict, Tuple, Set, Optional

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.StreamHandler(),
        logging.FileHandler('archive_standards.log')
    ]
)
logger = logging.getLogger("archive_standards")

# –ü—É—Ç–∏ –∫ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—è–º
STANDARDS_DIR = "[standards .md]"
ARCHIVE_DIR = f"{STANDARDS_DIR}/[archive]"
ARCHIVE_DATE_DIR = f"{ARCHIVE_DIR}/{datetime.datetime.now().strftime('%Y%m%d')}"

# –ì–ª–æ–±–∞–ª—å–Ω—ã–µ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ
TARGET_ACTIVE_STANDARDS = 40  # –ü—Ä–µ–¥–ø–æ—á—Ç–∏—Ç–µ–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∞–∫—Ç–∏–≤–Ω—ã—Ö —Å—Ç–∞–Ω–¥–∞—Ä—Ç–æ–≤

def ensure_directory_exists(directory: str) -> None:
    """
    –ü—Ä–æ–≤–µ—Ä—è–µ—Ç –Ω–∞–ª–∏—á–∏–µ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏ –∏ —Å–æ–∑–¥–∞–µ—Ç –µ–µ –ø—Ä–∏ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏.
    
    Args:
        directory: –ü—É—Ç—å –∫ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏
    """
    if not os.path.exists(directory):
        os.makedirs(directory, exist_ok=True)
        logger.info(f"–°–æ–∑–¥–∞–Ω–∞ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—è: {directory}")

def find_standards() -> List[Dict[str, str]]:
    """
    –ù–∞—Ö–æ–¥–∏—Ç –≤—Å–µ –∞–∫—Ç–∏–≤–Ω—ã–µ —Å—Ç–∞–Ω–¥–∞—Ä—Ç—ã.
    
    Returns:
        List[Dict[str, str]]: –°–ø–∏—Å–æ–∫ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–æ–≤ —Å –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–º–∏
    """
    standards = []
    
    if not os.path.exists(STANDARDS_DIR):
        logger.error(f"–î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è —Å—Ç–∞–Ω–¥–∞—Ä—Ç–æ–≤ –Ω–µ –Ω–∞–π–¥–µ–Ω–∞: {STANDARDS_DIR}")
        return standards
    
    for root, dirs, files in os.walk(STANDARDS_DIR):
        # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –∞—Ä—Ö–∏–≤–Ω—ã–µ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏
        if "[archive]" in root:
            continue
        
        for file in files:
            if not file.endswith(".md"):
                continue
            
            file_path = os.path.join(root, file)
            
            try:
                with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:
                    content = f.read()
                
                # –ò–∑–≤–ª–µ–∫–∞–µ–º –∑–∞–≥–æ–ª–æ–≤–æ–∫
                title_match = re.search(r'^# (.+)$', content, re.MULTILINE)
                title = title_match.group(1).strip() if title_match else file
                
                # –ò–∑–≤–ª–µ–∫–∞–µ–º –¥–∞—Ç—É
                date_match = re.search(r'date:.*?(\d{1,2}[- /.]\d{1,2}[- /.]\d{2,4}|\d{4}[- /.]\d{1,2}[- /.]\d{1,2})', content, re.IGNORECASE)
                date = date_match.group(1).strip() if date_match else None
                
                if not date:
                    # –ü—ã—Ç–∞–µ–º—Å—è –∏–∑–≤–ª–µ—á—å –¥–∞—Ç—É –∏–∑ –∏–º–µ–Ω–∏ —Ñ–∞–π–ª–∞
                    date_match = re.search(r'(\d{1,2}[- /.]\d{1,2}[- /.]\d{2,4}|\d{4}[- /.]\d{1,2}[- /.]\d{1,2})', file)
                    date = date_match.group(1).strip() if date_match else None
                
                # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –≤–µ—Ä—Å–∏—é
                version_match = re.search(r'version:.*?([0-9.]+)', content, re.IGNORECASE)
                if not version_match:
                    version_match = re.search(r'v([0-9.]+)', file, re.IGNORECASE)
                version = version_match.group(1) if version_match else "1.0"
                
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —è–≤–ª—è–µ—Ç—Å—è –ª–∏ —ç—Ç–æ –¥—É–±–ª–∏–∫–∞—Ç–æ–º –∏–ª–∏ —á–µ—Ä–Ω–æ–≤–∏–∫–æ–º
                is_draft = "draft" in file.lower() or "—á–µ—Ä–Ω–æ–≤–∏–∫" in file.lower() or "draft" in content.lower() or "—á–µ—Ä–Ω–æ–≤–∏–∫" in content.lower()
                
                # –†–∞–∑–º–µ—Ä —Ñ–∞–π–ª–∞
                file_size = os.path.getsize(file_path)
                
                # –î–∞—Ç–∞ –ø–æ—Å–ª–µ–¥–Ω–µ–≥–æ –∏–∑–º–µ–Ω–µ–Ω–∏—è
                mtime = os.path.getmtime(file_path)
                last_modified = datetime.datetime.fromtimestamp(mtime).strftime('%Y-%m-%d')
                
                standards.append({
                    "title": title,
                    "path": file_path,
                    "date": date,
                    "version": version,
                    "is_draft": is_draft,
                    "file_size": file_size,
                    "last_modified": last_modified,
                    "content": content
                })
                
                logger.debug(f"–ù–∞–π–¥–µ–Ω —Å—Ç–∞–Ω–¥–∞—Ä—Ç: {title} ({file_path})")
            except Exception as e:
                logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ —Ñ–∞–π–ª–∞ {file_path}: {e}")
    
    logger.info(f"–ù–∞–π–¥–µ–Ω–æ {len(standards)} –∞–∫—Ç–∏–≤–Ω—ã—Ö —Å—Ç–∞–Ω–¥–∞—Ä—Ç–æ–≤")
    return standards

def identify_duplicates(standards: List[Dict[str, str]]) -> List[Dict[str, List[Dict[str, str]]]]:
    """
    –ò–¥–µ–Ω—Ç–∏—Ñ–∏—Ü–∏—Ä—É–µ—Ç –¥—É–±–ª–∏—Ä—É—é—â–∏–µ —Å—Ç–∞–Ω–¥–∞—Ä—Ç—ã.
    
    Args:
        standards: –°–ø–∏—Å–æ–∫ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–æ–≤ —Å –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–º–∏
        
    Returns:
        List[Dict[str, List[Dict[str, str]]]]: –°–ø–∏—Å–æ–∫ –≥—Ä—É–ø–ø –¥—É–±–ª–∏–∫–∞—Ç–æ–≤
    """
    duplicates = []
    
    # –ì—Ä—É–ø–ø–∏—Ä—É–µ–º —Å—Ç–∞–Ω–¥–∞—Ä—Ç—ã –ø–æ –Ω–∞–∑–≤–∞–Ω–∏—é (–±–µ–∑ —É—á–µ—Ç–∞ —Ä–µ–≥–∏—Å—Ç—Ä–∞ –∏ –≤–µ—Ä—Å–∏–∏)
    title_groups = {}
    for standard in standards:
        # –û—á–∏—â–∞–µ–º –Ω–∞–∑–≤–∞–Ω–∏–µ –æ—Ç –≤–µ—Ä—Å–∏–∏ –∏ –¥–∞—Ç—ã
        clean_title = re.sub(r'v\d+(\.\d+)*', '', standard["title"])
        clean_title = re.sub(r'\d{1,2}[- /.]\d{1,2}[- /.]\d{2,4}|\d{4}[- /.]\d{1,2}[- /.]\d{1,2}', '', clean_title)
        clean_title = clean_title.lower().strip()
        
        if clean_title in title_groups:
            title_groups[clean_title].append(standard)
        else:
            title_groups[clean_title] = [standard]
    
    # –ù–∞—Ö–æ–¥–∏–º –≥—Ä—É–ø–ø—ã —Å –±–æ–ª–µ–µ —á–µ–º –æ–¥–Ω–∏–º —Å—Ç–∞–Ω–¥–∞—Ä—Ç–æ–º
    for title, group in title_groups.items():
        if len(group) > 1:
            duplicates.append({
                "title": title,
                "standards": group
            })
    
    # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ –ø—Ä–æ–≤–µ—Ä—è–µ–º —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ –¥–ª—è —Å—Ç–∞–Ω–¥–∞—Ä—Ç–æ–≤ —Å —Ä–∞–∑–Ω—ã–º–∏ –Ω–∞–∑–≤–∞–Ω–∏—è–º–∏
    content_similarity = []
    for i, standard1 in enumerate(standards):
        for j, standard2 in enumerate(standards[i+1:], i+1):
            # –°—Ä–∞–≤–Ω–∏–≤–∞–µ–º —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ (–±–µ–∑ —É—á–µ—Ç–∞ —Ä–µ–≥–∏—Å—Ç—Ä–∞ –∏ —Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏—è)
            content1 = re.sub(r'[^a-zA-Z–∞-—è–ê-–Ø0-9]', '', standard1["content"].lower())
            content2 = re.sub(r'[^a-zA-Z–∞-—è–ê-–Ø0-9]', '', standard2["content"].lower())
            
            # –ï—Å–ª–∏ —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ –æ—á–µ–Ω—å –ø–æ—Ö–æ–∂–µ, –Ω–æ –Ω–∞–∑–≤–∞–Ω–∏—è —Ä–∞–∑–Ω—ã–µ
            if content1 == content2 and standard1["title"].lower() != standard2["title"].lower():
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –Ω–µ –≤—Ö–æ–¥—è—Ç –ª–∏ —ç—Ç–∏ —Å—Ç–∞–Ω–¥–∞—Ä—Ç—ã —É–∂–µ –≤ –∫–∞–∫—É—é-—Ç–æ –≥—Ä—É–ø–ø—É –¥—É–±–ª–∏–∫–∞—Ç–æ–≤
                is_in_group = False
                for group in duplicates:
                    if standard1 in group["standards"] and standard2 in group["standards"]:
                        is_in_group = True
                        break
                
                if not is_in_group:
                    content_similarity.append({
                        "title": f"–ü–æ—Ö–æ–∂–∏–µ —Å—Ç–∞–Ω–¥–∞—Ä—Ç—ã: {standard1['title']} –∏ {standard2['title']}",
                        "standards": [standard1, standard2]
                    })
    
    # –î–æ–±–∞–≤–ª—è–µ–º –≥—Ä—É–ø–ø—ã —Å –ø–æ—Ö–æ–∂–∏–º —Å–æ–¥–µ—Ä–∂–∏–º—ã–º
    duplicates.extend(content_similarity)
    
    logger.info(f"–ù–∞–π–¥–µ–Ω–æ {len(duplicates)} –≥—Ä—É–ø–ø –¥—É–±–ª–∏—Ä—É—é—â–∏—Ö —Å—Ç–∞–Ω–¥–∞—Ä—Ç–æ–≤")
    for i, group in enumerate(duplicates, 1):
        logger.info(f"–ì—Ä—É–ø–ø–∞ {i}: {group['title']}")
        for standard in group["standards"]:
            logger.info(f"  - {standard['title']} ({standard['path']})")
    
    return duplicates

def identify_candidates_for_archiving(standards: List[Dict[str, str]], duplicates: List[Dict[str, List[Dict[str, str]]]]) -> List[Dict[str, str]]:
    """
    –û–ø—Ä–µ–¥–µ–ª—è–µ—Ç —Å—Ç–∞–Ω–¥–∞—Ä—Ç—ã-–∫–∞–Ω–¥–∏–¥–∞—Ç—ã –¥–ª—è –∞—Ä—Ö–∏–≤–∞—Ü–∏–∏.
    
    Args:
        standards: –°–ø–∏—Å–æ–∫ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–æ–≤ —Å –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–º–∏
        duplicates: –°–ø–∏—Å–æ–∫ –≥—Ä—É–ø–ø –¥—É–±–ª–∏–∫–∞—Ç–æ–≤
        
    Returns:
        List[Dict[str, str]]: –°–ø–∏—Å–æ–∫ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–æ–≤-–∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤ –¥–ª—è –∞—Ä—Ö–∏–≤–∞—Ü–∏–∏
    """
    candidates = []
    
    # 1. –ê—Ä—Ö–∏–≤–∏—Ä—É–µ–º —á–µ—Ä–Ω–æ–≤–∏–∫–∏
    for standard in standards:
        if standard["is_draft"]:
            candidates.append({
                "standard": standard,
                "reason": "–≠—Ç–æ —á–µ—Ä–Ω–æ–≤–∏–∫"
            })
    
    # 2. –ê—Ä—Ö–∏–≤–∏—Ä—É–µ–º –¥—É–±–ª–∏–∫–∞—Ç—ã (–æ—Å—Ç–∞–≤–ª—è–µ–º —Ç–æ–ª—å–∫–æ —Å–∞–º—É—é –ø–æ—Å–ª–µ–¥–Ω—é—é –≤–µ—Ä—Å–∏—é)
    for group in duplicates:
        # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ –≤–µ—Ä—Å–∏–∏ –∏ –¥–∞—Ç–µ –ø–æ—Å–ª–µ–¥–Ω–µ–≥–æ –∏–∑–º–µ–Ω–µ–Ω–∏—è
        sorted_standards = sorted(
            group["standards"],
            key=lambda s: (float(s["version"]), s["last_modified"]),
            reverse=True
        )
        
        # –û—Å—Ç–∞–≤–ª—è–µ–º —Ç–æ–ª—å–∫–æ —Å–∞–º—É—é –ø–æ—Å–ª–µ–¥–Ω—é—é –≤–µ—Ä—Å–∏—é
        for standard in sorted_standards[1:]:
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –Ω–µ –¥–æ–±–∞–≤–ª–µ–Ω –ª–∏ —Å—Ç–∞–Ω–¥–∞—Ä—Ç —É–∂–µ –≤ –∫–∞–Ω–¥–∏–¥–∞—Ç—ã
            if not any(c["standard"]["path"] == standard["path"] for c in candidates):
                candidates.append({
                    "standard": standard,
                    "reason": f"–î—É–±–ª–∏–∫–∞—Ç —Å—Ç–∞–Ω–¥–∞—Ä—Ç–∞ {sorted_standards[0]['title']}"
                })
    
    # 3. –ï—Å–ª–∏ —É –Ω–∞—Å –≤—Å—ë –µ—â—ë —Å–ª–∏—à–∫–æ–º –º–Ω–æ–≥–æ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–æ–≤, –∞—Ä—Ö–∏–≤–∏—Ä—É–µ–º —Å–∞–º—ã–µ —Å—Ç–∞—Ä—ã–µ
    if len(standards) - len(candidates) > TARGET_ACTIVE_STANDARDS:
        # –û—Ç–±–∏—Ä–∞–µ–º —Å—Ç–∞–Ω–¥–∞—Ä—Ç—ã, –∫–æ—Ç–æ—Ä—ã–µ –µ—â—ë –Ω–µ –≤ —Å–ø–∏—Å–∫–µ –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤
        remaining = [s for s in standards if not any(c["standard"]["path"] == s["path"] for c in candidates)]
        
        # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ –¥–∞—Ç–µ –ø–æ—Å–ª–µ–¥–Ω–µ–≥–æ –∏–∑–º–µ–Ω–µ–Ω–∏—è
        sorted_remaining = sorted(
            remaining,
            key=lambda s: s["last_modified"]
        )
        
        # –î–æ–±–∞–≤–ª—è–µ–º —Å—Ç–æ–ª—å–∫–æ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–æ–≤, —Å–∫–æ–ª—å–∫–æ –Ω—É–∂–Ω–æ –¥–ª—è –¥–æ—Å—Ç–∏–∂–µ–Ω–∏—è —Ü–µ–ª–µ–≤–æ–≥–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞
        num_to_archive = len(remaining) - TARGET_ACTIVE_STANDARDS
        for standard in sorted_remaining[:num_to_archive]:
            candidates.append({
                "standard": standard,
                "reason": "–£—Å—Ç–∞—Ä–µ–≤—à–∏–π —Å—Ç–∞–Ω–¥–∞—Ä—Ç (–Ω–∞ –æ—Å–Ω–æ–≤–µ –¥–∞—Ç—ã –ø–æ—Å–ª–µ–¥–Ω–µ–≥–æ –∏–∑–º–µ–Ω–µ–Ω–∏—è)"
            })
    
    logger.info(f"–ò–¥–µ–Ω—Ç–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–æ {len(candidates)} —Å—Ç–∞–Ω–¥–∞—Ä—Ç–æ–≤-–∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤ –¥–ª—è –∞—Ä—Ö–∏–≤–∞—Ü–∏–∏")
    for i, candidate in enumerate(candidates, 1):
        logger.info(f"{i}. {candidate['standard']['title']} ({candidate['standard']['path']})")
        logger.info(f"   –ü—Ä–∏—á–∏–Ω–∞: {candidate['reason']}")
    
    return candidates

def archive_standards(candidates: List[Dict[str, str]], dry_run: bool = False) -> int:
    """
    –ê—Ä—Ö–∏–≤–∏—Ä—É–µ—Ç —Å—Ç–∞–Ω–¥–∞—Ä—Ç—ã.
    
    Args:
        candidates: –°–ø–∏—Å–æ–∫ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–æ–≤-–∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤ –¥–ª—è –∞—Ä—Ö–∏–≤–∞—Ü–∏–∏
        dry_run: –†–µ–∂–∏–º –±–µ–∑ –≤–Ω–µ—Å–µ–Ω–∏—è –∏–∑–º–µ–Ω–µ–Ω–∏–π (—Ç–æ–ª—å–∫–æ –≤—ã–≤–æ–¥ –≤ –ª–æ–≥)
        
    Returns:
        int: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∞—Ä—Ö–∏–≤–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö —Å—Ç–∞–Ω–¥–∞—Ä—Ç–æ–≤
    """
    if not candidates:
        logger.info("–ù–µ—Ç —Å—Ç–∞–Ω–¥–∞—Ä—Ç–æ–≤ –¥–ª—è –∞—Ä—Ö–∏–≤–∞—Ü–∏–∏")
        return 0
    
    # –°–æ–∑–¥–∞–µ–º –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é –∞—Ä—Ö–∏–≤–∞
    ensure_directory_exists(ARCHIVE_DIR)
    ensure_directory_exists(ARCHIVE_DATE_DIR)
    
    count = 0
    for candidate in candidates:
        standard = candidate["standard"]
        reason = candidate["reason"]
        
        try:
            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –ø—É—Ç—å –∫ –∞—Ä—Ö–∏–≤–Ω–æ–π –∫–æ–ø–∏–∏
            filename = os.path.basename(standard["path"])
            archive_path = os.path.join(ARCHIVE_DATE_DIR, filename)
            
            if dry_run:
                logger.info(f"[–†–µ–∂–∏–º –ø—Ä–µ–¥–ø—Ä–æ—Å–º–æ—Ç—Ä–∞] –°—Ç–∞–Ω–¥–∞—Ä—Ç –±—É–¥–µ—Ç –∞—Ä—Ö–∏–≤–∏—Ä–æ–≤–∞–Ω: {standard['path']} -> {archive_path}")
                logger.info(f"  –ü—Ä–∏—á–∏–Ω–∞: {reason}")
                count += 1
                continue
            
            # –ê—Ä—Ö–∏–≤–∏—Ä—É–µ–º —Å—Ç–∞–Ω–¥–∞—Ä—Ç
            shutil.copy2(standard["path"], archive_path)
            
            # –î–æ–±–∞–≤–ª—è–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ–± –∞—Ä—Ö–∏–≤–∞—Ü–∏–∏ –≤ —Ñ–∞–π–ª
            with open(archive_path, 'r', encoding='utf-8', errors='ignore') as f:
                content = f.read()
            
            # –î–æ–±–∞–≤–ª—è–µ–º –æ—Ç–º–µ—Ç–∫—É –æ–± –∞—Ä—Ö–∏–≤–∞—Ü–∏–∏
            archive_note = f"\n\n> [!NOTE]\n> –ê—Ä—Ö–∏–≤–∏—Ä–æ–≤–∞–Ω–æ: {datetime.datetime.now().strftime('%Y-%m-%d %H:%M')}\n> –ü—Ä–∏—á–∏–Ω–∞: {reason}\n"
            with open(archive_path, 'w', encoding='utf-8') as f:
                f.write(content + archive_note)
            
            # –£–¥–∞–ª—è–µ–º –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–π —Ñ–∞–π–ª
            os.remove(standard["path"])
            
            logger.info(f"–°—Ç–∞–Ω–¥–∞—Ä—Ç –∞—Ä—Ö–∏–≤–∏—Ä–æ–≤–∞–Ω: {standard['path']} -> {archive_path}")
            logger.info(f"  –ü—Ä–∏—á–∏–Ω–∞: {reason}")
            
            count += 1
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∞—Ä—Ö–∏–≤–∞—Ü–∏–∏ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–∞ {standard['path']}: {e}")
    
    logger.info(f"–ê—Ä—Ö–∏–≤–∏—Ä–æ–≤–∞–Ω–æ {count} —Å—Ç–∞–Ω–¥–∞—Ä—Ç–æ–≤")
    return count

def update_todo_stats(num_archived: int, dry_run: bool = False) -> bool:
    """
    –û–±–Ω–æ–≤–ª—è–µ—Ç —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –≤ —Ñ–∞–π–ª–µ todo.md.
    
    Args:
        num_archived: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∞—Ä—Ö–∏–≤–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö —Å—Ç–∞–Ω–¥–∞—Ä—Ç–æ–≤
        dry_run: –†–µ–∂–∏–º –±–µ–∑ –≤–Ω–µ—Å–µ–Ω–∏—è –∏–∑–º–µ–Ω–µ–Ω–∏–π (—Ç–æ–ª—å–∫–æ –≤—ã–≤–æ–¥ –≤ –ª–æ–≥)
        
    Returns:
        bool: True, –µ—Å–ª–∏ –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ —É—Å–ø–µ—à–Ω–æ, –∏–Ω–∞—á–µ False
    """
    todo_file = "[todo ¬∑ incidents]/todo.md"
    
    if not os.path.exists(todo_file):
        logger.error(f"–§–∞–π–ª todo.md –Ω–µ –Ω–∞–π–¥–µ–Ω: {todo_file}")
        return False
    
    try:
        # –ß–∏—Ç–∞–µ–º —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ todo.md
        with open(todo_file, 'r', encoding='utf-8') as f:
            content = f.read()
        
        # –ù–∞—Ö–æ–¥–∏–º —Å–µ–∫—Ü–∏—é —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏
        stats_section = re.search(r'## üìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –∑–∞–¥–∞—á.*?(?=##|$)', content, re.DOTALL)
        if not stats_section:
            logger.warning("–°–µ–∫—Ü–∏—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω–∞ –≤ todo.md")
            return False
        
        stats_content = stats_section.group(0)
        
        # –°–æ–∑–¥–∞–µ–º –æ—Ç–º–µ—Ç–∫—É –æ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏–∏ –∑–∞–¥–∞—á–∏ –ø–æ –∞—Ä—Ö–∏–≤–∞—Ü–∏–∏
        timestamp = datetime.datetime.now().strftime('%Y-%m-%d %H:%M')
        task_note = f"\n- [x] **–ê—Ä—Ö–∏–≤–∞—Ü–∏—è —É—Å—Ç–∞—Ä–µ–≤—à–∏—Ö —Å—Ç–∞–Ω–¥–∞—Ä—Ç–æ–≤** [done] ¬∑ @ai assistant ¬∑ –∑–∞–≤–µ—Ä—à–µ–Ω–æ {timestamp}\n"
        task_note += f"  **—Ü–µ–ª—å**: –£–º–µ–Ω—å—à–∏—Ç—å –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∞–∫—Ç–∏–≤–Ω—ã—Ö —Å—Ç–∞–Ω–¥–∞—Ä—Ç–æ–≤ –¥–æ {TARGET_ACTIVE_STANDARDS}\n"
        task_note += f"  **—Ä–µ–∑—É–ª—å—Ç–∞—Ç**: –ê—Ä—Ö–∏–≤–∏—Ä–æ–≤–∞–Ω–æ {num_archived} —Å—Ç–∞–Ω–¥–∞—Ä—Ç–æ–≤, –æ—Å—Ç–∞–≤–ª–µ–Ω–æ {TARGET_ACTIVE_STANDARDS} –∞–∫—Ç–∏–≤–Ω—ã—Ö —Å—Ç–∞–Ω–¥–∞—Ä—Ç–æ–≤\n"
        
        # –ù–∞—Ö–æ–¥–∏–º –ø–æ–¥—Ö–æ–¥—è—â–µ–µ –º–µ—Å—Ç–æ –¥–ª—è –≤—Å—Ç–∞–≤–∫–∏ –∑–∞–¥–∞—á–∏
        insert_pos = content.find("## üîú –°–ª–µ–¥—É—é—â–∏–µ –¥–µ–π—Å—Ç–≤–∏—è")
        if insert_pos == -1:
            insert_pos = content.find("## üìù –ë—ç–∫–ª–æ–≥")
            if insert_pos == -1:
                insert_pos = len(content)
        
        # –°–æ–∑–¥–∞–µ–º –æ–±–Ω–æ–≤–ª–µ–Ω–Ω–æ–µ —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ
        new_content = content[:insert_pos] + task_note + content[insert_pos:]
        
        if not dry_run:
            # –ó–∞–ø–∏—Å—ã–≤–∞–µ–º –æ–±–Ω–æ–≤–ª–µ–Ω–Ω–æ–µ —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ
            with open(todo_file, 'w', encoding='utf-8') as f:
                f.write(new_content)
            
            logger.info(f"–°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –≤ todo.md –æ–±–Ω–æ–≤–ª–µ–Ω–∞")
        else:
            logger.info(f"[–†–µ–∂–∏–º –ø—Ä–µ–¥–ø—Ä–æ—Å–º–æ—Ç—Ä–∞] –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –≤ todo.md –±—É–¥–µ—Ç –æ–±–Ω–æ–≤–ª–µ–Ω–∞")
        
        return True
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±–Ω–æ–≤–ª–µ–Ω–∏–∏ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –≤ todo.md: {e}")
        return False

def update_cache():
    """
    –û–±–Ω–æ–≤–ª—è–µ—Ç –∫–µ—à –ø–æ—Å–ª–µ –≤–Ω–µ—Å–µ–Ω–∏—è –∏–∑–º–µ–Ω–µ–Ω–∏–π.
    
    Returns:
        bool: True, –µ—Å–ª–∏ –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ —É—Å–ø–µ—à–Ω–æ, –∏–Ω–∞—á–µ False
    """
    try:
        logger.info("–û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –∫–µ—à–∞...")
        
        # –§–æ—Ä–º–∏—Ä—É–µ–º –∫–æ–º–∞–Ω–¥—É –¥–ª—è –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è –∫–µ—à–∞
        cmd = "python sync_verification.py --sync"
        
        # –í—ã–ø–æ–ª–Ω—è–µ–º –∫–æ–º–∞–Ω–¥—É
        import subprocess
        result = subprocess.run(cmd, shell=True, capture_output=True, text=True)
        
        if result.returncode == 0:
            logger.info("–ö–µ—à —É—Å–ø–µ—à–Ω–æ –æ–±–Ω–æ–≤–ª–µ–Ω")
            logger.info(result.stdout)
            return True
        else:
            logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±–Ω–æ–≤–ª–µ–Ω–∏–∏ –∫–µ—à–∞: {result.stderr}")
            return False
    except Exception as e:
        logger.error(f"–ò—Å–∫–ª—é—á–µ–Ω–∏–µ –ø—Ä–∏ –æ–±–Ω–æ–≤–ª–µ–Ω–∏–∏ –∫–µ—à–∞: {e}")
        return False

def main():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è —Å–∫—Ä–∏–ø—Ç–∞."""
    parser = argparse.ArgumentParser(description='–ê—Ä—Ö–∏–≤–∞—Ü–∏—è —É—Å—Ç–∞—Ä–µ–≤—à–∏—Ö —Å—Ç–∞–Ω–¥–∞—Ä—Ç–æ–≤')
    
    parser.add_argument('--dry-run', action='store_true',
                        help='–†–µ–∂–∏–º –±–µ–∑ –≤–Ω–µ—Å–µ–Ω–∏—è –∏–∑–º–µ–Ω–µ–Ω–∏–π (—Ç–æ–ª—å–∫–æ –≤—ã–≤–æ–¥ –≤ –ª–æ–≥)')
    parser.add_argument('--update-cache', action='store_true',
                        help='–û–±–Ω–æ–≤–∏—Ç—å –∫–µ—à –ø–æ—Å–ª–µ –∞—Ä—Ö–∏–≤–∞—Ü–∏–∏')
    parser.add_argument('--target', type=int, default=TARGET_ACTIVE_STANDARDS,
                        help=f'–¶–µ–ª–µ–≤–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∞–∫—Ç–∏–≤–Ω—ã—Ö —Å—Ç–∞–Ω–¥–∞—Ä—Ç–æ–≤ (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é {TARGET_ACTIVE_STANDARDS})')
    
    args = parser.parse_args()
    
    # –û–±–Ω–æ–≤–ª—è–µ–º —Ü–µ–ª–µ–≤–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∞–∫—Ç–∏–≤–Ω—ã—Ö —Å—Ç–∞–Ω–¥–∞—Ä—Ç–æ–≤
    TARGET_ACTIVE_STANDARDS = args.target
    
    logger.info(f"–ó–∞–ø—É—Å–∫ –∞—Ä—Ö–∏–≤–∞—Ü–∏–∏ —É—Å—Ç–∞—Ä–µ–≤—à–∏—Ö —Å—Ç–∞–Ω–¥–∞—Ä—Ç–æ–≤ (—Ü–µ–ª–µ–≤–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ: {TARGET_ACTIVE_STANDARDS})")
    
    # –ù–∞—Ö–æ–¥–∏–º –≤—Å–µ –∞–∫—Ç–∏–≤–Ω—ã–µ —Å—Ç–∞–Ω–¥–∞—Ä—Ç—ã
    standards = find_standards()
    
    # –ï—Å–ª–∏ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–æ–≤ –º–µ–Ω—å—à–µ –∏–ª–∏ —Ä–∞–≤–Ω–æ —Ü–µ–ª–µ–≤–æ–º—É –∫–æ–ª–∏—á–µ—Å—Ç–≤—É, –Ω–∏—á–µ–≥–æ –Ω–µ –¥–µ–ª–∞–µ–º
    if len(standards) <= TARGET_ACTIVE_STANDARDS:
        logger.info(f"–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∞–∫—Ç–∏–≤–Ω—ã—Ö —Å—Ç–∞–Ω–¥–∞—Ä—Ç–æ–≤ ({len(standards)}) —É–∂–µ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É–µ—Ç —Ü–µ–ª–µ–≤–æ–º—É ({TARGET_ACTIVE_STANDARDS})")
        return
    
    # –ò–¥–µ–Ω—Ç–∏—Ñ–∏—Ü–∏—Ä—É–µ–º –¥—É–±–ª–∏—Ä—É—é—â–∏–µ —Å—Ç–∞–Ω–¥–∞—Ä—Ç—ã
    duplicates = identify_duplicates(standards)
    
    # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Å—Ç–∞–Ω–¥–∞—Ä—Ç—ã-–∫–∞–Ω–¥–∏–¥–∞—Ç—ã –¥–ª—è –∞—Ä—Ö–∏–≤–∞—Ü–∏–∏
    candidates = identify_candidates_for_archiving(standards, duplicates)
    
    # –ï—Å–ª–∏ –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤ —Å–ª–∏—à–∫–æ–º –º–Ω–æ–≥–æ, –æ–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –∏—Ö –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ
    if len(standards) - len(candidates) < TARGET_ACTIVE_STANDARDS:
        num_to_remove = len(candidates) - (len(standards) - TARGET_ACTIVE_STANDARDS)
        candidates = candidates[:-num_to_remove]
    
    # –ê—Ä—Ö–∏–≤–∏—Ä—É–µ–º —Å—Ç–∞–Ω–¥–∞—Ä—Ç—ã
    num_archived = archive_standards(candidates, args.dry_run)
    
    # –û–±–Ω–æ–≤–ª—è–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –≤ todo.md
    update_todo_stats(num_archived, args.dry_run)
    
    # –û–±–Ω–æ–≤–ª—è–µ–º –∫–µ—à
    if args.update_cache and not args.dry_run:
        update_cache()
    
    logger.info("–ê—Ä—Ö–∏–≤–∞—Ü–∏—è —É—Å—Ç–∞—Ä–µ–≤—à–∏—Ö —Å—Ç–∞–Ω–¥–∞—Ä—Ç–æ–≤ –∑–∞–≤–µ—Ä—à–µ–Ω–∞")

if __name__ == "__main__":
    main()